WEBVTT

00:00:01.760 --> 00:00:05.759
2025년 현재 상황과

00:00:03.919 --> 00:00:07.600
인터넷 초기 시절을 비교해 주시면 정말 감사하겠습니다.

00:00:07.600 --> 00:00:13.519
인공지능이 2025년의 모습이라면,

00:00:10.960 --> 00:00:17.119
인터넷은 몇 년도의 모습이라고 할 수 있을까요?   음,

00:00:13.519 --> 00:00:20.000
인터넷 역사를 생각해 보면

00:00:17.119 --> 00:00:22.960
70년대에는 아르포넷 같은 게 있었던 것 같네요.  음, 하지만

00:00:20.000 --> 00:00:24.560
웹은

00:00:24.560 --> 00:00:29.199
모자이크가 93년쯤에 출시되었고, 그로부터

00:00:29.199 --> 00:00:36.880
몇 년 후에 넷스케이프가 나왔다고 생각해요.  음,

00:00:33.840 --> 00:00:38.559
어떤 면에서는

00:00:36.880 --> 00:00:41.600
유사점을 찾을 수 있다고 생각해요.

00:00:38.559 --> 00:00:44.719
음, 뭐라고 해야 할까요?

00:00:41.600 --> 00:00:48.320
2017년에 나온 트랜스포머가

00:00:44.719 --> 00:00:50.480
새로운 유형의 언어 모델에 대한 첫 번째 징후였다고 할 수 있을까요?

00:00:50.480 --> 00:01:00.239
하지만 여러 면에서 정말 다르다고 생각합니다

00:00:53.760 --> 00:01:02.320
.  제 말은, 우선

00:01:00.239 --> 00:01:05.680
인터넷은 정말

00:01:02.320 --> 00:01:09.520
훌륭했고 많은 것을 가능하게 했지만,

00:01:05.680 --> 00:01:11.280
기술적으로 혁명적이라고 할 수는 없다는 겁니다.

00:01:11.280 --> 00:01:16.640
예를 들어,

00:01:13.920 --> 00:01:18.960
CERN의 팀 버너스 리는

00:01:16.640 --> 00:01:23.439
과학자들의 데이터를 정리

00:01:18.960 --> 00:01:25.840
하고 공유하기 위해 웹을 만들었고, 그들은 정말 잘 해냈고,

00:01:23.439 --> 00:01:27.280
그것은 마치 바이러스처럼 퍼져나가 조직화에 큰 역할을 했습니다. 물론

00:01:27.280 --> 00:01:31.920
오해는 하지 마세요.  하지만

00:01:37.439 --> 00:01:43.119
5년 전만 해도 그게 물리적으로 가능한지 의문을 제기하는 사람은 아무도 없었을 거예요.  실질적인

00:01:40.400 --> 00:01:46.320
제한은 없었습니다.  음, 이 경우에는

00:01:43.119 --> 00:01:48.479
지능이 정확히 무엇인지 잘 모르겠습니다.

00:01:46.320 --> 00:01:51.680
우리는 이것을 어디까지 끌고 갈 수 있을지 모릅니다.

00:01:48.479 --> 00:01:54.000
음, 저를

00:01:51.680 --> 00:01:56.000
포함해서 많은 사람들이 이렇게 빠르게,

00:01:54.000 --> 00:01:59.200
그리고 이렇게 광범위하게 확대되는 것에 놀랐던 것 같아요.

00:01:56.000 --> 00:02:00.479
음, 그리고 그건

00:01:59.200 --> 00:02:03.360
중요한

00:02:00.479 --> 00:02:06.640
차이점이에요.  우리는 최고치가 얼마일지조차 모르는 상황이잖아요

00:02:03.360 --> 00:02:07.759
.

00:02:06.640 --> 00:02:09.920
인터넷처럼,

00:02:07.759 --> 00:02:11.680
모든 사람이

00:02:09.920 --> 00:02:14.160
서로 초고속으로 소통할 수 있다고 상상할 수 있습니다.  아시다시피

00:02:11.680 --> 00:02:16.480
, 모든 회사가

00:02:14.160 --> 00:02:18.640
웹사이트를 가지고 있을 텐데, 지금처럼 말이죠.  음,

00:02:16.480 --> 00:02:22.080
하지만

00:02:18.640 --> 00:02:25.440
1990년쯤에는 그런 걸 현실적으로 상상할 수 있었겠죠.

00:02:22.080 --> 00:02:28.000
아니, 그

00:02:25.440 --> 00:02:30.640
이전에도

00:02:28.000 --> 00:02:32.959
고퍼 같은 게 있었어요. 제가

00:02:30.640 --> 00:02:33.920
나이를 밝히는 건 아니겠지만, 웹 이전에도 그런 비슷한 것들이 있었죠. 하지만

00:02:36.959 --> 00:02:43.040
AI는

00:02:40.000 --> 00:02:45.920
정점이 어디인지, 아니면 정점이 아예 없는지조차 알 수 없어요. 그게

00:02:43.040 --> 00:02:47.920
중요한 차이점 중 하나죠.

00:02:45.920 --> 00:02:53.599
또 다른 중요한 차이점은,

00:02:47.920 --> 00:02:56.879
좋든 나쁘든 간에, 이제 AI가

00:02:53.599 --> 00:02:58.400
국제적으로 엄청난

00:02:56.879 --> 00:03:01.040
관심을 받고 있다는 거예요.  제 말은,

00:03:01.040 --> 00:03:05.360
인공지능에 투입되는 자원, 자금, 컴퓨팅 능력, 에너지의 양이

00:03:03.200 --> 00:03:06.959
엄청나다는 겁니다.  아시다시피,

00:03:05.360 --> 00:03:11.120
웹 초창기에는 저희는 스타트업이었어요.

00:03:11.120 --> 00:03:15.840
100만 달러도 안 되는 소액의 초기 자금 대출을 받아서 시작했죠. 그러다

00:03:18.000 --> 00:03:22.640
벤처 투자 유치로 1,000만 달러 정도를 받았고, 그렇게 시작하게 됐습니다.

00:03:19.920 --> 00:03:24.080
음, 아시다시피 요즘 기업들은

00:03:25.920 --> 00:03:32.720
세계 최고의 AI 모델을 구축하는 데 수십억 달러를 쏟아붓고 있습니다.  음,

00:03:30.319 --> 00:03:34.400
그건 좋은 점도 있고 나쁜 점도 있지만,

00:03:32.720 --> 00:03:34.720
확실히 다르긴 해요.

00:03:34.400 --> 00:03:39.400
응.

00:03:34.720 --> 00:03:39.400
음, 그러니까 제 생각에는

00:03:39.440 --> 00:03:42.720
유사점이 있는 만큼, 앞으로

00:03:41.280 --> 00:03:43.599
어떻게 될지는 전혀 알 수 없다는 거죠

00:03:42.720 --> 00:03:48.000
.

00:03:43.599 --> 00:03:52.159
그렇다면,

00:03:48.000 --> 00:03:54.080
인공지능은 근본적으로 발명이라기보다는 발견에 더 가깝다고 생각하시나요

00:03:52.159 --> 00:03:55.920
?

00:03:54.080 --> 00:03:57.519
이것이

00:03:55.920 --> 00:03:59.360
우주의 어떤 새로운 속성이고 우리가 우연히

00:03:57.519 --> 00:04:02.560
발견한 것이라고 생각하시나요, 아니면

00:03:59.360 --> 00:04:04.159
인류 창조의 궁극적인 시험과 같은 것이라고 생각하시나요?

00:04:02.560 --> 00:04:05.280
음, 제 생각에는 방금

00:04:04.159 --> 00:04:05.760
말씀하신 두 가지 모두 맞는 것 같아요.

00:04:05.280 --> 00:04:07.599
응.

00:04:05.760 --> 00:04:09.840
음, 그것들은

00:04:09.840 --> 00:04:13.439
웹이 원래 어떤 모습이었을지와는 분명히 다릅니다

00:04:11.920 --> 00:04:15.599
.  제 말은, 네, 지능의 한계가 어디까지인지

00:04:13.439 --> 00:04:17.919
우리가 아직 알지 못한다는 점에서 일종의 발견이라고 생각합니다

00:04:17.919 --> 00:04:24.240
.  "

00:04:20.720 --> 00:04:26.240
당신이

00:04:24.240 --> 00:04:29.040
아인슈타인보다 100배 더 똑똑해질 수 있을까?"라고 말하는 법은 없잖아요.  당신은

00:04:26.240 --> 00:04:30.639
10억 배 더 똑똑해질 수 있을까요?

00:04:29.040 --> 00:04:33.360
구글보다 시간을 더 효율적으로 활용할 수 있을까요?  아시다시피, 음...

00:04:30.639 --> 00:04:36.000
저는 잘 모르겠어요. 저희는 그 분야를

00:04:33.360 --> 00:04:37.199
규율하는 법이 뭔지 전혀 모르는 것 같아요

00:04:36.000 --> 00:04:40.880
.   음,

00:04:37.199 --> 00:04:43.680
그래서 뭐랄까,

00:04:40.880 --> 00:04:48.000
일종의 발견이라고 할 수 있겠죠.  음,

00:04:43.680 --> 00:04:51.840
비유하자면 양자

00:04:48.000 --> 00:04:53.600
컴퓨팅과 비슷한데, 우주에서

00:04:51.840 --> 00:04:54.960
실제로 얼마나 많은 연산 능력을

00:04:53.600 --> 00:04:56.000
끌어낼 수 있을지 정확히 알 수 없는 것과 같습니다

00:04:54.960 --> 00:04:57.680
.

00:04:56.000 --> 00:05:00.160
음, 양자역학의 기본 법칙에 따르면

00:04:57.680 --> 00:05:02.160
그 값은 매우 높을 거라고 예상됩니다.

00:05:00.160 --> 00:05:03.600
하지만

00:05:02.160 --> 00:05:05.680
실제로는 지금

00:05:03.600 --> 00:05:06.639
알지 못하는 다른 제약 사항들이 있을지도 모르잖아요

00:05:05.680 --> 00:05:08.800
.

00:05:06.639 --> 00:05:10.560
응.  응.  응.  아니, 그거 정말

00:05:08.800 --> 00:05:14.880
흥미롭네요.  하지만 당신은 궁극적으로

00:05:10.560 --> 00:05:16.479
인공지능이 인터넷보다 훨씬 더 중대한

00:05:14.880 --> 00:05:17.759
발견이나 발명이라고 생각하는군요

00:05:16.479 --> 00:05:23.479
.

00:05:17.759 --> 00:05:23.479
네, 제 생각엔 인터넷이

00:05:24.160 --> 00:05:30.400
정말 중요했던 것 같아요.

00:05:26.720 --> 00:05:32.240
하지만 그것은 일종의

00:05:30.400 --> 00:05:34.080
사회적

00:05:32.240 --> 00:05:36.720
발전과도 같았습니다. 모두가

00:05:34.080 --> 00:05:38.800
이러한 프로토콜을 사용하기로 합의하고

00:05:41.280 --> 00:05:46.400
TCP, IP,

00:05:44.320 --> 00:05:48.720
HTML, HP 등을 통해 자신의 데이터와 시스템을 다른 모든 사람들이 사용할 수 있도록 공개한 것이죠. HP는

00:05:46.400 --> 00:05:52.800
프로토콜에 동의하고 이를 발전시키고

00:05:48.720 --> 00:05:55.520
번성하도록 허용한 것입니다.  음, 아시다시피, 어쩌면

00:05:52.800 --> 00:05:57.199
돈이 수천

00:05:55.520 --> 00:05:58.639
년 전에 발명되어 사람들이

00:05:57.199 --> 00:06:01.120
실제로 거래하고 여러 가지 일을 할 수 있게 해준 것과 비슷할지도 모르겠어요. 하지만

00:06:01.120 --> 00:06:08.720
돈이나

00:06:05.440 --> 00:06:10.479
인터넷이 우주의 한계를 시험하고 있는 건 아니라는 거죠

00:06:08.720 --> 00:06:11.360
.

00:06:10.479 --> 00:06:14.479
하지만 AI는...

00:06:11.360 --> 00:06:16.400
네.  왜냐하면 우리는 생명체가

00:06:14.479 --> 00:06:19.440
얼마나 지능적일 수 있는지 잘 모르고,

00:06:19.440 --> 00:06:24.479
뇌에 대해서도 아는 것이 별로 없기 때문입니다.  뇌에는 아마

00:06:21.840 --> 00:06:28.319
1000억 개의 뉴런과

00:06:24.479 --> 00:06:29.840
100조 개의 시냅스가 있을 텐데, 그것들은

00:06:28.319 --> 00:06:31.840
엄청나게 빠르게 움직이죠. 그런데

00:06:29.840 --> 00:06:34.720
우리 컴퓨터로 그걸 시뮬레이션할 수 있을까요?

00:06:34.720 --> 00:06:37.600
아니, 그 이상으로 나아갈 수 있을까요? 어디까지 가능할까요? 그리고 그렇게 된다면 어떤 모습일까요

00:06:36.960 --> 00:06:38.080
?

00:06:37.600 --> 00:06:39.759
응.

00:06:38.080 --> 00:06:42.080
음,

00:06:39.759 --> 00:06:44.639
저희는 잘 모르겠습니다.  제 생각에는 이런

00:06:42.080 --> 00:06:46.639
질문들, 즉 '어떻게

00:06:44.639 --> 00:06:48.080
접근해야 할까? 무엇을 만들어야 할까? 누가 이 일을 해야 할까?' 하는 질문들은

00:06:49.919 --> 00:06:53.840
기술적이거나

00:06:52.720 --> 00:06:56.319
경제적인 질문인 동시에 철학적인 질문이기도 한 것 같아요.

00:06:53.840 --> 00:06:57.759
의식이라는 개념도 여기에

00:06:56.319 --> 00:06:58.639
포함되는 것 같은데,

00:07:00.400 --> 00:07:03.840
예를 들어 인터넷은 의식에 대한 질문을 제기하지 않았잖아요. 하지만

00:07:02.000 --> 00:07:06.080
만약 이 AI가 충분히 똑똑하고

00:07:03.840 --> 00:07:08.080
자각 능력이 있다면, 그게

00:07:06.080 --> 00:07:08.639
무슨 의미가 있을까요? 잘 모르겠네요.

00:07:08.639 --> 00:07:14.080
음, 당신은 1998년에

00:07:11.919 --> 00:07:16.240
래리 페이지와 함께 멘로 파크의 차고에서 구글을 시작하셨죠.

00:07:14.080 --> 00:07:17.599
두 사람이 기회를

00:07:16.240 --> 00:07:19.280
보고 무언가를 만들어보려고 했던 거예요

00:07:17.599 --> 00:07:20.960
.  현재 구글은 2조

00:07:19.280 --> 00:07:24.400
달러 규모의 회사이며, 제가 맞다면

00:07:20.960 --> 00:07:24.800
직원 수는 18만 명입니다.  음,

00:07:24.800 --> 00:07:28.800
아마도 플러스 마이너스일 수도 있겠죠.

00:07:27.280 --> 00:07:31.039
저도 잘 알고 있지만,

00:07:28.800 --> 00:07:33.440
AI의 흐름을 타고 있다는 건 분명

00:07:33.440 --> 00:07:39.840
여러 가지 장점이 있을 거예요. 하지만 혹시라도

00:07:39.840 --> 00:07:44.479
1%라도 20살 시절로 돌아가고 싶다는 생각이 드는 부분이 있는지 궁금해요.

00:07:42.800 --> 00:07:46.319
스탠퍼드를 갓 졸업하고

00:07:44.479 --> 00:07:48.560
차고에서 두 명이서 공부하던 시절로요.   음,

00:07:48.560 --> 00:07:53.919
좋은 질문이네요.  음, 그러니까

00:07:50.960 --> 00:07:55.919
, 저는

00:07:53.919 --> 00:08:00.479
컴퓨터 과학자로서

00:07:55.919 --> 00:08:02.479
이 시대에 어떤 나이로든 살아있다는 것 자체에 감사할 뿐입니다.  음,

00:08:00.479 --> 00:08:04.240
제 생각에는

00:08:02.479 --> 00:08:05.599
길을 건너가다 보면, 어쩌면 지나가는

00:08:04.240 --> 00:08:08.960
사람들에게

00:08:05.599 --> 00:08:11.039
나중에 알려달라고 부탁할 수 있을지도 몰라요.  어, 아시다시피

00:08:08.960 --> 00:08:13.520
인공지능 연구원들이

00:08:11.039 --> 00:08:15.360
커피 머신 주변에 모여서 이야기 나누고 있는데,

00:08:13.520 --> 00:08:16.800
다들 들떠 있는 모습이잖아요

00:08:15.360 --> 00:08:18.319
.  제 말은, 굉장히

00:08:16.800 --> 00:08:21.919
스타트업 같은 느낌이라는 거예요.  음.

00:08:18.319 --> 00:08:23.360
음, 사실 차고는 아니죠.

00:08:21.919 --> 00:08:24.800
음, 그런데 엄밀히 말하면, 처음

00:08:23.360 --> 00:08:25.840
시작했을 때는 차고랑

00:08:24.800 --> 00:08:26.560
침실 두 개 정도였어요.

00:08:25.840 --> 00:08:28.160
좋아요, 그럼요.   마치 그런 것 같았

00:08:26.560 --> 00:08:31.919
지만, 우리에겐

00:08:28.160 --> 00:08:32.479
차고가 있었잖아요.  하지만 그곳은 단순한 차고가 아니었어요.

00:08:31.919 --> 00:08:33.839
음,

00:08:32.479 --> 00:08:35.490
제가 가장 걱정했던 건

00:08:33.839 --> 00:08:37.120
역사적 사실을 잘못 아는 거였는데, 이제 괜찮아져서 다행이에요

00:08:35.490 --> 00:08:38.959
. [웃음]

00:08:37.120 --> 00:08:40.800
아니, 아니, 우리는 이야기를 마치

00:08:38.959 --> 00:08:42.880
차고처럼 들려줍니다.  하지만, 어, 하지만,

00:08:42.880 --> 00:08:45.360
추가로 방이 몇 개 더 있어서 큰 도움이 됐어요.

00:08:44.720 --> 00:08:49.920
좋아요.

00:08:45.360 --> 00:08:54.320
음, 하지만 제 생각에는 현재

00:09:00.160 --> 00:09:04.880
최첨단 기술을 놓고 경쟁하는 데 필요한 컴퓨팅 요구 사항들을 고려할 때, 기업가 정신이 매우 중요하다고 봅니다

00:09:01.680 --> 00:09:07.120
.  음,

00:09:04.880 --> 00:09:08.800
그리고 음, 거기에

00:09:07.120 --> 00:09:11.600
들어가는 과학적 지식의 양 같은 거죠

00:09:08.800 --> 00:09:15.839
.  차고에서 몇몇 사람들이 모여서

00:09:15.839 --> 00:09:21.040
기초 모델 측면에서 큰 진전을 이루려고 하는 건 정말 어려울 거예요

00:09:18.399 --> 00:09:23.040
.  많은

00:09:21.040 --> 00:09:25.120
사람들이 차고에서

00:09:23.040 --> 00:09:28.800
이러한 모델을 사용하여 새롭고

00:09:25.120 --> 00:09:30.160
놀라운 것들을 만들어낼 수 있습니다.  음, 그리고 음,

00:09:30.160 --> 00:09:35.519
누군가가 정말 기발한 아이디어를 떠올려서

00:09:35.519 --> 00:09:39.440
차고에 있는 몇몇 사람들만으로도 해낼 수 있을 가능성을 완전히 배제하고 싶지는 않아요

00:09:37.600 --> 00:09:43.200
.

00:09:39.440 --> 00:09:48.480
음, 하지만 우리 회사처럼

00:09:48.480 --> 00:09:52.000
꽤 큰 회사들이 그 새로운 분야를 개척해 나가는 것 같습니다

00:09:50.000 --> 00:09:54.240
.  저는 우리가 지금 새로운 경계선에 서 있다고 생각합니다.   저는

00:09:54.240 --> 00:09:59.200
지난 한 해 동안 우리가 이룬 제품 개발 진전에 대해 매우 자랑스럽게 생각합니다

00:09:55.839 --> 00:10:00.800
.  음, 솔직히 말씀드리면, 그 일에 참여할 수 있게 되어 정말

00:09:59.200 --> 00:10:01.839
감사하게 생각합니다

00:10:00.800 --> 00:10:04.320
.  음, 저는 아직

00:10:04.320 --> 00:10:07.040
어린 시절의 제 모습으로 순간 이동하는 기술을 쓰고 싶지는 않아요

00:10:06.560 --> 00:10:10.080
.

00:10:07.040 --> 00:10:13.440
응.  앞으로 10년 안에 현실이 될 가능성이 꽤 높다고 생각하는, 가장 공상과학처럼 들리는 것은 무엇인가요

00:10:15.440 --> 00:10:19.279
?

00:10:17.519 --> 00:10:21.040
[코웃음] 제가 생각하기에 가장 흥미로운 점은

00:10:23.680 --> 00:10:26.680
제미니가

00:10:29.839 --> 00:10:36.320
자체적으로

00:10:32.560 --> 00:10:38.720
머신러닝

00:10:36.320 --> 00:10:41.440
아이디어를 내놓고

00:10:38.720 --> 00:10:43.600
구현하는 등 실질적인 기여를 해서 차세대

00:10:41.440 --> 00:10:47.279
버전을 개발하는 것일 겁니다.  저희는 이미 Gemini를

00:10:43.600 --> 00:10:49.360
많이 활용하고 있습니다. 예를 들어 AI

00:10:47.279 --> 00:10:53.680
연구원이 "

00:10:49.360 --> 00:10:56.160
이 코드 디버깅 좀 해주세요" 또는 "

00:10:53.680 --> 00:10:57.920
이 수학 계산 좀 도와주세요"와 같은 일회성 요청을 할 때뿐만 아니라,

00:11:07.600 --> 00:11:14.160
AI 자체가 만들어내는 실질적이고 중요한 돌파구를 마련할 때도 Gemini를 사용합니다.  음, 제 생각엔 그건 공상

00:11:11.440 --> 00:11:15.760
과학 소설 같지만,

00:11:14.160 --> 00:11:18.079
충분히 일어날 수도 있을 것 같아요.

00:11:15.760 --> 00:11:20.399
응.

00:11:18.079 --> 00:11:21.760
제미니가 차세대

00:11:20.399 --> 00:11:24.240
제미니를 언제쯤 출시할 거라고 대략적으로 예상해 보시겠어요?

00:11:21.760 --> 00:11:27.279
음, 그러니까 제가 말씀드렸듯이 이미 도움을 주고 있는 중이에요

00:11:24.240 --> 00:11:30.480
.  음,

00:11:27.279 --> 00:11:32.720
하지만 말하자면 완전히 처음부터

00:11:30.480 --> 00:11:36.320
다시 쓰는 것과 같은 방식이에요.  모르겠습니다.

00:11:32.720 --> 00:11:38.959
그건 어려운 질문이네요.  그게 얼마나 중요한 우선순위인지 어느 정도는 잘

00:11:36.320 --> 00:11:43.200
모르겠어요.

00:11:38.959 --> 00:11:44.959
저희가 방향을 제시해

00:11:44.959 --> 00:11:49.760
드릴 수는 있겠지만, 언제쯤 가능할지 모르겠네요. 음,

00:11:53.360 --> 00:11:58.640
3년이나 4년쯤? 그게 그 자체로 얼마나 큰 의미를 가질지는 잘 모르겠지만요.

00:12:02.473 --> 00:12:08.240
[코웃음] 하지만 생각해 보세요.

00:12:05.120 --> 00:12:09.920
저희의 새로운 비디오 모델,

00:12:08.240 --> 00:12:12.160
V3 말인데요,

00:12:12.160 --> 00:12:15.360
데모 시연장에서 저를 감동시켜 눈물짓게 했어요. [웃음]

00:12:13.680 --> 00:12:17.839
좋은 의미로요. 정말 좋은 의미로요.   네, 그냥

00:12:15.360 --> 00:12:18.720
소리 말이에요. 뭔가 특별한 게 있었어요.

00:12:18.720 --> 00:12:22.560
소리가 없다는 게 정말 큰 문제였어요. 소리가

00:12:22.560 --> 00:12:25.600
없어진 게 얼마나 큰 의미였는지, 그걸 깨닫지 못했어요. 모든 게 다 있길래 마치

00:12:25.600 --> 00:12:29.200
머리를 얻어맞은 것처럼 충격을 받았어요.

00:12:26.240 --> 00:12:31.600
오, 감사합니다.  음,

00:12:29.200 --> 00:12:33.680
그런데 아시다시피, 이론적으로는, 제가 직접

00:12:31.600 --> 00:12:35.360
시도해 본 적은 없지만, 우선,

00:12:35.360 --> 00:12:40.320
사용자 인터페이스에서 이 기능을 지원하는지는 모르겠지만, 굳이

00:12:37.360 --> 00:12:42.560
프롬프트를 표시할 필요는 없을 것 같습니다.  음, 그러니까 그냥

00:12:40.320 --> 00:12:43.839
비디오를 생성하는 거예요.  저희

00:12:42.560 --> 00:12:46.240
사용자 인터페이스는

00:12:43.839 --> 00:12:48.320
프롬프트가 표시되지 않는 것을 실제로 지원하지 않을 수도 있습니다.  하지만

00:12:46.240 --> 00:12:48.959
그러면 결과물이 어떻게 나올지 전혀 알 수 없겠죠

00:12:48.320 --> 00:12:49.360
.

00:12:48.959 --> 00:12:53.200
응.

00:12:49.360 --> 00:12:56.240
음, 그냥 좋은

00:12:53.200 --> 00:12:58.639
영상을 만들라고 하면 될 것 같네요.  음, 그건 그냥

00:12:56.240 --> 00:13:00.880
모델 자체로도 괜찮은 결과물을 내는 방식일 수 있지만,

00:12:58.639 --> 00:13:03.200
일반적으로 사람이 직접 지시하고, 아마도

00:13:00.880 --> 00:13:06.399
당신이

00:13:03.200 --> 00:13:07.839
하신 것처럼 어떤 목표나 방향을 제시해 주면

00:13:06.399 --> 00:13:10.240
훨씬 좋은 결과가 나온다고 생각합니다. 제

00:13:07.839 --> 00:13:12.480
말은,

00:13:10.240 --> 00:13:15.519
만약 제미니가 차세대

00:13:12.480 --> 00:13:17.920
제미니를 만든다면, 당분간은 적어도 어느 정도까지는 사람이 방향을 제시해 주는 것이 더 나은 결과를 가져올 거라고 생각한다는 겁니다

00:13:24.480 --> 00:13:30.320
.  제 말은,

00:13:26.720 --> 00:13:32.160
언젠가 백지상태에서 시작해서

00:13:30.320 --> 00:13:35.040
아무런 지침 없이 스스로 모든 걸 해낼 수도 있다는 거죠.

00:13:32.160 --> 00:13:36.639
음, 하지만 네, 그건

00:13:35.040 --> 00:13:37.040
우리가 아직 도달하지 못한 수준의 SF라고 생각해요

00:13:36.639 --> 00:13:39.360
.

00:13:37.040 --> 00:13:42.160
알았어요.  그러니까 가까운 미래에는

00:13:42.160 --> 00:13:46.639
구글 직원들이 인공지능을 도와

00:13:44.959 --> 00:13:47.760
제미니의 차세대 버전, 즉

00:13:46.639 --> 00:13:48.720
우리가 나아가야 할 미래를 만들어가는 세상이 될 거라고 예상하시는 건가요?

00:13:47.760 --> 00:13:51.200
네.  네, 맞아요.

00:13:48.720 --> 00:13:53.040
응.  응.  [코웃음] 음, 저는 당신이

00:13:57.680 --> 00:14:03.040
인공지능에 대한 더 깊고 심오한 철학적 질문들에 얼마나 많은 시간과 에너지를 쏟는지, 그리고

00:14:01.199 --> 00:14:04.639
기술적 질문,

00:14:03.040 --> 00:14:06.800
실용적 질문, 사업적

00:14:04.639 --> 00:14:08.480
질문들에 얼마나 많은 시간을 할애하는지 궁금합니다.   네,

00:14:06.800 --> 00:14:10.480
어떻게.

00:14:10.480 --> 00:14:14.560
우리가 멋진 기술을 개발하는 동시에

00:14:14.560 --> 00:14:18.560
우주의 근본적인 가능성을 발견하고 있는 것 같은 느낌이 드는데, 그 모든 것에 얼마나 많은 에너지를 쏟고 있나요?

00:14:18.560 --> 00:14:22.720
응.

00:14:20.480 --> 00:14:24.480
제 말은,

00:14:26.240 --> 00:14:31.120
실질적인 문제라는 관점에서 보면 철학적인 질문들이 그다지 중요한 비중을 차지하지는 않을 거라는 거죠.  마치 목적지에 도착하기까지 처리

00:14:28.720 --> 00:14:32.959
해야 할 기술적인 세부 사항이 너무 많은 것과 같아요

00:14:31.120 --> 00:14:33.440
.

00:14:32.959 --> 00:14:36.240
응.

00:14:33.440 --> 00:14:38.720
음, 아시다시피,

00:14:36.240 --> 00:14:41.120
울트라 마라톤이나 VO3 같은 대회에 참가 신청을 할 수 있을지,

00:14:38.720 --> 00:14:42.800
그리고

00:14:41.120 --> 00:14:45.120
제가 기대했던 대로 잘 진행되지 않는 모든 것들에 대해 걱정하고 있어요.  그리고

00:14:42.800 --> 00:14:46.959
저는 지금 엔지니어들

00:14:45.120 --> 00:14:49.920
과 제품 관리자들에게 사소한 문제들에 대해 계속해서 따지고 있습니다

00:14:46.959 --> 00:14:52.320
.

00:14:49.920 --> 00:14:55.600
음, 그러니까 잠시 한 발짝 물러서서 생각해 보는 것도 확실히 좋죠

00:14:52.320 --> 00:15:00.000
.

00:14:55.600 --> 00:15:03.680
음, 철학적인 질문 중 일부는

00:15:03.680 --> 00:15:08.959
기술적인 세부 사항에서 자연스럽게 생겨납니다. 예를 들어,

00:15:06.000 --> 00:15:11.519
새로운 모델이 있는데

00:15:08.959 --> 00:15:13.440
어떻게 평가해야 할까요?

00:15:15.440 --> 00:15:19.440
좋은 모델이란 무엇을 의미할까요? 표준

00:15:17.279 --> 00:15:21.360
벤치마크 같은 것들이 있는데, 어느 시점에 이르면

00:15:19.440 --> 00:15:23.199
모델은

00:15:21.360 --> 00:15:24.880
이러한 벤치마크에서 매우 뛰어난 성능을 보이게 됩니다.

00:15:24.880 --> 00:15:30.399
그리고 모델을 재설계해야 할 때마다, 한 발짝 물러나서

00:15:30.399 --> 00:15:34.560
철학적으로

00:15:32.399 --> 00:15:38.720
무엇이 중요한지 고민하게 됩니다.

00:15:34.560 --> 00:15:40.560
음, 새로운 종류의 AI 모델,

00:15:38.720 --> 00:15:42.720
예를 들어 확산 모델 같은 게 있는데,

00:15:40.560 --> 00:15:44.720
이제는 텍스트 확산을 가지고 실험해 볼 수 있죠

00:15:46.880 --> 00:15:52.639
. 이건 완전히 똑같은 비교는 아니에요.  음.

00:15:48.800 --> 00:15:54.639
그래서 이제 우리는 이런 질문을 던지게 됩니다. 왼쪽에서 오른쪽으로 움직이지

00:15:52.639 --> 00:15:56.160
않고

00:15:56.160 --> 00:15:59.920
전체를

00:15:58.560 --> 00:16:00.399
한 번에 회전시키는 것을 어떻게 비교할 수 있을까요?

00:15:59.920 --> 00:16:04.720
응.

00:16:00.399 --> 00:16:08.079
음, 그걸

00:16:04.720 --> 00:16:11.120
우리가 일반적으로 사용하는

00:16:08.079 --> 00:16:13.199
자동 회복 모델과 비교해서 어떻게 측정할 수 있을까요?  그래서 이러한

00:16:11.120 --> 00:16:14.720
것들 중 많은 부분이 철학적인

00:16:13.199 --> 00:16:17.199
질문을 불러일으킵니다.

00:16:14.720 --> 00:16:19.360
음,

00:16:17.199 --> 00:16:22.399
하지만 당신은 꽤 현실적이고 성실한 분이시네요.

00:16:19.360 --> 00:16:23.600
농담은 아니고요. 질문에 답하는 걸 보니 말이죠

00:16:22.399 --> 00:16:25.600
.

00:16:23.600 --> 00:16:27.199
응.  응.  음, 그건 아주

00:16:25.600 --> 00:16:29.120
실용적인 이유에서 비롯된 거죠.

00:16:27.199 --> 00:16:30.560
사실,

00:16:29.120 --> 00:16:31.920
다른 사람들이 무엇을 연구하고 있는지에 대한 이론적인 이야기가 아니라,

00:16:30.560 --> 00:16:33.040
실제로

00:16:31.920 --> 00:16:35.279
연구실 건물에 있을 때의 이야기입니다.

00:16:33.040 --> 00:16:36.959
응.  응.  글쎄요, 저도 잘 모르겠네요.

00:16:36.959 --> 00:16:39.680
철학적인

00:16:38.399 --> 00:16:42.560
질문들에 더 많은 시간을 할애할 수 있다면 제게는 좋을 것 같습니다.

00:16:39.680 --> 00:16:43.600
음, 하지만 지금 여러 가지 일이 벌어지고 있어요.

00:16:42.560 --> 00:16:45.920
거기에는 많은 것이 있습니다.

00:16:43.600 --> 00:16:47.839
응.  음, 남은 시간이 얼마나 되는지는 모르겠지만

00:16:45.920 --> 00:16:49.440
, 꼭 여쭤보고 싶은 게 하나 있는데요, 저

00:16:52.000 --> 00:16:55.360
같은 인터뷰어들이

00:16:54.480 --> 00:16:57.600
당신에게 더 많이 물어봤으면 하는 질문이나 주제가 있다면 무엇인가요?

00:16:55.360 --> 00:17:01.920
사람들이 제게 물어봐 줬으면 하는 질문이나 주제는 무엇인가요

00:16:57.600 --> 00:17:04.720
?  세상에.  음,

00:17:04.720 --> 00:17:08.480
좋아요. 이걸 질문 형태로 바꿔서 얘기해 볼까요? 아니면 그냥

00:17:08.480 --> 00:17:13.520
질문에 답해 드리는 게 나을지도 모르겠네요.  그게 더

00:17:10.079 --> 00:17:16.520
쉬울지도 몰라.  음, 어, 제 생각엔

00:17:19.280 --> 00:17:24.400
사람들이

00:17:22.400 --> 00:17:27.839
새로운 AI 발표 같은 것에 반응하는 전반적인 현상인 것 같아요.

00:17:24.400 --> 00:17:30.240
어제 여러 가지를 발표했는데,

00:17:27.839 --> 00:17:32.160
이제 여러분은

00:17:30.240 --> 00:17:33.840
그 발표들을 어떻게 활용할 수 있을까요?

00:17:32.160 --> 00:17:35.280
음,

00:17:33.840 --> 00:17:37.440
그리고

00:17:35.280 --> 00:17:39.760
그런 것들로 할 수 있는 멋진 일들이 많이 있어요.  음, 그리고

00:17:37.440 --> 00:17:42.400
할 수 없거나

00:17:39.760 --> 00:17:44.160
제대로 되지 않는 것들이 꽤 많아요.

00:17:42.400 --> 00:17:47.679
하지만 제가 생각하기에 흥미로운 질문은 앞으로

00:17:51.440 --> 00:17:57.840
1년, 2년 안에 다음 세대가 그러한 것들을 가지고 무엇을 할 수 있을 것인가 하는 점입니다.  음, 그리고 그게 바로

00:17:56.160 --> 00:17:59.600
온갖

00:17:57.840 --> 00:18:01.840
흥미로운 질문들을 불러일으키는 거죠.  음, 그러니까

00:18:03.919 --> 00:18:07.600
2년 전 언어 모델들이 정말

00:18:05.840 --> 00:18:08.559
부끄러운

00:18:07.600 --> 00:18:09.120
오류를 너무 많이 저질렀어요.

00:18:08.559 --> 00:18:12.400
음.   마치 "와

00:18:12.400 --> 00:18:17.600
, 이 기계가 이걸 정말

00:18:14.000 --> 00:18:19.360
제대로 해냈네. 정말 멋지다!"라는 느낌이었어요.

00:18:17.600 --> 00:18:22.080
그건 "세상에,

00:18:19.360 --> 00:18:24.320
이걸 실제로 도구로 쓸 수 있겠네.

00:18:22.080 --> 00:18:26.160
만약

00:18:24.320 --> 00:18:28.720
이게 20% 정도 맞다면, 그걸

00:18:28.720 --> 00:18:32.720
트윗에 올릴 수도 있겠다"라는 생각과는 완전히 다르죠. 후자가 바로 "와

00:18:30.559 --> 00:18:35.280
, 멋지다"라는 느낌이니까요.  하지만 저는

00:18:32.720 --> 00:18:38.559
그걸 일상적으로 사용할 수는 없어요.   그리고 음,

00:18:38.559 --> 00:18:41.840
그럼에도 불구하고 추세를 살펴보면

00:18:44.480 --> 00:18:50.320
일상적으로 상당히 안정적으로 사용할 수 있을 것 같습니다.

00:18:48.160 --> 00:18:51.919
음, 그러니까

00:18:50.320 --> 00:18:53.440
사람들이

00:18:51.919 --> 00:18:55.120
이런 도구들을 어디에 연결해서 사용할지

00:18:57.919 --> 00:19:00.320
, 예를 들어

00:18:59.760 --> 00:19:00.640
영화를 만들려고 할 때 어떻게 활용할지 생각하게 되는 것 같아요.

00:19:00.320 --> 00:19:02.799
응.

00:19:00.640 --> 00:19:04.320
어, VO는 멋지고 이제 소리도 나네요.   아시다시피

00:19:02.799 --> 00:19:05.600
, 아마 1년 전만 해도 '

00:19:04.320 --> 00:19:09.200
소리가 안 나니까

00:19:05.600 --> 00:19:11.039
불편하겠네'라고 생각했을지도 몰라요.  음, 캐릭터의 연속성 등을 위해 몇 가지 작업을 진행했지만,

00:19:11.039 --> 00:19:15.440
어... 그리고

00:19:14.080 --> 00:19:18.080
실제로 이 기술을 활용한 영화 몇 편을 제작 중이긴 합니다.

00:19:15.440 --> 00:19:20.720
하지만

00:19:20.720 --> 00:19:25.360
2시간짜리 장편 영화를 만들기에는 아직 이상적인 기술은 아닌 것 같습니다.

00:19:22.480 --> 00:19:27.520
확신하는.  음 [코웃음] 하지만 그럼에도 불구하고

00:19:27.520 --> 00:19:31.440
지난 몇 년 동안 이러한 도구들이 얼마나 발전했는지 살펴보면, 우리 것뿐만 아니라

00:19:29.760 --> 00:19:33.440
모든 비디오 모델들을 보면, 앞으로

00:19:36.320 --> 00:19:39.280
몇 년 안에

00:19:37.760 --> 00:19:40.799
정말 흥미로운 일들을 많이 할 수 있을 거라고 생각합니다

00:19:39.280 --> 00:19:42.080
.  음, 그럼

00:19:42.080 --> 00:19:45.760
예를 좀 들어주시겠어요?

00:19:43.919 --> 00:19:48.400
제가 들은 주장 중 하나는, 여러 가지

00:19:48.400 --> 00:19:51.440
방식을 비교해 봤을 때

00:19:49.840 --> 00:19:53.200
비디오가 가장 비용이 많이 든다는 거예요.

00:19:53.200 --> 00:19:58.640
유튜브의 재밌는 영상이나

00:19:55.520 --> 00:20:01.440
틱톡의 AI 콘텐츠처럼 실용적인 활용도가 높지 않은데,

00:20:01.440 --> 00:20:05.760
왜

00:20:04.160 --> 00:20:07.200
우리가 거기에 그렇게 많은 시간과 에너지를 투자해야 하는지에 대한 논란이나 이야기가 많잖아요.

00:20:05.760 --> 00:20:08.799
물론 AI가

00:20:07.200 --> 00:20:09.840
정말 멋지다는 건 인정하지만요.

00:20:09.840 --> 00:20:14.000
이러한 비디오 모델의 다른 실용적인 응용 사례에는 어떤 것들이 있는지 알려주시겠습니까?

00:20:12.320 --> 00:20:18.400
네, 제 말은,

00:20:14.000 --> 00:20:20.400
멋진

00:20:18.400 --> 00:20:23.600
장난감

00:20:20.400 --> 00:20:25.520
과 유용한 도구의 차이와 같다고 생각해요.

00:20:23.600 --> 00:20:29.280
시간이 지나

00:20:25.520 --> 00:20:30.880
면서 서서히 변화하는 거죠.  그리고 음, 아시다시피

00:20:29.280 --> 00:20:34.720
, 저희는

00:20:30.880 --> 00:20:38.000
유용한 도구를 만드는 것을 목표로 하고 있습니다.  음,

00:20:34.720 --> 00:20:39.679
여기 영화 제작자 몇 분이 계십니다.  음, 제 생각엔

00:20:38.000 --> 00:20:41.360
대런 아로노스키가 이미

00:20:39.679 --> 00:20:42.960
이야기를 했을지도 모르겠어요.

00:20:41.360 --> 00:20:46.400
패널로 참여했는지는 잘 모르겠지만, 어쨌든

00:20:42.960 --> 00:20:50.000
영상을 제작 중이라고 하네요.  음, 제 친한 친구인

00:20:46.400 --> 00:20:52.000
더스틴이 영상을 제작하고 있어요.  음, 하지만 아시다시피

00:20:50.000 --> 00:20:54.000
, 일부 진정한 예술가들은

00:20:52.000 --> 00:20:55.520
이러한 도구를 사용하고 있지만, 아직

00:20:54.000 --> 00:20:57.840
초기 단계입니다.  제 말은, 그들이

00:20:57.840 --> 00:21:03.840
다루고 있는 것은 분명히

00:21:00.159 --> 00:21:06.000
2년 후 영화감독들이

00:21:03.840 --> 00:21:08.240
농담처럼 여길 만한 일이라는 거죠.  음.

00:21:06.000 --> 00:21:10.400
하지만 그들은

00:21:08.240 --> 00:21:14.159
개척자의 자리에 서기 위해 그런 상황을 감수하고 있는 것이고,

00:21:10.400 --> 00:21:15.760
저는 이 모델들이

00:21:14.159 --> 00:21:19.200
정말 매력적인

00:21:15.760 --> 00:21:21.760
영상을 만들어낼 수 있을 거라고 생각합니다.  음, 그들은

00:21:19.200 --> 00:21:23.679
인간

00:21:21.760 --> 00:21:24.000
감독, 인간 배우 등과 협력하여 그 일을 해낼 수 있을 겁니다.

00:21:23.679 --> 00:21:26.559
네, 아

00:21:24.000 --> 00:21:29.520
레노프스키 감독의 영화는

00:21:29.520 --> 00:21:35.840
실제 연기와

00:21:32.559 --> 00:21:37.760
AI 생성 기술을 꽤 멋지게 결합한 것 같아요.

00:21:35.840 --> 00:21:41.039
음,

00:21:37.760 --> 00:21:44.640
하지만 아시다시피 오늘날

00:21:41.039 --> 00:21:46.640
우리는 산업 조명, 조명, 마법, 그리고

00:21:44.640 --> 00:21:48.480
루카스필름과 같은 회사들을 보유하고 있죠.

00:21:46.640 --> 00:21:50.320
그들은 모든 특수 효과를 담당하고 있고,

00:21:48.480 --> 00:21:53.360
우리는 이미 기술을 사용하여

00:21:50.320 --> 00:21:56.720
영화를 제작하고 있습니다.  음, 이건

00:21:53.360 --> 00:21:59.520
그 점에서 일종의 새로운 차원이고, 당연히

00:21:56.720 --> 00:22:02.000
초기 단계라서

00:21:59.520 --> 00:22:05.360
최상의 해상도는 아닐 수도 있습니다.  오랜 기간에 걸쳐

00:22:02.000 --> 00:22:07.120
완벽한 연속성을 보장하는 건 아니지만

00:22:05.360 --> 00:22:08.720
, 저는 그런

00:22:07.120 --> 00:22:11.760
부분들이 차차 해결될 거라고 생각합니다

00:22:08.720 --> 00:22:14.080
.  음, 그래서 저희는

00:22:11.760 --> 00:22:15.280
이런 것들이 단순한 장난감이 아니라 진정한 도구가 될 수 있도록 한계를 뛰어넘으려고 노력하고 있습니다

00:22:15.280 --> 00:22:17.840
.

00:22:16.480 --> 00:22:19.840
응.  응.  응.

00:22:17.840 --> 00:22:21.520
제가 제 플랫폼에서 항상 하는 말 중 하나는,

00:22:19.840 --> 00:22:23.520
오늘 여러분이 어떻게 생각하시든 간에, 지금이 앞으로 가장

00:22:21.520 --> 00:22:24.799
최악의 모습일 거라는 겁니다.

00:22:23.520 --> 00:22:25.120
네, 맞아요.  네, 완전히

00:22:24.799 --> 00:22:26.720
맞습니다.

00:22:25.120 --> 00:22:28.640
음, 네. 질문 하나만 더 해도 된다는 신호를 받았어요

00:22:26.720 --> 00:22:31.679
.  음, 저는

00:22:28.640 --> 00:22:34.320
이 질문을 구글 직원이

00:22:34.320 --> 00:22:38.640
아니지만, 이 순간에 대해 매우 기뻐하는 모든 개발자들을 대표해서 드리겠습니다

00:22:37.039 --> 00:22:41.520
.  그들은 프론티어 랩에서 일하는 게 아닙니다

00:22:38.640 --> 00:22:42.960
.

00:22:41.520 --> 00:22:44.080
이 사람들에게 어떤 방향을 제시해 주시겠습니까

00:22:42.960 --> 00:22:45.919
?

00:22:44.080 --> 00:22:48.480
소프트웨어 엔지니어링 배경을 가진 사람이든, 아니면 단순히

00:22:48.480 --> 00:22:52.159
이 순간의 깊이와 중요성을 이해하고

00:22:50.080 --> 00:22:53.440
참여하고 싶어하는 사람이든 상관없습니다.

00:22:53.440 --> 00:22:58.559
그들에게 "여기서부터 시작하세요"라고 말할 수 있는 방향을 제시해 주시겠어요?  아니면

00:22:58.559 --> 00:23:01.760
그 방향으로 가지 말라고 말씀하실 만한 다른 방향이 있을까요?

00:23:00.000 --> 00:23:04.080
시간 낭비하지 마세요.   음

00:23:04.080 --> 00:23:08.000
, 좋은 아이디어가 너무 많아서

00:23:05.840 --> 00:23:09.760
누구의 계획도 막고 싶지 않아요. 무슨 일이 일어날지 모르잖아요.

00:23:09.760 --> 00:23:12.559
응.   음,

00:23:12.559 --> 00:23:18.400
제 생각에는

00:23:20.240 --> 00:23:26.159
대형 연구소 밖에서도 정말 흥미로운 학술 연구가 점점 늘어나고 있는 것 같아요.  음, 그리고 제 생각에는

00:23:26.159 --> 00:23:30.320
추론 모델의 등장으로 인해 이러한 현상이 더욱 두드러지게 나타나는 것 같습니다. 즉,

00:23:30.320 --> 00:23:35.840
강화 학습의 사후

00:23:32.400 --> 00:23:39.039
단계에서 이러한 현상이 더 많이 발생하는데, 이는

00:23:39.039 --> 00:23:44.960
많은 학술 기관이나

00:23:41.679 --> 00:23:47.120
소규모 기업이 보유한 컴퓨팅 자원으로 관리하기가 더 용이하기 때문입니다.  그리고

00:23:44.960 --> 00:23:49.360
실제로

00:23:47.120 --> 00:23:51.760
저희가 개발한 자바 모델을 포함하여 다양한 오픈 소스 가중치 모델이 있으므로,

00:23:49.360 --> 00:23:55.679
이를 사용하여 실험해 보실 수 있습니다.  음, 앞으로는

00:23:58.240 --> 00:24:04.240
최고 수준의 모델들이 강화 학습 API를 제공하는 모습을 점점 더 많이 보게 될 거라고 생각합니다. 예를 들어, 여러분이

00:24:00.880 --> 00:24:06.400
문제를 보내주실 때

00:24:04.240 --> 00:24:09.520
정답이나 채점 기준표 등을 함께 보내주시면, 저희가

00:24:06.400 --> 00:24:11.520
여러분의 문제를 개선하는 데 도움을 드릴 수 있습니다.

00:24:09.520 --> 00:24:14.240
모델이 특정 분야에 강점을 갖도록 하는 것도

00:24:11.520 --> 00:24:18.000
좋은 방법입니다.  음, 그래서 저는 그런

00:24:14.240 --> 00:24:20.799
일이 일어날 거라고 생각해요.  네, 사실 저는

00:24:28.000 --> 00:24:31.360
기초 모델을 따로 훈련시키지 않고도 영향력을 행사할 수 있는 좋은 시기라고 생각합니다.

00:24:30.000 --> 00:24:34.640
놀라운.

00:24:31.360 --> 00:24:34.640
시간 내주셔서 정말 감사합니다.  감사합니다.

