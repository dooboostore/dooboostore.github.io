WEBVTT

00:00:00.240 --> 00:00:05.200
미친 게 뭔지 알아?  이 모든 것이 진짜라는 것.
무슨 뜻이야?

00:00:05.200 --> 00:00:10.320
그렇게 생각하지 않으세요?  이 모든 AI와
베이 지역에서 일어나는 일들... 그것이 일어나고 있다는 것입니다.

00:00:11.440 --> 00:00:16.240
이건 마치 공상과학 소설에서 튀어나온 것 같지 않나요?
또 다른 미친 점은

00:00:16.240 --> 00:00:21.520
느린 이륙이 얼마나 정상적으로 느껴지는가 하는 점입니다.  GDP의
1%를 AI에 투자한다는 아이디어는

00:00:21.520 --> 00:00:26.880
더 큰 일처럼 느껴졌지만, 지금은 그저 그런 느낌입니다.

00:00:26.880 --> 00:00:32.640
우리는 사물에 꽤 빨리 익숙해지는 것 같습니다.
하지만 어느 정도 추상적이기도 하죠.  무슨

00:00:32.640 --> 00:00:37.920
뜻이에요?  즉, 뉴스에서
어떤 회사가 어떤 금액을 발표했다는 것을 본다는 의미입니다

00:00:37.920 --> 00:00:45.840
.  그게 당신이 보는 전부예요.
지금까지는 다른 방식으로는 느껴지지 않습니다.

00:00:45.840 --> 00:00:48.000
실제로 여기서부터 시작해야 할까요?
흥미로운 토론인 것 같아요.

00:00:48.000 --> 00:00:49.920
확신하는.  저는

00:00:49.920 --> 00:00:55.440
평균적인 사람의 관점에서 보면
아무것도 다르지 않다는 당신의 관점이

00:00:55.440 --> 00:00:58.880
특이점에 도달해도 계속 사실일 것이라고 생각합니다.
아니, 그렇게 생각하지 않아요.

00:00:58.880 --> 00:01:01.920
알겠습니다. 흥미롭네요.
제가

00:01:01.920 --> 00:01:10.880
달리 느끼지 못한다고 말한 것은, 이런저런
회사가 이해하기 어려운

00:01:10.880 --> 00:01:15.040
금액의 투자를 발표했다는 것입니다.
그걸 어떻게 해야 할지 아는 사람은 아무도 없을 것 같아요.

00:01:15.920 --> 00:01:24.160
하지만 저는 AI의 영향이 느껴질 것이라고 생각합니다.
AI는 경제 전반에 확산될 것입니다.   여기에는

00:01:24.160 --> 00:01:28.560
매우 강력한 경제적 힘이 있을 것이고
, 그 영향이

00:01:28.560 --> 00:01:32.640
매우 강하게 느껴질 것이라고 생각합니다.
언제쯤 그런 효과가 나타날 것으로 예상하시나요?

00:01:32.640 --> 00:01:38.480
저는 이 모델이
경제적 영향보다 더 똑똑하다고 생각합니다.

00:01:38.480 --> 00:01:44.720
응.  이건
지금 모델에 대한 가장 혼란스러운 점 중 하나입니다.

00:01:44.720 --> 00:01:52.400
그들이 평가에서 좋은 성적을 거두고 있다는 사실을 어떻게 조화시킬 수 있을까?

00:01:53.040 --> 00:01:57.680
평가 결과를 보면 "
꽤 어려운 평가구나"라고 생각하게 됩니다.  그들은 정말 잘하고 있어요

00:01:57.680 --> 00:02:04.080
.  하지만 경제적 영향은
극적으로 줄어든 듯합니다.   이

00:02:07.920 --> 00:02:12.240
모델이 한편으로는

00:02:12.240 --> 00:02:18.880
놀라운 일을 해내면서도 다른
한편으로는 어떤 상황에서는 두 번이나 반복되는 것은 어떻게 이해할 수 있을까요?

00:02:20.560 --> 00:02:24.560
예를 들어,
분위기 코딩을 사용하여 무언가를 한다고 가정해 보겠습니다.

00:02:24.560 --> 00:02:28.240
어떤 장소에 갔다가 벌레를 만났어요.
그러면 모델에게

00:02:28.240 --> 00:02:32.000
"버그를 수정해 주시겠어요?"라고 말하세요.
그리고 그 모델은 "맙소사, 정말 그렇군요

00:02:32.000 --> 00:02:35.760
. 버그가 생겼어요. 제가
고쳐드릴게요."라고 말합니다.  그리고 두 번째 버그가 발생합니다.

00:02:36.960 --> 00:02:40.080
그러면 "
두 번째 버그가 생겼어요"라고 말하면

00:02:40.080 --> 00:02:43.520
"맙소사, 어떻게 그럴 수 있었을까? 정말
맞는 말이야"라고 하면서

00:02:43.520 --> 00:02:52.000
첫 번째 버그를 다시 불러오는데, 두 버그 사이를 번갈아가며 사용할 수 있습니다
.  어떻게 가능한가?  확실하지는 않지만,

00:02:52.000 --> 00:03:02.080
뭔가 이상한 일이 일어나고 있다는 것을 암시합니다.
가능한 설명은 두 가지가 있습니다.  좀 더 기발한

00:03:02.080 --> 00:03:07.760
설명은 아마도 RL 훈련이
모델을 너무 단일 사고방식으로 만들고, 좁은 시야에

00:03:07.760 --> 00:03:17.680
집중하게 하며, 너무 무지하게 만들지만,
다른 면에서는 인식하게 만든다는 것입니다.

00:03:17.680 --> 00:03:25.040
이로 인해 그들은 기본적인 일도 할 수 없습니다.
하지만 또 다른 설명도 있습니다.

00:03:25.040 --> 00:03:31.440
사람들이 사전 학습을 하던 시절에는
어떤 데이터를 사용하여 학습할 것인가에 대한 질문에 대한

00:03:31.440 --> 00:03:41.040
답이 있었습니다. 왜냐하면 그 답이 전부였기 때문입니다.
사전 학습을 할 때는 모든 데이터가 필요합니다.

00:03:41.040 --> 00:03:44.960
그러니까
이 데이터인지 저 데이터인지 고민할 필요가 없습니다.

00:03:44.960 --> 00:03:48.560
하지만 사람들이 RL 훈련을 할 때는
생각해야 합니다.

00:03:48.560 --> 00:03:52.080
그들은 "좋아요, 우리는
이 문제에 대해 이런 종류의 RL 훈련을 하고,

00:03:52.080 --> 00:03:58.160
저 문제에 대해 저런 종류의 RL 훈련을 하고 싶습니다."라고 말합니다.
제가 들은 바에 따르면, 모든 회사에는

00:03:58.160 --> 00:04:02.800
새로운 RL 환경을 만들어내고
이를 훈련 믹스에 추가하는 팀이 있습니다.

00:04:02.800 --> 00:04:06.080
문제는, 글쎄, 그게 뭐냐는 거예요.
자유도에는 여러 가지가 있습니다.

00:04:06.080 --> 00:04:10.080
제작할 수 있는 RL 환경의 종류는 정말 다양합니다.

00:04:12.720 --> 00:04:17.360
여러분이 할 수 있는 한 가지는, 제 생각에 이건 의도
치 않게 행해지는 일인데,

00:04:17.360 --> 00:04:24.880
사람들이 평가에서 영감을 얻는다는 것입니다.
"모델을

00:04:24.880 --> 00:04:28.720
출시할 때 정말 좋은 성능을 내기를 바랍니다.
평가 결과도 훌륭해 보이기를 바랍니다.

00:04:28.720 --> 00:04:32.640
이 작업에 도움이 될 수 있는 RL 훈련은 무엇일까요?"라고 말씀하실 수 있습니다.   저는 그것이 실제로 일어나는 일이라고

00:04:33.840 --> 00:04:39.040
생각하고, 그것이 지금 일어나고 있는 일의
많은 부분을 설명할 수 있을 것이라고 생각합니다.

00:04:39.040 --> 00:04:44.720
이것을
모델이 실제로 부적절하다는 일반화와 결합하면

00:04:44.720 --> 00:04:48.640
우리가 보고 있는 많은 것, 즉

00:04:48.640 --> 00:04:56.320
평가 성능과 실제 현실 성능 간의 단절을 설명할 수 있는 잠재력이 있습니다.
이는

00:04:56.320 --> 00:05:03.680
오늘날 우리가 이해하지 못하는 것이며, 우리가 의미하는 바이기도 합니다.  저는

00:05:03.680 --> 00:05:07.920
인간 연구자들이
평가에만 너무 집중해서 해킹을 시도하는 것이 진정한 보상이라는 이 아이디어를 좋아합니다.

00:05:12.960 --> 00:05:18.880
방금 지적하신 내용을 이해하거나 생각해 볼 수 있는 방법이 두 가지 있다고 생각합니다.
첫 번째는

00:05:18.880 --> 00:05:23.600
코딩 경연대회에서 초인적인 능력을 발휘한다고 해서
모델이 자동으로 세련되게

00:05:23.600 --> 00:05:30.080
변하거나
코드베이스를 개선하는 방법에 대한 더 나은 판단을 내릴 수 있는 것은 아니라는 것입니다. 그렇다면

00:05:35.120 --> 00:05:38.560
코딩 경연대회에서 최고의 성과를 내는 것만을 테스트하는 것이 아니라 환경 세트를 확장해야 합니다.
또한

00:05:38.560 --> 00:05:44.400
X나 Y, Z에 대한 최상의 종류의 애플리케이션을 만들 수 있어야 합니다.
또 다른 하나는, 아마도 당신이 암시하고 있는 것이겠지만,

00:05:44.400 --> 00:05:50.320
"
애초에

00:05:50.320 --> 00:05:54.400
코딩 경연대회에서 초인이 되는 것이 당신을
더 품위 있는 프로그래머로 만들어주지 않는 이유가 무엇인가?"라고 말하는 것입니다.

00:05:54.400 --> 00:05:59.280
아마도 해야 할 일은 환경의
양과 다양성을 계속 늘리는 것이 아니라

00:05:59.280 --> 00:06:05.200
,
한 환경에서 배우고

00:06:05.200 --> 00:06:12.560
다른 환경에서 성과를 향상시킬 수 있는 접근 방식을 찾아내는 것일 것입니다.
도움이 될 만한 인간에 대한 비유가 있습니다.   당신이 언급한 대로

00:06:14.160 --> 00:06:18.160
경쟁 프로그래밍의 경우를 살펴보겠습니다
.  학생이 두 명 있다고 가정해 보겠습니다

00:06:18.160 --> 00:06:24.240
.  그중 한 명은
최고의 경쟁력 있는 프로그래머가 되고 싶다고 결심하고

00:06:24.240 --> 00:06:31.200
그 분야에 10,000시간을 투자하기로 했습니다.
그들은 모든 문제를 풀고, 모든

00:06:31.200 --> 00:06:39.920
증명 기술을 암기하고,
모든 알고리즘을 빠르고 정확하게 구현하는 데 매우 능숙할 것입니다.   그렇게 하여

00:06:40.640 --> 00:06:46.240
그들은 최고 중 하나가 되었습니다.
두 번째 학생은 "아,

00:06:46.240 --> 00:06:50.400
경쟁 프로그래밍은 멋진 거야."라고 생각했습니다.
어쩌면 그들은 100시간, 그보다

00:06:50.400 --> 00:06:54.640
훨씬 적은 시간 동안 연습했을지도 모르지만, 그래도 정말 좋은 성과를 거두었을 수도 있습니다.  나중에 경력에서
더 나은 성과를 거둘 사람은 누구라고 생각하시나요

00:06:54.640 --> 00:06:56.800
?  두
번째.

00:06:56.800 --> 00:07:00.560
오른쪽.  저는 기본적으로 그런 일이 일어나고 있다고 생각해요.
모델은

00:07:00.560 --> 00:07:04.400
첫 번째 학생과 훨씬 더 비슷하지만, 그 이상입니다.
그러면 모델은

00:07:04.400 --> 00:07:10.080
경쟁 프로그래밍에 적합해야 하므로 지금까지 나온
모든 경쟁 프로그래밍 문제를 다루어 보겠습니다.

00:07:10.080 --> 00:07:13.120
그리고
좀 더 경쟁력 있는

00:07:13.120 --> 00:07:18.480
프로그래밍 문제를 만들기 위해 데이터 증강을 수행하고 이를 바탕으로 학습해 보겠습니다.
이제 당신은 뛰어난 경쟁력을 갖춘 프로그래머를 갖게 되었습니다.

00:07:18.480 --> 00:07:27.120
이 비유는 더 직관적이라고 생각합니다.
네, 알겠습니다. 그렇게 잘 훈련되었다면 모든

00:07:27.120 --> 00:07:32.480
알고리즘과
증명 기술을 손쉽게 사용할 수 있겠죠.

00:07:32.480 --> 00:07:36.320
그리고 이
수준의 준비를 하면

00:07:36.320 --> 00:07:42.160
반드시 다른 것에 일반화되지는 않을 것이라는 게 더 직관적입니다.
그렇다면

00:07:42.160 --> 00:07:48.160
두 번째 학생이
100시간의 미세 조정을 하기 전에 하는 일과 어떤 비유를 들 수 있을까요?

00:07:48.160 --> 00:07:56.480
나는 그들이 그것을 가지고 있다고 생각한다.  "그것"
요소.  제가 학부생이었을 때, 저와

00:07:56.480 --> 00:08:01.280
함께 공부한 학생 중에 이런 학생이 있었던 걸 기억합니다. 그래서 그런 학생이 있다는 걸 알고 있습니다.

00:08:01.840 --> 00:08:06.160
"그것"을 사전 훈련과 구별하는 게 흥미롭다고 생각해요.   사전 학습에서 데이터를 선택할 필요가 없다는 말을

00:08:06.160 --> 00:08:10.240
이해하는 한 가지 방법은

00:08:10.240 --> 00:08:15.040
실제로
10,000시간의 연습과 크게 다르지 않다는 것입니다.

00:08:15.040 --> 00:08:20.320
10,000시간 분량
의 연습 자료가

00:08:20.320 --> 00:08:25.120
사전 훈련 배포 어딘가에 이미 포함되어 있기 때문에 무료로 제공되는 것입니다.
하지만

00:08:25.120 --> 00:08:28.800
사전 훈련에서 실제로 일반화가 그렇게 많이 이루어지지 않았다고 말할 수도 있습니다.
사전 학습에는 많은 데이터가 있지만, 반드시

00:08:28.800 --> 00:08:33.360
RL보다 일반화가 더 나은 것은 아닙니다.
사전 학습의 주요 장점은

00:08:33.360 --> 00:08:40.160
다음과 같습니다. A, 데이터가 매우 방대하고 B,

00:08:40.160 --> 00:08:45.680
사전 학습에 어떤 데이터를 넣어야 할지 고민할 필요가 없습니다.
매우 자연스러운 데이터이고

00:08:45.680 --> 00:08:54.800
사람들이 하는 일, 즉
사람들의 생각과 많은 특징이 포함되어 있습니다.   마치

00:08:54.800 --> 00:09:01.120
사람들이 텍스트에 투사한 전 세계와 같으며, 사전 학습은

00:09:01.120 --> 00:09:08.960
엄청난 양의 데이터를 사용하여 이를 포착하려고 시도합니다.
사전 학습은

00:09:08.960 --> 00:09:17.760
모델이 사전 학습 데이터에 의존하는 방식을 이해하기 어렵기 때문에 추론하기가 매우 어렵습니다.

00:09:17.760 --> 00:09:23.600
모델이 실수를 할 때마다, 그것은
우연히

00:09:23.600 --> 00:09:30.720
사전 학습 데이터에서 뒷받침되지 않기 때문일 수 있을까요?  "
사전 훈련을 통한 지원"은 아마도 느슨한 용어일 것입니다.

00:09:30.720 --> 00:09:33.680
여기에 더 유용한 내용을 추가할 수 있을지 모르겠습니다.   저는

00:09:36.000 --> 00:09:38.400
사전 훈련과 유사한 인간적 사례가 없다고 생각합니다.

00:09:39.440 --> 00:09:43.600
다음은
인간의 사전 훈련에 대한 비유를 사람들이 제안한 것입니다.   왜 그들이 잠재적으로 틀렸는지에 대한

00:09:43.600 --> 00:09:47.920
여러분의 생각이 궁금합니다
.

00:09:47.920 --> 00:09:54.160
하나는 사람이

00:09:54.160 --> 00:10:00.320
반드시 경제적으로 생산적이지는 않지만

00:10:00.320 --> 00:10:07.200
세상을 더 잘 이해하게 해주는 일을 하는 인생의 처음 18년, 15년, 13년을 생각해 보는 것입니다.
또 다른 방법은 진화를

00:10:07.200 --> 00:10:14.480
30억 년 동안의 일종의 탐색으로 생각하는 것입니다.
그러면 인간의 일생이라는 결과가 나옵니다.   이 두 가지가

00:10:14.480 --> 00:10:17.360
사전 훈련과 유사하다고 생각하시는지 궁금합니다.

00:10:18.000 --> 00:10:22.800
사전 훈련이 아니라면, 평생 인간의 학습이 어떤 것인지 어떻게 생각하시나요?

00:10:22.800 --> 00:10:28.800
저는 이 둘과 사전 훈련 사이에 몇 가지 유사점이 있다고 생각하고
, 사전 훈련은 이

00:10:28.800 --> 00:10:32.000
둘의 역할을 하려고 노력합니다.
하지만 저는 몇 가지

00:10:32.000 --> 00:10:38.240
큰 차이점도 있다고 생각합니다.
사전 학습 데이터의 양은 정말 엄청

00:10:38.240 --> 00:10:41.520
납니다.
예.

00:10:41.520 --> 00:10:47.600
인간은
사전 훈련 데이터의 극히 일부만 가지고 15년을 보낸 후에도

00:10:47.600 --> 00:10:50.800
훨씬 적은 것을 알게 됩니다.
하지만 그들이 무엇을 알고 있든,

00:10:50.800 --> 00:10:57.120
그들은 어떻게든 훨씬 더 깊이 알고 있습니다.
그 나이가 되면 당신은

00:10:57.120 --> 00:11:02.240
AI가 저지르는 실수를 저지르지 않을 겁니다.  또 다른
것이 있습니다.

00:11:02.240 --> 00:11:07.600
진화와 비슷한 것이 아닐까요?  답은 아마도 그럴 수도 있다.  하지만 이 경우에는
진화론이 실제로 우위를 가질 수도 있다고 생각합니다.   저는

00:11:08.880 --> 00:11:19.680
이 사건에 대해 읽은 적이 있습니다.
신경과학자들이 뇌에 대해 배울 수 있는 한 가지 방법은

00:11:19.680 --> 00:11:25.920
뇌의 여러 부분이 손상된 사람들을 연구하는 것입니다.

00:11:26.960 --> 00:11:30.640
어떤 사람들은 상상할 수 있는 가장 이상한 증상을 보입니다
.  사실 정말,

00:11:30.640 --> 00:11:35.680
정말 흥미롭죠.  관련 있는 사례가 하나 생각납니다
.

00:11:35.680 --> 00:11:44.000
저는
뇌 손상, 뇌졸중 또는 사고로 인해

00:11:44.000 --> 00:11:51.520
감정 처리가 불가능해진 사람에 대한 기사를 읽었습니다.
그래서 그는 더 이상 감정을 느끼지 않게 되었습니다.

00:11:54.800 --> 00:11:58.240
그는 여전히 매우 유창한 말을 했고
작은 퍼즐도 풀 수 있었으며

00:11:58.240 --> 00:12:03.520
시험에서도 괜찮은 성적을 거두었습니다.
하지만 그는 아무런 감정도 느끼지 못했습니다.  그는 슬픔을 느끼지 않았고,

00:12:03.520 --> 00:12:10.000
분노도 느끼지 않았고, 활력도 느끼지 않았습니다.
그는 어떻게 된 일인지 전혀 결정을 내리는 데 매우 서툴러졌습니다

00:12:10.000 --> 00:12:12.240
.

00:12:12.240 --> 00:12:17.120
어떤 양말을 신을지 결정하는 데 몇 시간이 걸렸습니다.
그는 매우 나쁜 재정적 결정을 내릴 것입니다.   본질적으로 우리를 실행 가능한 행위자로 만드는 데 있어서

00:12:23.120 --> 00:12:34.240
우리의 내재된 감정의 역할에 대해 무엇을 말해 주는가
?

00:12:34.240 --> 00:12:41.680
사전 훈련에 대한 질문에 답하자면, 사전 훈련을 통해
모든 것을 얻는 데 능숙하다면

00:12:41.680 --> 00:12:46.480
그것도 얻을 수 있을 것입니다.
하지만 그건 일종의 것 같습니다...

00:12:50.960 --> 00:12:56.160
글쎄요,
사전 훈련을 통해 그것을 얻는 것이 가능할 수도 있고 불가능할 수도 있습니다.

00:12:56.160 --> 00:13:04.800
저게 뭐에요"?  분명히 감정만이 직접적인 것은 아닙니다
.  이는

00:13:04.800 --> 00:13:12.080
어떤 결정에 대한 최종 보상이 무엇인지 알려주는 가치 함수와 비슷한 것 같습니다.

00:13:12.080 --> 00:13:15.200
당신은 그것이
암묵적으로 사전 훈련에서 나온 것이 아니라고 생각하시나요?   그럴 수 있다고

00:13:15.200 --> 00:13:19.440
생각해요.  그냥
100% 확실하지 않다고 말하는 거예요.

00:13:20.400 --> 00:13:26.320
하지만 그게 뭔데요?  당신은 감정에 대해 어떻게 생각하시나요?
ML에서 감정에 대한 비유는 무엇입니까?

00:13:26.320 --> 00:13:31.440
어떤 종류의 가치 함수여야 합니다.
하지만 저는 ML에 대한 좋은 비유가 없다고 생각합니다.

00:13:31.440 --> 00:13:36.480
왜냐하면 지금 당장 가치 함수는
사람들이 하는 일에서 그다지 중요한 역할을 하지 않기 때문입니다.   원한다면

00:13:36.480 --> 00:13:40.000
청중에게 가치 함수가 무엇인지 정의하는 것이 좋을 수도 있습니다
.

00:13:40.560 --> 00:13:50.880
물론입니다. 저는 기꺼이 그렇게 하겠습니다.
사람들이 강화 학습을 할 때, 즉 지금

00:13:50.880 --> 00:13:56.560
강화 학습이 이루어지는 방식으로
, 사람들은 어떻게 에이전트를 훈련시킬까요?

00:13:56.560 --> 00:14:00.160
신경망이 있고,
문제를 주고,

00:14:00.160 --> 00:14:03.440
모델에게 "해결해라"라고 말합니다.
이 모델은 아마도 수천,

00:14:03.440 --> 00:14:09.040
수십만 개의 행동이나 생각 또는
무언가를 취한 다음 해결책을 도출합니다.

00:14:09.040 --> 00:14:14.960
해결책은 등급이 매겨집니다.  그리고 점수는

00:14:14.960 --> 00:14:23.200
궤적의 모든 단일 동작에 대한 훈련 신호를 제공하는 데 사용됩니다.
즉,

00:14:23.200 --> 00:14:29.120
오랜 시간이 걸리는 작업을 하고 있다면, 즉
해결하는 데 오랜 시간이 걸리는 작업을 훈련하고 있다면, 제안된 해결책을 생각해 낼 때

00:14:29.120 --> 00:14:33.440
까지 전혀 학습이 이루어지지 않을 것입니다
.

00:14:33.440 --> 00:14:39.760
강화 학습은 이렇게 순진하게 이루어집니다.
o1, R1은 표면적으로 이렇게 이루어집니다.

00:14:40.800 --> 00:14:48.240
가치 함수는
"항상은 아니지만 가끔은

00:14:48.240 --> 00:14:52.400
당신이 잘하고 있는지, 아니면 잘 못하고 있는지 알려줄 수도 있을 것 같아요."와 같은 의미입니다.
가치 함수의 개념은

00:14:52.400 --> 00:14:56.960
어떤 영역에서는 다른 영역보다 더 유용합니다.
예를 들어, 체스를 두다가

00:14:56.960 --> 00:15:01.440
말을 하나 잃으면, 내가 실수한 거예요.  게임
전체를 플레이하지 않아도

00:15:01.440 --> 00:15:08.400
내가 방금 한 일이 나쁘다는 걸 알 수 있고,
따라서 그 전에 한 일도 나쁘다는 걸 알 수 있습니다.

00:15:08.960 --> 00:15:14.800
가치 함수를 사용하면
마지막까지 기다리는 시간을 단축할 수 있습니다.

00:15:19.040 --> 00:15:23.040
여러분이 어떤 종류의
수학적 문제나 프로그래밍 문제를 다루고 있다고 가정해 보겠습니다.

00:15:23.040 --> 00:15:26.800
그리고 여러분은
특정한 해결책이나 방향을 모색하고 있습니다.   예를 들어

00:15:26.800 --> 00:15:34.320
, 수천 단계를 생각한 후,
이 방향은 유망하지 않다는 결론을 내렸습니다.

00:15:34.320 --> 00:15:39.760
이것을 결론짓는 순간, 당신은 이 길을 추구하기로 결정했을 때
이미 1,000번의 타임스텝 전에 보상 신호를 받았을 수 있습니다

00:15:39.760 --> 00:15:43.520
.   당신은 실제로 해결책을 제시하기도 전에

00:15:43.520 --> 00:15:49.600
"다음에 비슷한 상황이 생기면 이 길을 가면 안 된다"고 말합니다

00:15:49.600 --> 00:15:56.480
.
DeepSeek R1 논문에서는

00:15:56.480 --> 00:16:02.640
궤적 공간이 너무 넓어서

00:16:02.640 --> 00:16:08.800
중간 궤적과 값에서 매핑을 학습하기 어려울 수 있다고 했습니다.
그리고 예를 들어 코딩을 할 때

00:16:08.800 --> 00:16:12.720
잘못된 생각이 들 수도 있고, 다시
돌아가서 뭔가를 바꿀 수도 있습니다.

00:16:12.720 --> 00:16:15.840
이건
딥러닝에 대한 믿음이 부족한 것 같습니다.

00:16:16.800 --> 00:16:23.520
물론 어려울 수도 있지만,
딥러닝으로 할 수 없는 일은 없습니다.

00:16:23.520 --> 00:16:32.640
저는 가치 함수가
유용할 것으로 기대하며, 이미 사용 중이더라도

00:16:32.640 --> 00:16:37.040
앞으로 사용될 것이라고 확신합니다.

00:16:37.040 --> 00:16:47.680
감정적 중심이 손상된 사람에 대해 제가 암시한 것은, 아마도

00:16:47.680 --> 00:16:55.360
인간의 가치 기능이
진화에 의해 굳건히 코드화된 어떤 중요한 방식으로 감정에 의해 조절된다는 것을 암시하는 것일 수도 있습니다.

00:16:55.360 --> 00:17:00.400
그리고 아마도
사람들이 세상에서 효과적으로 살아가는 데는 그것이 중요할 것입니다.   제가

00:17:00.400 --> 00:17:02.640
묻고 싶었던 건 바로 그거였어요.

00:17:02.640 --> 00:17:06.160
가치 함수의 감정에는 정말 흥미로운 점이 있는데,

00:17:06.160 --> 00:17:16.160
이해하기가 꽤 간단하면서도 이렇게 많은 유용성을 가지고 있다는 점이 인상적이라는 점입니다.

00:17:16.160 --> 00:17:25.600
저는 두 가지 답변을 가지고 있습니다.  저는
우리가 배우는 것과

00:17:25.600 --> 00:17:30.240
우리가 이야기하는 것, 우리가
이야기하는 AI의 종류와 비교하면 감정은 상대적으로 단순하다는 데 동의합니다.

00:17:31.120 --> 00:17:35.680
심지어 너무 간단해서 사람이
이해할 수 있는 방식으로 매핑할 수도 있을 겁니다.   그렇게 하면

00:17:35.680 --> 00:17:40.960
멋질 것 같아요.
하지만 유용성 측면에서 보면

00:17:40.960 --> 00:17:49.360
복잡성과 견고성 사이에 상충 관계가 있는 것 같아요. 즉, 복잡한

00:17:49.360 --> 00:17:58.960
것은 매우 유용할 수 있지만, 간단한 것도
매우 광범위한 상황에서 매우 유용할 수 있습니다.

00:18:00.240 --> 00:18:06.720
우리가 보고 있는 것을 해석하는 한 가지 방법은
우리의 감정이 주로

00:18:06.720 --> 00:18:13.200
포유류 조상으로부터 진화한 다음
우리가 호미니드였을 때 약간 미세하게 조정되었다는 것입니다.

00:18:13.200 --> 00:18:19.760
우리는 포유류에게는 없는 상당한 양의 사회적 감정을 가지고 있습니다
.  하지만

00:18:19.760 --> 00:18:24.720
그다지 정교하지는 않습니다.  그리고 그들은
정교하지 않기 때문에 우리가 살고 있는 세상과는 매우 다른 세상에서 우리에게 매우 잘 맞습니다

00:18:24.720 --> 00:18:28.480
. 사실,

00:18:28.480 --> 00:18:32.800
그들도 실수를 합니다.  예를 들어, 우리의
감정은... 사실, 잘 모르겠어요.

00:18:32.800 --> 00:18:39.760
배고픔도 감정으로 볼 수 있나요?  논란의 여지가 있습니다.
하지만 저는 예를 들어, 우리의 직관적인

00:18:39.760 --> 00:18:49.120
배고픔의 느낌은 이
풍부한 음식이 있는 세상에서 우리를 올바르게 인도하는 데 성공하지 못하고 있다고 생각합니다.

00:18:50.000 --> 00:18:56.240
사람들은
데이터 확장, 매개변수 확장, 컴퓨팅 확장에 대해 이야기해 왔습니다.

00:18:56.240 --> 00:18:58.000
확장에 대해 생각하는 보다 일반적인 방법이 있습니까?

00:18:58.000 --> 00:19:10.480
다른 스케일링 축은 무엇입니까?
제 생각에는 맞는 관점이 하나 있습니다.

00:19:12.160 --> 00:19:16.320
ML이 작동하던 방식은
사람들이 그저

00:19:16.320 --> 00:19:28.240
사물을 조작하고 흥미로운 결과를 얻으려고 시도하는 것이었습니다.
과거에도 그런 일이 일어났습니다.  그러다가

00:19:28.240 --> 00:19:39.120
확장에 대한 통찰력이 생겨났습니다.  확장법, GPT-3,
그리고 갑자기 모든 사람이 확장해야 한다는 것을 깨달았습니다.

00:19:40.400 --> 00:19:47.120
이는 언어가
사고에 어떤 영향을 미치는지 보여주는 예입니다.  "스케일링"은 단

00:19:47.120 --> 00:19:51.120
하나의 단어일 뿐이지만,
사람들에게 무엇을 해야 할지 알려주기 때문에 매우 강력한 단어입니다.

00:19:51.120 --> 00:19:57.280
그들은 "규모를 확장해 보자"고 말합니다.
그러면, 우리는 무엇을 확장하고 있나요?

00:19:57.280 --> 00:20:02.240
사전 훈련이 확장의 핵심이었습니다.
그것은 특별한 스케일링 레시피였습니다.

00:20:02.240 --> 00:20:08.000
사전 훈련의 가장 큰 혁신은
이 요리법이 훌륭하다는 것을 깨달은 것입니다.

00:20:08.000 --> 00:20:14.400
"어떤 계산
과 데이터를 특정 크기의 신경망에 섞으면

00:20:14.400 --> 00:20:19.600
결과가 나올 겁니다. 그냥 레시피를 확장하면
더 나을 거라는 걸 알게 될 겁니다

00:20:19.600 --> 00:20:26.560
."라고 말할 수도 있겠죠.  이것도 훌륭하네요.
기업들은 이를 매우

00:20:26.560 --> 00:20:34.800
낮은 위험으로 자원을 투자할 수 있는 방법으로 여기기 때문에 선호합니다.  연구에
자원을 투자하는 것은 훨씬 더 어렵습니다

00:20:34.800 --> 00:20:39.760
.  비교해 보세요.  연구를 한다면
"연구자 여러분, 나가서

00:20:39.760 --> 00:20:45.440
연구해서 뭔가를 만들어 보세요"라고 해야지,
더 많은 데이터와 더 많은 컴퓨팅을 얻는 게 아닙니다.

00:20:45.440 --> 00:20:52.400
사전 훈련을 하면 뭔가를 얻을 수 있다는 걸 알고 있죠.
실제로,

00:20:54.640 --> 00:21:00.160
트위터에서 일부 사람들이 말한 다양한 내용을 바탕으로 볼 때,
쌍둥이자리는

00:21:00.160 --> 00:21:02.560
사전 훈련에서 더 많은 것을 얻을 수 있는 방법을 찾은 것 같습니다.
하지만 어느 시점에서는

00:21:02.560 --> 00:21:06.480
사전 학습 데이터가 부족해질 것입니다.
데이터는 매우 한정적입니다.  그

00:21:06.480 --> 00:21:11.280
다음에는 무엇을 하나요?  이전에 해왔던 것과는
다른 방법으로 강화된 사전 훈련을 하거나

00:21:11.280 --> 00:21:15.920
,
RL을 하거나, 아니면 다른 것을 할 수도 있습니다.

00:21:15.920 --> 00:21:20.320
하지만 이제 컴퓨팅은 거대해졌습니다. 컴퓨팅은
이제 매우 거대해졌습니다. 어떤 의미에서 우리는

00:21:20.320 --> 00:21:24.160
연구 시대로 돌아왔습니다.
어쩌면 다른 표현도 있을 수 있겠네요.

00:21:24.160 --> 00:21:31.280
2012년부터
2020년까지는 연구의 시대였습니다.

00:21:31.280 --> 00:21:35.920
이제 2020년부터 2025년까지는
확장의 시대였습니다. 더하기 또는 빼기,

00:21:35.920 --> 00:21:39.440
그 해에 오차 막대를 추가해 봅시다. 왜냐하면
사람들이 "이건 놀랍습니다.

00:21:39.440 --> 00:21:45.280
더 확장해야 합니다. 계속 확장하세요."라고 말하기 때문입니다.  한 단어로 말하면:
확장입니다.  하지만 지금은 규모가 너무 커졌어요.

00:21:46.320 --> 00:21:53.440
그 믿음은 정말 "아, 정말 크잖아. 하지만
100배 더 많았다면 모든 게 달라졌을 텐데"라는 것인가요?

00:21:53.440 --> 00:21:58.000
분명 다를 겁니다.
하지만

00:21:58.000 --> 00:22:04.560
규모를 100배로 확대하면 모든 것이 변할 것이라는 믿음일까요?
저는 그게 사실이라고 생각하지 않아요.  그래서

00:22:04.560 --> 00:22:10.400
다시 연구 시대로 돌아왔습니다. 다만 대형 컴퓨터만 있을 뿐이죠.
정말 흥미로운 표현이네요.

00:22:10.400 --> 00:22:12.560
하지만 방금
당신이 던진 질문을 다시 한 번 물어보겠습니다.

00:22:12.560 --> 00:22:16.560
우리는 무엇을 확장하고 있으며,
레시피를 갖는 것은 무엇을 의미할까요?

00:22:23.680 --> 00:22:27.920
사전 훈련에서 물리 법칙처럼 보이는 매우 깔끔한 관계가 존재했던 적이 있는지 모르겠습니다.
데이터 또는

00:22:27.920 --> 00:22:33.920
컴퓨팅 또는 매개변수와 손실 사이에 거듭제곱 법칙이 있었습니다.

00:22:33.920 --> 00:22:38.640
우리가 추구해야 할 관계는 어떤 종류일까요? 그리고
이 새로운 레시피가 어떤 모습일지 어떻게 생각해야 할까요?

00:22:40.240 --> 00:22:48.000
우리는 이미 한 가지
유형의 스케일링에서 다른 유형의 스케일링으로,

00:22:48.000 --> 00:22:56.800
사전 학습에서 RL로의 전환을 목격했습니다.  이제 사람들은
RL을 확장하고 있습니다.  이제 사람들이 트위터에서 말하는 내용을 바탕으로 볼 때,

00:22:56.800 --> 00:23:01.280
RL은 사전 학습보다 더 많은 컴퓨팅을 사용합니다.
왜냐하면 RL은

00:23:01.280 --> 00:23:07.840
실제로 상당한 양의 컴퓨팅을 소모할 수 있기 때문입니다.
매우 긴 롤아웃을 수행하므로

00:23:07.840 --> 00:23:11.360
해당 롤아웃을 생성하는 데 많은 컴퓨팅이 필요합니다.
그러면

00:23:11.360 --> 00:23:15.760
롤아웃당 학습량이 상대적으로 적어서
실제로 많은 컴퓨팅을 사용할 수 있습니다.

00:23:21.680 --> 00:23:27.280
저는 그것을 스케일링이라고 부르지도 않을 것입니다.
저는 "이봐요, 뭐 하고 계세요? 지금

00:23:27.280 --> 00:23:31.840
하고 있는 일이 가장
생산적인 일인가요? 컴퓨터를 사용하는

00:23:31.840 --> 00:23:36.000
더 생산적인 방법을 찾을 수 있나요
?"라고 묻곤 했습니다.

00:23:36.000 --> 00:23:39.520
우리는
이전에 가치 함수 사업에 대해 논의했습니다.

00:23:39.520 --> 00:23:43.920
사람들이 가치
함수에 능숙해지면, 자신의

00:23:44.720 --> 00:23:50.320
자원을 보다 생산적으로 활용할 수 있을 것입니다.  모델을 훈련하는
완전히 다른 방법을 찾는다면

00:23:50.320 --> 00:23:56.240
, "이게
확장 가능한 건가요, 아니면 그냥 리소스를 사용하는 건가요?"라고 말할 수 있을 겁니다.

00:23:56.240 --> 00:23:59.680
조금 모호해지는 것 같아요.  그 당시
사람들이

00:23:59.680 --> 00:24:03.600
연구의 시대에 있었을 때,
"이것과 이것, 이것을 시도해 보자.

00:24:03.600 --> 00:24:07.680
저것과 저것, 저것을 시도해 보자.
오, 봐요, 흥미로운 일이 일어나고 있어요."라고 했던 것과 같은 의미입니다.   저는 다시 그런 일이 일어날 것이라고

00:24:07.680 --> 00:24:12.480
생각합니다.
우리가 연구 시대로 돌아간다면,

00:24:12.480 --> 00:24:17.040
한 걸음 물러나서,
우리가 가장 생각해야 할 레시피의 부분은 무엇일까요?

00:24:17.040 --> 00:24:21.200
가치 함수라고 하면 사람들은
이미 현재의 방법을 시도하고 있지만,

00:24:21.200 --> 00:24:24.720
LLM을 판사로 둔다는 식의 방법을 시도하고 있는 거죠.
가치 함수라고 할 수도 있지만,

00:24:24.720 --> 00:24:26.880
훨씬 더 근본적인 것을 염두에 두고 계신 것 같습니다.

00:24:29.920 --> 00:24:35.920
사전 훈련 자체를 재고해야 할까요?
그 과정의 끝에 단계를 더 추가하는 게 아니라요?

00:24:38.240 --> 00:24:41.600
가치 함수에 대한 논의는
흥미로웠습니다.

00:24:41.600 --> 00:24:48.320
저는 가치
함수가 강화 학습을 더욱

00:24:48.320 --> 00:24:55.440
효율적으로 만들어 줄 것이라고 생각하며, 그것이 차이를 만들어낸다고 생각합니다.
하지만 값 함수로 할 수 있는 일은 값 함수

00:24:55.440 --> 00:25:02.080
없이도 할 수 있다고 생각합니다. 다만 속도가 좀 더 느릴 뿐입니다.
제 생각에 가장 근본적인 것은

00:25:02.080 --> 00:25:08.560
이러한 모델이
사람보다 훨씬 더 나쁘게 일반화한다는 것입니다.  너무나

00:25:08.560 --> 00:25:18.480
당연한 일이죠.  그건 아주 근본적인 것 같아요.
그러니까 핵심은 일반화입니다.  두 개의

00:25:18.480 --> 00:25:24.160
하위 질문이 있습니다.  샘플 효율성에 대한 질문이 하나 있습니다
.

00:25:24.160 --> 00:25:29.520
이 모델이 인간보다 훨씬 더 많은 데이터를 학습하는 데 왜 그렇게 많은 데이터가 필요할까요?
두 번째 질문이 있습니다.  데이터 양을 제외하더라도

00:25:29.520 --> 00:25:35.840
,
우리가 원하는 것을 모델에게 가르치는 것이 사람에게 가르치는 것보다 왜 그렇게 어려울까요?

00:25:37.360 --> 00:25:43.920
인간의 경우,
검증 가능한 보상이 반드시 필요한 것은 아닙니다. 여러분은

00:25:43.920 --> 00:25:48.560
지금 많은 연구자들을 멘토링하고 있을 것이고,
그들과 대화하고, 여러분의

00:25:48.560 --> 00:25:52.560
코드를 보여주고, 여러분의 생각을 보여주고 있을 것입니다.
이를 통해 그들은 당신의

00:25:52.560 --> 00:25:56.800
사고방식과 그들이 어떻게 연구해야 하는지를 알게 됩니다.

00:25:56.800 --> 00:25:59.680
"좋아요, 이게 커리큘럼의 다음 부분이고
, 이제 이게 여러분의 커리큘럼의 다음 부분이에요

00:25:59.680 --> 00:26:06.080
. 아, 이 훈련은 불안정했어요."와 같이 검증 가능한 보상을 설정할 필요는 없습니다.
이런 엉성하고 맞춤형 과정은 없습니다.

00:26:06.640 --> 00:26:10.160
아마도 이 두 가지 문제는 실제로
어떤 면에서 관련이 있을 수 있지만, 저는 지속적인 학습과

00:26:10.160 --> 00:26:15.520
더 유사한 두 번째 사항
과 샘플 효율성과 유사한 첫 번째 사항을 탐구하는 데 관심이 있습니다

00:26:15.520 --> 00:26:22.720
.
실제로

00:26:22.720 --> 00:26:30.720
인간의 표본 효율성에 대한 가능한 설명 중 하나는
진화라고 생각할 수 있습니다.

00:26:31.760 --> 00:26:38.880
진화는 우리에게
가능한 가장 유용한 정보의 일부를 제공했습니다.

00:26:38.880 --> 00:26:45.040
시각, 청각, 운동과 같은 것들에 있어서

00:26:45.040 --> 00:26:55.120
진화가 우리에게 많은 것을 주었다는 강력한 증거가 있다고 생각합니다.
예를 들어, 인간의 손재주는 훨씬 뛰어납니다. 즉,

00:26:55.120 --> 00:27:00.480
로봇도
시뮬레이션에서 엄청난 양의 훈련을 받으면 손재주를 갖게 될 수 있습니다.

00:27:00.480 --> 00:27:04.560
하지만 현실 세계에서 로봇을 사람
처럼 빠르게 새로운 기술을 습득하도록 훈련시키는 것은

00:27:04.560 --> 00:27:10.560
매우 불가능해 보입니다.
여기서는 "아, 맞아요, 이동이죠.

00:27:10.560 --> 00:27:14.640
우리 조상들은 모두
뛰어난 이동력이 필요했어요, 다람쥐.

00:27:15.920 --> 00:27:19.760
그래서 이동력에 대해선 어쩌면
믿을 수 없을 만큼 선험적인 정보가 있을지도 몰라요."라고 말할 수 있을 겁니다.

00:27:19.760 --> 00:27:24.400
시각에 대해서도 같은 주장을 할 수 있습니다.
저는 얀 르쿤이

00:27:25.440 --> 00:27:30.800
아이들이 10
시간의 연습을 통해 운전을 배운다는 점을 지적한 것으로 생각하는데, 이는 사실입니다.

00:27:30.800 --> 00:27:35.280
하지만 우리의 비전은 정말 훌륭해요.
적어도 저는

00:27:35.280 --> 00:27:41.120
다섯 살 때로 기억합니다.  그 당시
저는 자동차에 대해 정말 관심이 많았습니다.   저는

00:27:41.120 --> 00:27:47.200
5살 때부터 이미 자동차에 대한 지식이 운전에 충분하다고 확신합니다.

00:27:47.200 --> 00:27:49.040
5살짜리 아이는 그렇게 많은 데이터를 볼 수 없습니다.

00:27:49.040 --> 00:27:53.200
당신은 대부분의 시간을 부모님 집에서 보내기
때문에 데이터 다양성이 매우 낮습니다.

00:27:53.200 --> 00:28:00.400
하지만 그것도 진화라고 할 수 있겠죠.
하지만 언어, 수학, 코딩 분야에서는 아마도 그렇지 않을 겁니다.

00:28:00.400 --> 00:28:04.720
그래도 모델보다는 나은 것 같습니다.
분명 모델은

00:28:04.720 --> 00:28:07.760
언어, 수학, 코딩 측면에서 일반 인간보다 뛰어납니다.
하지만 그들이

00:28:07.760 --> 00:28:12.160
평균적인 인간보다 학습 능력이 더 뛰어날까요?
오, 그렇죠.  네, 물론입니다.  제가

00:28:12.160 --> 00:28:18.320
말하고자 했던 것은 언어, 수학, 코딩,
특히 수학과 코딩은

00:28:18.320 --> 00:28:25.920
사람을 학습에 능숙하게 만드는 것이
복잡한 사전 지식이라기보다는 그보다 더

00:28:25.920 --> 00:28:30.880
근본적인 것이라는 것을 시사한다는 것입니다.  제가 이해했는지 잘 모르겠어요
.  왜 그래야 할까요

00:28:30.880 --> 00:28:35.120
?
그러면

00:28:35.120 --> 00:28:45.120
사람들이 어떤 종류의 뛰어난 신뢰성을 보이는 기술을 생각해 보자.
만약 그 기술이

00:28:45.120 --> 00:28:52.240
수백만 년, 수억 년 동안 우리 조상들에게 매우 유용했던 것이라면,

00:28:52.240 --> 00:29:00.720
인간이 그 기술에 능숙한 것은 진화의 결과일지도 모른다고 주장할 수 있을 것입니다.
우리에게는 선험적 지식이 있고, 그 선험적 지식이

00:29:00.720 --> 00:29:06.400
매우 명백하지 않은 방식으로 인코딩되어 있어서
우리를 그 기술에 능숙하게 만들어주기 때문입니다.

00:29:07.200 --> 00:29:16.560
하지만 사람들이 최근까지 존재하지 않았던 분야에서 뛰어난 능력, 신뢰성,
견고성, 학습 능력을 보인다면,

00:29:16.560 --> 00:29:23.760
이는 사람들이 더

00:29:23.760 --> 00:29:34.800
나은 머신 러닝을 갖추고 있을 가능성을 나타내는 것일 뿐입니다.  그것이
무엇인지 어떻게 생각해야 할까?

00:29:34.800 --> 00:29:41.200
ML 비유는 무엇입니까?  흥미로운
점이 몇 가지 있습니다.  샘플 수가 줄어듭니다.

00:29:41.200 --> 00:29:47.200
감독이 거의 없습니다.  자동차 운전을 배우는 아이
…아이들은 자동차 운전을 배우는 것이 아닙니다.

00:29:47.200 --> 00:29:55.600
자동차 운전을 배우는 청소년은 미리
만들어진 검증 가능한 보상을 받는 것이 아닙니다.

00:29:56.400 --> 00:30:01.760
이는
기계와 환경과의 상호 작용을 통해 이루어집니다.

00:30:02.720 --> 00:30:07.440
훨씬 적은 샘플이 필요합니다.
감독이 덜 된 것 같습니다.  더 튼튼해 보이시나요?

00:30:07.440 --> 00:30:12.400
훨씬 더 견고해졌습니다.
사람들의 강인함은 정말 놀랍습니다.   이 모든 일이 한꺼번에 일어나는 이유에

00:30:14.160 --> 00:30:18.320
대해 통일된 사고방식이 있나요
?   이런 것을 실현할

00:30:18.320 --> 00:30:23.920
수 있는 ML 비유는 무엇일까요
?

00:30:26.320 --> 00:30:33.680
여러분이 궁금해하시는 것 중 하나는
십대 운전자가 외부 교사의 도움 없이 어떻게 스스로 문제를 해결하고 경험을 통해 배울 수 있는가입니다.

00:30:33.680 --> 00:30:41.600
답은 그들이 가치 함수를 가지고 있기 때문입니다.

00:30:41.600 --> 00:30:46.800
그들은 또한
사람들 중에서도 매우 강력한 일반적인 감각을 가지고 있습니다.

00:30:50.480 --> 00:30:56.080
인간의 가치 함수가 무엇이든,
중독과 관련된 몇 가지 예외를 제외하면

00:30:56.080 --> 00:31:00.800
실제로 매우 강력합니다.

00:31:00.800 --> 00:31:07.520
운전을 배우는 청소년의 경우, 운전을 시작하면 자신이
어떻게 운전하는지

00:31:07.520 --> 00:31:13.200
, 얼마나 형편없는지, 얼마나 자신감이 없는지 즉시 알게 됩니다.
그리고 그들은 "좋아요."라고 말합니다.  그리고 물론,

00:31:13.200 --> 00:31:17.520
십대의 학습 속도는 정말 빠릅니다.
10시간이 지나면 사용할 수 있습니다.

00:31:17.520 --> 00:31:19.760
인간에게는 어떤
해결책이 있는 듯하지만,

00:31:20.800 --> 00:31:24.960
그들이 어떻게 그것을 하는지, 그리고 왜 그렇게 어려운지 궁금합니다.  이런 일이 가능하려면 모델을 훈련하는
방식을 어떻게 재구성해야 할까요

00:31:24.960 --> 00:31:27.520
?   정말

00:31:28.720 --> 00:31:37.200
좋은 질문이네요. 저도 이 질문에
대해 많은 의견을 가지고 있어요.

00:31:37.200 --> 00:31:43.200
하지만 안타깝게도 우리는
모든 머신 러닝 아이디어가 자유롭게 논의되는 세상에 살고 있지 않습니다.

00:31:43.200 --> 00:31:49.360
이것이 그 중 하나입니다.
아마 방법이 있을 거예요.

00:31:49.360 --> 00:31:54.240
나는 그것이 가능하다고 생각한다.
사람들이 그런다는 사실은

00:31:54.240 --> 00:31:57.840
그것이 가능하다는 증거라고 생각합니다.
하지만

00:31:57.840 --> 00:32:07.760
인간의 뉴런이 우리가 생각하는 것보다 더 많은 계산을 할 가능성이 있다는 점이 또 다른 방해 요소가 될 수 있습니다.

00:32:07.760 --> 00:32:13.760
만약 그게 사실이라면, 그리고 그것이 중요한
역할을 한다면, 상황은 더 어려울지도 모릅니다.

00:32:13.760 --> 00:32:20.080
하지만 그럼에도 불구하고, 저는 그것이 제가 의견을 갖고 있는
어떤 머신 러닝

00:32:20.080 --> 00:32:25.840
원리의 존재를 가리킨다고 생각합니다.
하지만 안타깝게도 상황상

00:32:25.840 --> 00:32:31.680
자세히 논의하기는 어렵습니다.
이 팟캐스트를 듣는 사람은 아무도 없어, 일리아.

00:32:32.560 --> 00:35:53.360
궁금해요.  우리가
연구의 시대로 돌아갔다고 한다면, 당신은 2012년부터 2020년까지 거기에 있었습니다.

00:35:55.840 --> 00:36:00.640
우리가 연구의 시대로 돌아간다면 지금은 어떤 분위기가 될까요?

00:36:00.640 --> 00:36:05.920
예를 들어, AlexNet 이후에도 실험을 실행하는
데 사용되는 컴퓨팅 양은

00:36:05.920 --> 00:36:12.240
계속 증가했으며,
프런티어 시스템의 크기도 계속 증가했습니다.

00:36:13.440 --> 00:36:18.640
당신은 지금의 연구 시대에도
여전히 엄청난 양의 컴퓨팅이 필요하다고 생각하시나요?   기록

00:36:19.760 --> 00:36:24.400
보관소로 돌아가서 오래된 논문을 읽어야 할 것 같나요?

00:36:28.000 --> 00:36:34.960
당신은 연구 분위기가 더 강했던 구글, 오픈AI, 스탠포드 같은
곳에 있었나요?

00:36:34.960 --> 00:36:38.720
우리는
지역 사회에서 어떤 종류의 일을 기대해야 합니까?

00:36:40.160 --> 00:36:49.600
스케일링 시대의 한 가지 결과는
스케일링이 방 안의 모든 공기를 빨아들였다는 것입니다.

00:36:53.120 --> 00:36:59.680
스케일링으로 인해 방 안의 공기가 모두 없어졌기 때문에
, 다른 사람들도 똑같이 하기 시작했습니다.

00:36:59.680 --> 00:37:05.840
우리는

00:37:05.840 --> 00:37:11.440
아이디어보다 기업이 훨씬 더 많은 세상에 살고 있습니다.
사실, 실리콘

00:37:11.440 --> 00:37:18.480
밸리에는 아이디어는
싸고 실행이 전부라는 말이 있습니다.

00:37:18.480 --> 00:37:25.280
사람들은 이런 말을 많이 하는데, 그 말에는 진실이 있습니다.
그런데 트위터에서 누군가가

00:37:25.280 --> 00:37:30.880
"아이디어가 그렇게 싸다면
왜 아무도 아이디어를 떠올리지 못하는 걸까?"라고 말하는 걸 봤어요.   저도 그게

00:37:30.880 --> 00:37:37.840
사실이라고 생각해요.
연구 진행 상황을

00:37:37.840 --> 00:37:47.200
병목 현상의 관점에서 생각해 보면, 병목 현상은 여러 가지가 있습니다.
그 중 하나는 아이디어이고, 다른 하나는 아이디어를

00:37:47.200 --> 00:37:52.240
현실로 구현하는 능력인데, 이는
컴퓨팅일 수도 있고 엔지니어링일 수도 있습니다.

00:37:52.240 --> 00:37:56.880
90년대로 돌아가서,
꽤 좋은 아이디어를 가진 사람들이 있다고 가정해 보겠습니다.

00:37:56.880 --> 00:38:01.120
그들에게 훨씬 더 큰 컴퓨터가 있다면,
그들의 아이디어가 실현 가능하다는 것을 보여줄 수 있을지도 모릅니다.

00:38:01.120 --> 00:38:05.200
하지만 그들은 그렇게 할 수 없었고,

00:38:05.200 --> 00:38:10.480
아무도 설득하지 못하는 아주 작은 시위만 할 수 있었습니다.  그래서
병목 현상은 컴퓨팅이었습니다.  그리고

00:38:10.480 --> 00:38:17.120
확장 시대에 들어서면서 컴퓨팅이 엄청나게 증가했습니다.
물론, 얼마나 많은 컴퓨팅이 필요한지에 대한 의문이 있지만

00:38:17.120 --> 00:38:26.640
컴퓨팅은 규모가 큽니다.
컴퓨팅이 너무 커서

00:38:26.640 --> 00:38:33.520
어떤 아이디어를 증명하는 데 그렇게 많은 컴퓨팅이 필요하다는 것이 분명하지 않습니다.

00:38:33.520 --> 00:38:40.640
비유를 하나 들어보겠습니다.  AlexNet은 두 개의 GPU를 기반으로 구축되었습니다.
그것이 여기에 사용된 총 컴퓨팅 양이었습니다.

00:38:40.640 --> 00:38:48.720
이 변압기는 8~64개의 GPU를 기반으로 제작되었습니다.  2017년 기준으로
단일 변압기 논문 실험에서

00:38:48.720 --> 00:38:57.200
64개 이상의 GPU가 사용된 적이 없는데, 이는
오늘날의 GPU 2개에 해당합니다.  ResNet이

00:38:57.200 --> 00:39:08.000
맞죠?  o1 추론이
세상에서 가장 컴퓨팅 집약적인 것은 아니라고 주장할 수도 있습니다.

00:39:08.000 --> 00:39:17.200
따라서 연구를 위해서는 확실히
어느 정도의 컴퓨팅이 필요하지만,

00:39:17.200 --> 00:39:22.160
연구를 위해 역대 최대 규모의 컴퓨팅이 필요하다는 것은 전혀 자명하지 않습니다.

00:39:22.160 --> 00:39:28.800
여러분은 주장할 수도 있고, 저는 그것이 사실이라고 생각합니다.
최고의 시스템을 구축하려면

00:39:31.360 --> 00:39:35.360
훨씬 더 많은 컴퓨팅이 필요하다는 것입니다.
특히 모든 사람이 동일한

00:39:35.360 --> 00:39:42.000
패러다임에 속해 있다면 컴퓨팅은
가장 큰 차별화 요소 중 하나가 됩니다.

00:39:46.400 --> 00:39:48.400
당신이 실제로 그곳에 있었기 때문에 그 역사를 묻는 거예요.

00:39:48.400 --> 00:39:51.680
실제로 무슨 일이 일어났는지 잘 모르겠어요.

00:39:51.680 --> 00:39:56.480
최소한의 컴퓨팅을 사용하여 이러한 아이디어를 개발하는 것이 가능해 보입니다.
하지만 변압기는

00:39:56.480 --> 00:39:59.840
곧바로 유명해지지는 않았습니다.
이는 모든 사람이 하기 시작한 일이 되었고, 점점 더 높은

00:40:04.320 --> 00:40:07.360
수준의 컴퓨팅에서 검증되었기 때문에 이를 바탕으로 실험하고 구축하기 시작했습니다.
옳은.

00:40:07.360 --> 00:40:13.840
그리고 SSI에 50개의 서로 다른 아이디어가 있다면, 다른 최첨단 연구소가 보유한 컴퓨팅 능력이 없다면
어떤 아이디어가 다음 변압기

00:40:13.840 --> 00:40:21.920
이고 어떤 아이디어가 취약한지 어떻게 알 수 있겠습니까
?   그 점에

00:40:22.960 --> 00:40:30.320
대해서는 제가 말씀드릴 수 있습니다.  간단히
말해서 SSI에 대해 언급하셨습니다.

00:40:30.320 --> 00:40:40.640
특히 우리의 경우,
SSI가 연구를 위해 보유한 컴퓨팅 용량은 실제로 그렇게

00:40:40.640 --> 00:40:45.200
작지 않습니다.  그 이유를 설명하고 싶습니다.  간단한 수학만으로도 우리가 연구에 사용하는
컴퓨팅 양이 사람들이 생각하는 것보다 훨씬 많은 이유를 설명할 수 있습니다

00:40:45.200 --> 00:40:58.880
.  설명해 드리겠습니다.  SSI는 30억 달러를 모금했는데,

00:40:58.880 --> 00:41:05.440
이는 절대적인 의미로 보면 많은 액수입니다.
하지만 "

00:41:05.440 --> 00:41:13.920
다른 회사들이 훨씬 더 많은 자금을 조달하는 것을 보세요."라고 말할 수도 있습니다.
하지만 그들의 컴퓨팅의 많은 부분은 추론에 사용됩니다.

00:41:13.920 --> 00:41:20.160
이런 큰 숫자, 이런 큰 대출은
추론을 위해 따로 마련되었습니다.  그게 1위예요.    두 번째

00:41:20.160 --> 00:41:25.120
, 추론이 가능한 제품을 원한다면

00:41:25.120 --> 00:41:31.120
엔지니어와 영업 담당자로 구성된 대규모 직원이 필요합니다.

00:41:31.120 --> 00:41:37.440
모든 종류의 제품 관련 기능을 생산하는 데 많은 연구가 필요합니다.
그러면 실제로

00:41:37.440 --> 00:41:45.760
연구에 얼마나 투자해야 하는지 살펴보면 그 차이는 훨씬 작아집니다.
또 다른 점은, 만약 여러분이 다른 것을 한다면

00:41:45.760 --> 00:41:51.040
,
그것을 증명하기 위해 정말 절대적인 최대 규모가 필요할까요?

00:41:51.040 --> 00:41:58.080
저는 그것이 전혀 사실이 아니라고 생각합니다.
저는 우리의 경우,

00:41:58.080 --> 00:42:02.320
우리가 하는 일이 옳다는 것을 우리 자신과 다른 사람들에게 증명하고 확신시킬 수 있는 충분한 컴퓨팅 능력을 갖추고 있다고 생각합니다.

00:42:02.880 --> 00:42:08.320
OpenAI와 같은 회사가 지금까지 실험에 1년에 50~60억 달러를 지출했다는 대중적 추산이 있습니다

00:42:08.320 --> 00:42:13.680
.
이는

00:42:13.680 --> 00:42:18.720
추론 등에 사용하는 금액과는 별개입니다.
그러니까 그들이 연구 실험을 하는 데 1년 동안 투자한 금액이

00:42:18.720 --> 00:42:22.960
여러분이 투자한 총 금액보다 많은 것 같습니다.   문제는 그것을

00:42:22.960 --> 00:42:26.880
어떻게 활용하느냐에 달려 있다고 생각합니다.
문제는 그것을 어떻게 활용하느냐입니다.

00:42:29.840 --> 00:42:35.120
그들의 경우, 다른 사람들의 경우,
훈련 컴퓨팅에 대한 수요가 훨씬 더 많습니다.

00:42:35.120 --> 00:42:41.120
훨씬 더 다양한 업무 스트림이 있고, 작업
방식도 다르고, 할

00:42:41.120 --> 00:42:46.320
일도 더 많습니다.  그래서 그것은 단편화됩니다.
SSI는 어떻게 수익을 창출할까요?

00:42:48.480 --> 00:42:55.680
이 질문에 대한 내 대답은 다음과 같습니다.
지금은 단지 연구에만 집중하고 있고, 그러면

00:42:55.680 --> 00:43:01.360
그 질문에 대한 답은 저절로 드러날 것입니다.
가능한 답변이 많을 것 같아요.

00:43:01.360 --> 00:43:05.040
SSI의 계획은 여전히
초지능을 직접 구현하는 것인가요?

00:43:05.040 --> 00:43:11.280
아마도.  나는 그것이 장점이 있다고 생각한다.  저는 매일매일의 시장 경쟁에 영향을 받지 않는다는 점에서
많은 장점이 있다고 생각합니다

00:43:11.280 --> 00:43:17.840
.

00:43:17.840 --> 00:43:25.200
하지만 저는
우리가 계획을 변경하게 만드는 데는 두 가지 이유가 있다고 생각합니다.

00:43:25.200 --> 00:43:31.360
하나는 실용적입니다. 타임라인이
길어질 수도 있지만요.

00:43:31.360 --> 00:43:38.400
두 번째로, 저는
가장 뛰어나고

00:43:38.400 --> 00:43:46.480
강력한 AI가 세상에 영향을 미치는 데에는 많은 가치가 있다고 생각합니다.
저는 이것이 의미 있고 가치 있는 일이라고 생각합니다.

00:43:46.480 --> 00:43:49.440
그렇다면 왜 당신의 기본 계획은
곧바로 초지능에 집중하는 것일까요?

00:43:49.440 --> 00:43:54.880
OpenAI, Anthropic, 그 밖의
다른 회사들이 분명히 생각하는 바는

00:43:54.880 --> 00:44:01.040
"
대중이 익숙해지고 대비할 수 있는 점점 더 약한 지능이 우리에게 있다"는 것입니다.

00:44:01.040 --> 00:44:06.400
초지능을 직접 만드는 것이 잠재적으로 더 나은 이유는 무엇입니까?

00:44:06.400 --> 00:44:14.000
찬성과 반대를 말씀드리겠습니다.

00:44:14.000 --> 00:44:20.320
사람들이 시장에 있을 때 직면하는 과제 중 하나는
쥐 경주에 참여해야 한다는 것입니다.

00:44:20.320 --> 00:44:24.720
쥐 경주는 당신이 해야 할
어려운 타협에 당신을 노출시킨다는 점에서 매우 힘듭니다

00:44:24.720 --> 00:44:32.000
.
"우리는 이 모든 것에서 우리 자신을 보호하고

00:44:32.000 --> 00:44:38.160
연구에만 집중하고
준비가 되었을 때만 나오고, 그 전에는 나오지 않을 거야"라고 말하는 게 좋습니다.

00:44:38.160 --> 00:44:43.600
하지만 반대 의견도 타당하며,
그 둘은 상반되는 세력입니다.

00:44:43.600 --> 00:44:50.880
반론은 "
세상이 강력한 AI를 보는 것은 유용하다.

00:44:50.880 --> 00:44:53.600
세상이
강력한 AI를 보는 것은 유용하다. 왜냐하면 그것이

00:44:53.600 --> 00:44:56.800
그것을 전달할 수 있는 유일한 방법이기 때문이다."입니다.
글쎄요, 단순히

00:44:56.800 --> 00:44:59.760
아이디어 자체를 전달하는 것만으로는 충분하지 않은 것 같아요. 아이디어 자체가 아니라
AI를 전달하는 거죠

00:44:59.760 --> 00:45:04.080
.  AI와 소통하세요.
"AI와 소통한다"는 건 무슨 뜻인가요?

00:45:04.640 --> 00:45:10.080
AI에 대한 에세이를 쓴다고 가정해 보겠습니다.
에세이의 내용은 "AI는 이러할 것이고, AI는 저러할 것이고

00:45:10.080 --> 00:45:13.760
, 이것이 될 것이다"입니다.  이
글을 읽고 "좋아,

00:45:13.760 --> 00:45:18.320
흥미로운 글이네."라고 말하게 되죠.
이제 AI가 이것을 하고,

00:45:18.320 --> 00:45:27.920
AI가 저것을 한다고 가정해 보겠습니다.  비교할 수 없을 정도입니다.  기본적으로
저는 AI가 대중에게 공개되는 데 큰 이점이 있다고 생각하고

00:45:27.920 --> 00:45:35.600
, 그것이
우리가 솔직하게 말하지 않는 이유가 될 것입니다.   그게 아닐 수도 있겠지만,

00:45:36.320 --> 00:45:40.320
저는 그것이
중요한 부분이라고 생각합니다.

00:45:40.320 --> 00:45:45.360
또 다른 중요한 점은
인간 공학과

00:45:45.360 --> 00:45:53.280
연구 분야에서

00:45:53.280 --> 00:45:58.000
항공기 추락 사고가 수십 년 전보다 마일당

00:45:58.000 --> 00:46:02.560
훨씬 낮은 이유를 생각하는 대신, 단지 어떻게 안전하게 만들 것인가에 대한 고민만으로 최종 결과물을 더 안전하게 만드는 다른 분야를 생각해낼 수 없다는 것입니다.
왜 수십 년 전보다 리눅스에서 버그를 찾는 것이 훨씬 더 어려울까요

00:46:02.560 --> 00:46:06.320
?
제 생각에는 이런

00:46:06.320 --> 00:46:11.040
시스템이 전 세계에 배포되었기 때문일 겁니다.
여러분은 실패를 발견했고, 그 실패는

00:46:11.040 --> 00:46:17.280
수정되었으며 시스템은 더욱 강력해졌습니다.
AGI와 초인적 지능이 왜

00:46:17.280 --> 00:46:23.600
다를지 잘 모르겠습니다. 특히,
이 문제에 대해 이야기할 날이 오기를 바랍니다.

00:46:23.600 --> 00:46:29.760
초지능의 해악이 그저
사악한 종이 클립을 갖는 것에만 국한되지 않는 것 같기 때문입니다.

00:46:29.760 --> 00:46:34.640
하지만 이건 정말 강력한 것인데,
사람들이 이것과 어떻게 상호작용하는지

00:46:34.640 --> 00:46:39.760
, 사람들이 이것으로 무엇을 할지 개념화하는 방법조차 우리는 모릅니다.
점진적으로 접근할 수 있게 하는 것이 그

00:46:40.880 --> 00:46:45.440
영향을 분산시키고
사람들이 이에 대비하도록 돕는 더 나은 방법인 듯합니다.

00:46:45.440 --> 00:46:52.720
글쎄요, 제 생각에는 이 점에 있어서, 직접적인
시나리오에서도 점진적으로

00:46:52.720 --> 00:47:01.760
풀어내는 것이 좋을 것 같아요. 제 생각에는 그렇습니다.
점진주의는

00:47:01.760 --> 00:47:04.560
모든 계획의 본질적인 구성 요소가 될 것입니다.  그냥 문 밖으로 나가서 가장
먼저 무엇을 꺼내느냐는 문제일 뿐입니다

00:47:04.560 --> 00:47:11.680
.  그게
1번이에요.  두 번째로, 저는 당신이

00:47:11.680 --> 00:47:17.680
다른 사람들보다 지속적인 학습을 더 많이 옹호했다고 믿으며,
실제로 저는 이것이 중요

00:47:17.680 --> 00:47:29.600
하고 옳은 일이라고 생각합니다.  그 이유는 다음과 같습니다.
언어가 사고에 어떤 영향을 미치는지 또 다른 예를 들어보겠습니다.

00:47:29.600 --> 00:47:35.840
이 경우,
모든 사람의 생각을 형성한 두 단어가 있을 것이라고 나는 주장한다.

00:47:37.520 --> 00:47:48.400
첫 번째 단어: AGI(민첩성 지능)  두 번째 단어: 사전 훈련.
설명해드리죠.  AGI라는 용어는 왜

00:47:48.400 --> 00:47:55.120
존재할까요?  매우 특별한 용어죠.  왜
존재하는가?  이유가 있어요.

00:47:55.120 --> 00:48:02.640
제 생각에 AGI라는 용어가 존재하는 이유는

00:48:02.640 --> 00:48:14.160
지능의 최종 상태를 설명하는 매우 중요하고 필수적인 기술어이기 때문이라기보다는 기존에

00:48:14.160 --> 00:48:19.360
존재하던 좁은 의미의 AI라는 용어에 대한 반응이기 때문입니다.

00:48:19.360 --> 00:48:25.600
게임 플레이와 AI, 체커 AI, 체스
AI, 컴퓨터 게임 AI의 고대 역사를 돌아보면, 누구나 이렇게 말할 것입니다.

00:48:25.600 --> 00:48:29.600
이 좁은 지능을 보세요.
물론, 체스 AI는 카스파로프를 이길 수 있지만, 그

00:48:29.600 --> 00:48:34.720
외에는 아무것도 할 수 없습니다.
너무 좁은 범위의 인공 지능이에요.

00:48:34.720 --> 00:48:41.280
그래서 이에 대한 반응으로,
어떤 사람들은 이건 좋지 않다고 말했습니다.

00:48:41.280 --> 00:48:50.800
너무 좁아요.  우리에게 필요한 것은
모든 일을 할 수 있는 일반 AI입니다.

00:48:53.040 --> 00:48:59.360
그 용어는 많은 관심을 얻었습니다.
두 번째로 많은 관심을 받은 것은

00:48:59.360 --> 00:49:03.120
사전 훈련, 특히
사전 훈련의 레시피입니다.

00:49:03.120 --> 00:49:12.960
저는 사람들이 지금 RL을 하는 방식이
사전 훈련의 개념적 각인을 무너뜨리는 것 같다고 생각합니다.

00:49:12.960 --> 00:49:17.520
하지만 사전 훈련에는 이런 특성이 있었습니다.
사전 학습을 더 많이 하면 모델은

00:49:17.520 --> 00:49:29.760
모든 면에서 더 나아지고, 대체로 균일해집니다.
일반 AI.  사전 훈련을 통해 AGI를 얻을 수 있습니다.  하지만

00:49:29.760 --> 00:49:36.400
AGI와 사전 훈련에서 일어난 일은
어떤 면에서 목표를 넘어섰다는 것입니다.

00:49:38.160 --> 00:49:43.360
"AGI"라는 용어에 대해 생각해 보면,
특히 사전 훈련의 맥락에서 생각해 보면,

00:49:43.360 --> 00:49:53.760
인간은 AGI가 아니라는 것을 깨닫게 될 것입니다.
물론, 기술이라는 기반은 분명히 있지만,

00:49:53.760 --> 00:50:00.080
인간은
엄청난 양의 지식이 부족합니다.

00:50:00.080 --> 00:50:06.720
그 대신, 우리는 지속적인 학습에 의지합니다.
그러니 "좋아요,

00:50:06.720 --> 00:50:12.240
우리가 성공을 거두고
어떤 종류의 안전한 초지능을 만들어낸다고 가정해 봅시다."라고 생각해 보세요.

00:50:12.240 --> 00:50:16.320
문제는 그것을 어떻게 정의하느냐는 것입니다.
지속적인 학습의 곡선에서 어느 지점에 있을까요

00:50:16.320 --> 00:50:20.640
?
저는

00:50:20.640 --> 00:50:25.200
매우 열정적인 15세의 초지능 소년을 배출했습니다.
그들은 전혀 아는 것이 없지만,

00:50:25.200 --> 00:50:29.280
훌륭한 학생이고 매우 열정적입니다.
프로그래머가 되든,

00:50:29.280 --> 00:50:34.720
의사가 되든, 배우세요.
따라서 배포

00:50:34.720 --> 00:50:38.880
자체에는 일종의
학습 시행착오 기간이 포함될 것이라고 상상할 수 있습니다.

00:50:38.880 --> 00:50:43.520
그것은
완성된 것을 그냥 버리는 것과는 달리 하나의 과정입니다.   알겠어요

00:50:44.240 --> 00:50:51.120
.  당신은
초지능이라는 말로 지적하는 것이 경제의 모든 일을 처리하는

00:50:51.120 --> 00:50:58.560
방법을 아는 완성된 정신이 아니라고 말하고 있습니다
.   예를 들어

00:50:58.560 --> 00:51:05.120
, 원래 OpenAI 헌장
이나 AGI를 정의하는 방식은

00:51:05.120 --> 00:51:11.680
인간이 할 수 있는 모든 일, 모든 일을 할 수 있다는 것입니다.
당신은

00:51:11.680 --> 00:51:15.520
모든 일을 하나하나 배울 수 있는 마음을 제안하는데,
그것이 바로 초지능입니다.

00:51:15.520 --> 00:51:19.840
예.
하지만 학습 알고리즘이 있으면

00:51:19.840 --> 00:51:25.200
인간 노동자가 조직에 합류하는 것과 같은 방식으로 세상에 배포됩니다.

00:51:25.200 --> 00:51:27.360
정확히.
이 두 가지 중 하나가

00:51:27.360 --> 00:51:35.440
일어날 것 같고, 어쩌면 두 가지 모두 일어나지 않을 수도 있습니다.
첫째, 이 초고효율 학습 알고리즘은

00:51:35.440 --> 00:51:40.400
초인적이 되어

00:51:40.400 --> 00:51:45.920
ML 연구 과제에서 여러분만큼 능숙해지고 잠재적으로는 여러분보다 더 뛰어나질 수도 있습니다.
결과적으로 알고리즘

00:51:45.920 --> 00:51:50.560
자체가 점점 더 초인적이 됩니다.
다른 하나는, 그런 일이 일어나지 않더라도,

00:51:50.560 --> 00:51:56.800
단일 모델이 있다면(이것은 분명히
여러분의 비전입니다) 경제 전반에 걸쳐 배포되는 모델의 인스턴스가

00:51:56.800 --> 00:52:00.800
다양한 작업을 수행하고, 해당 작업을 수행하는 방법을 배우고,

00:52:00.800 --> 00:52:05.520
직장에서 지속적으로 학습하고,
사람이 습득할 수 있는 모든 기술을 습득

00:52:05.520 --> 00:52:10.480
하지만, 동시에 모든 기술을 습득하고
학습 내용을 통합하면

00:52:10.480 --> 00:52:15.440
기본적으로

00:52:15.440 --> 00:52:20.560
소프트웨어에서 재귀적 자기 개선이 전혀 없더라도 기능적으로 초지능이 되는 모델을 갖게 됩니다.
이제

00:52:20.560 --> 00:52:25.040
경제의 모든 일을 처리할 수 있는 하나의 모델이 있고, 인간은
같은 방식으로 마음을 합칠 수 없습니다.

00:52:25.040 --> 00:52:28.640
그렇다면
광범위한 배포를 통해 일종의 정보 폭발이 일어날 것으로 기대하시나요?   저는

00:52:28.640 --> 00:52:37.280
우리의
경제가 급속히 성장할 가능성이 높다고 생각합니다.

00:52:37.280 --> 00:52:46.640
광범위한 배포와 관련해서는 상충되는 두 가지 주장이 있을 수 있다고 생각합니다
.

00:52:46.640 --> 00:52:59.120
하나는
AI가 빠르게 일을 처리하는 법을 배울 수 있는 지점에 도달하고 그 수가 많아지면

00:52:59.120 --> 00:53:07.280
경제에 AI를 배치하려는 강력한 세력이 생길 것이라는 것입니다.

00:53:07.280 --> 00:53:13.360
물론 이를 막는 규제가 있을 수도 있지만요.

00:53:13.360 --> 00:53:19.120
하지만 당분간 매우 빠른
경제 성장이 가능하다는 아이디어는

00:53:19.120 --> 00:53:25.440
광범위하게 확산되면 매우 가능할 것이라고 생각합니다.
문제는 그것이 얼마나 빨리 일어날 것인가입니다.

00:53:25.440 --> 00:53:30.720
제 생각에는 이것을 아는 것은 어려운데,
한편으로는 매우 효율적인 근로자가 있기 때문입니다.

00:53:30.720 --> 00:53:36.160
반면에 세상은
정말 크고 많은 것들이 있으며,

00:53:36.160 --> 00:53:41.600
그 것들의 움직임은 다릅니다.
하지만 반면에, 이제 AI는...

00:53:41.600 --> 00:53:47.920
그래서 저는 매우 빠른 경제 성장이 가능하다고 생각합니다.
우리는 서로

00:53:47.920 --> 00:53:51.280
다른 규칙을 가진 여러 나라와 더
우호적인 규칙을 가진 나라 등 다양한 상황을 보게 될 것이며,

00:53:51.280 --> 00:55:10.320
경제 성장은 더 빨라질 것입니다.  예측하기 어렵습니다.  제 생각에는
이것은 매우 위태로운

00:55:10.320 --> 00:55:14.080
상황인 듯합니다. 하지만
한계 내에서는

00:55:14.080 --> 00:55:17.760
이것이 가능하다는 것을 우리는 알고 있습니다.

00:55:17.760 --> 00:55:24.400
인간만큼 학습 능력이 뛰어나고, 뇌를 합칠 수 있는 기계가 있다면, 즉

00:55:24.400 --> 00:55:28.320
인간이 합칠 수 없는 방식으로 서로 다른 인스턴스를 합칠 수 있다면, 이는
물리적으로 가능한 일처럼 보입니다.

00:55:28.320 --> 00:55:30.800
인간은 가능하고, 디지털
컴퓨터도 가능합니다.

00:55:30.800 --> 00:55:33.440
이걸 만들려면 두 가지를 합치면 됩니다.

00:55:33.440 --> 00:55:40.080
이런 종류의 것은
매우 강력한 것 같습니다.

00:55:41.440 --> 00:55:45.680
경제 성장이 한 가지 표현입니다.
다이슨 구체는 많은 경제적 성장을 가져왔습니다.

00:55:45.680 --> 00:55:50.480
하지만 다른 말로 표현하자면,
잠재적으로 매우 짧은 기간 안에...

00:55:52.080 --> 00:55:55.200
SSI에서 사람을 고용하면 6
개월 안에 순생산성을 갖게 될 것입니다.

00:55:56.000 --> 00:56:00.640
사람은 정말 빨리 배우고, 이건 점점 더
똑똑해지고 있어요.

00:56:01.200 --> 00:56:05.840
어떻게 하면 그것을 잘 할 수 있을 것 같아요?
SSI가 그렇게 잘할 수 있는 이유는 무엇입니까?   제가 묻고 싶은 것은

00:56:05.840 --> 00:56:08.160
SSI의 계획이 무엇인지입니다
.

00:56:12.160 --> 00:56:22.880
내 생각이 변화한 방식 중 하나는

00:56:22.880 --> 00:56:34.960
AI가 점진적이고 사전에 배포되는 데 더 큰 중요성을 두게 되었다는 것입니다.
AI에 대한 매우 어려운 점 중 하나는

00:56:34.960 --> 00:56:42.480
아직 존재하지 않는 시스템에 대해 이야기하고 있고,
이를 상상하기 어렵다는 것입니다.

00:56:43.600 --> 00:56:52.080
제 생각에 현재 일어나고 있는 일 중 하나는
실제로 AGI를 느끼는 것이 매우 어렵다는 것입니다.

00:56:52.080 --> 00:57:01.440
AGI를 느끼는 건 매우 어렵습니다.
우리는 이에 대해 이야기할 수 있지만, 늙고 허약한 사람이 늙은다는 것이

00:57:01.440 --> 00:57:07.840
어떤 것인지에 대해 대화하는 것을 상상해보세요
.

00:57:07.840 --> 00:57:12.640
대화를 나눌 수도 있고,
상상해 볼 수도 있지만 그저 어려울 뿐이고,

00:57:12.640 --> 00:57:22.400
현실로 돌아오면 그렇지 않다는 것을 깨닫게 됩니다.  저는
AGI

00:57:22.400 --> 00:57:30.960
와 그 미래의 힘을 둘러싼 많은 문제가
그것이 상상하기 매우 어렵다는 사실에서 비롯된다고 생각합니다.

00:57:30.960 --> 00:57:37.680
미래의 AI는 달라질 것이다.
강력할 거예요.  사실, 전체적인 문제는

00:57:37.680 --> 00:57:43.360
AI와 AGI의 문제점이 무엇인가?
문제는 바로 권력입니다.

00:57:43.360 --> 00:57:48.320
문제는 바로 권력입니다.
힘이 정말 커지면

00:57:48.320 --> 00:57:53.120
무슨 일이 일어날까요?

00:57:53.120 --> 00:58:02.320
지난 1년 동안 제가 마음을 바꾼 방법 중 하나는, 그리고 그
마음의 변화는, 조금은 모호하게 말씀드리겠습니다만,

00:58:02.320 --> 00:58:12.960
회사 계획에 역전파될 수도 있습니다.
상상하기 어려운 것이라면, 어떻게 하시겠습니까?   그걸

00:58:12.960 --> 00:58:16.560
보여줘야죠.  그걸
보여줘야죠.

00:58:16.560 --> 00:58:24.400
저는 AI 분야에서 일하는 대부분의 사람들도 그것이

00:58:24.400 --> 00:58:31.440
사람들이 일상적으로 보는 것과 너무 다르기 때문에 상상조차 할 수 없다고 생각합니다.
저는 다음과 같은 일이

00:58:31.440 --> 00:58:40.960
일어날 것이라고 예측합니다.  이것은 예측입니다.
저는 AI가 더욱 강력해짐에 따라

00:58:40.960 --> 00:58:48.320
사람들의 행동도 바뀔 것이라고 생각합니다.
우리는 지금 일어나지 않는 온갖 전례 없는

00:58:48.320 --> 00:58:56.720
일들을 보게 될 것입니다.
몇 가지 예를 들어보겠습니다.  좋든 나쁘든, 저는

00:58:57.440 --> 00:59:03.120
개척지 기업들이 앞으로 일어날 일들에 매우 중요한
역할을 할 것이라고 생각합니다. 그리고 정부도 마찬가지입니다.

00:59:03.120 --> 00:59:06.800
제가 생각하기에
여러분이 보게 될 일들은

00:59:06.800 --> 00:59:15.920
치열한
경쟁을 벌이는 회사들이 AI 안전에 대해 협업을 시작하는 것입니다.

00:59:15.920 --> 00:59:22.880
여러분은 OpenAI와 Anthropic이
첫 번째 작은 발걸음을 내딛는 것을 보았을지도 모르지만, 그런 일은 존재하지 않았습니다.   저는

00:59:22.880 --> 00:59:27.360
3년 전 제 강연 중 하나에서 그런 일이

00:59:27.360 --> 00:59:30.960
일어날 것이라고 예측했습니다.
저는 AI가 점점

00:59:30.960 --> 00:59:38.320
더 강력해지고 눈에 띄게 강력해짐에 따라

00:59:38.320 --> 00:59:46.160
정부와 대중도 무언가를 하려는 욕구가 생길 것이라고 생각합니다.
저는 이것이 AI를 보여주는 매우 중요한 힘이라고 생각합니다

00:59:46.160 --> 00:59:51.600
.  그게 1위예요.
두 번째, 좋아요, AI가

00:59:51.600 --> 00:59:59.600
개발되고 있어요.  무엇을 해야 하나요?
제가 주장하는 한 가지는 지금 당장

00:59:59.600 --> 01:00:06.560
AI를 연구하는 사람들이
AI가 자신의 실수 때문에 강력하다고 느끼지 않는다는 것입니다.

01:00:06.560 --> 01:00:10.880
저는 어느 순간 AI가
실제로 강력해지기 시작할 것이라고 생각합니다.   그런 일이

01:00:10.880 --> 01:00:18.160
일어나면
모든 AI 회사가 안전에 접근하는 방식에 큰 변화가 생길 거라고 생각합니다

01:00:18.160 --> 01:00:25.680
.  그들은 훨씬 더 편집증을 느끼게 될 것이다.
저는 이런 일이 일어날 것이라는 예측을 말씀드립니다

01:00:25.680 --> 01:00:30.320
.  내가 옳은지 보자.  하지만 저는

01:00:30.320 --> 01:00:34.160
AI가 점점 더 강력해지는 것을 보게 될 것이기 때문에 이런 일이 일어날 것이라고 생각합니다.
지금 일어나고 있는 모든 일은

01:00:34.160 --> 01:00:42.080
사람들이 오늘날의
AI를 보고 미래의 AI를 상상하기 어렵기 때문이라고 저는 생각합니다.

01:00:42.640 --> 01:00:49.520
세 번째로 일어나야 할 일이 있습니다.
저는

01:00:49.520 --> 01:00:54.640
SSI의 관점만이 아니라 더 넓은 관점에서 이야기하고 있습니다.
왜냐하면 귀하께서 저희 회사에 대해 물으셨기 때문입니다.

01:00:54.640 --> 01:00:58.400
문제는,
기업들이 무엇을 만들고자 노력해야 하는가?

01:00:58.400 --> 01:01:02.160
그들은 무엇을 만들고자 열망해야 할까?  모든 사람이 사로잡힌
하나의 큰 아이디어가 있는데

01:01:04.000 --> 01:01:11.120
, 바로
자기개선형 AI입니다.  왜 그런 일이 일어났을까?

01:01:11.120 --> 01:01:17.200
기업보다 아이디어가 적기 때문입니다.
하지만 저는 더 나은 것을

01:01:17.200 --> 01:01:21.360
만드는 것이 있다고 주장하며,
모든 사람이 그것을 원할 것이라고 생각합니다.

01:01:21.360 --> 01:01:28.880
특히 지각 있는 생명체에 관심을 갖도록 견고하게 조정된 AI입니다.

01:01:29.680 --> 01:01:35.280
특히, 인간의 삶만을 걱정하는 AI

01:01:35.280 --> 01:01:40.640
보다 인간의 삶에 관심을 갖는 AI를 만드는 것이 더 쉽다는 주장이 있다고 생각합니다.

01:01:40.640 --> 01:01:46.080
왜냐하면 AI 자체가 의식을 가지고 있기 때문입니다.
그리고 거울

01:01:46.080 --> 01:01:53.200
뉴런과 동물에 대한 인간의 공감과 같은 것을 생각해 보면,
그것이 충분히 크지 않다고 주장할 수도 있지만, 그런 것도 존재합니다.

01:01:53.200 --> 01:01:58.240
저는 이것이

01:01:58.240 --> 01:02:03.680
우리가 자신을 모델링하는 데 사용하는 것과 동일한 회로로 다른 사람을 모델링한다는 사실에서 비롯된 새로운 속성이라고 생각합니다.
왜냐하면 그것이 가장 효율적인 일이기 때문입니다.

01:02:03.680 --> 01:02:08.880
따라서
지각 있는 존재에 관심을 갖는 AI가 있다고 하더라도(그리고 정렬 문제를 해결했다면 그렇게 해야 할지 실제로는 명확하지 않지만)

01:02:10.720 --> 01:02:16.480
대부분의 지각 있는 존재는 여전히 AI일 것입니다.

01:02:16.480 --> 01:02:19.360
수조 개,
결국에는 수천 조 개의 AI가 존재하게 될 것입니다.

01:02:19.360 --> 01:02:22.400
인간은
지각 있는 존재의 아주 작은 일부일 것입니다.

01:02:23.280 --> 01:02:32.480
따라서
이 미래 문명에 대한 어떤 종류의 인간 통제가 목표인지,

01:02:32.480 --> 01:02:39.680
이것이 최선의 기준인지는 확실하지 않습니다.
사실이에요.  이것이

01:02:39.680 --> 01:02:53.120
최고의 기준은 아닐 수도 있습니다.  두 가지 말씀드리겠습니다.
첫째, 생명체에 대한 배려는 가치 있는 일이라고 생각합니다

01:02:53.120 --> 01:03:01.120
.  고려해 볼 만한 사항입니다.

01:03:01.120 --> 01:03:10.240
이런 상황에 처한 회사들이 활용할 수 있는 간단한 아이디어 목록이 있다면 도움이 될 것 같습니다.  두 번째는요.

01:03:10.240 --> 01:03:16.480
세 번째,

01:03:16.480 --> 01:03:23.680
가장 강력한 초지능의 힘을 어떻게든 제한한다면
이런 우려 사항들을 많이 해결할 수 있을 테니 물질적으로 정말 도움이 될 것 같아요.

01:03:23.680 --> 01:03:29.680
그것을 어떻게 할 것인가에 대한 질문은 확실하지 않지만, 정말로 강력한 시스템에 대해 이야기할
때는 그것이 실질적으로 도움이 될 것이라고 생각합니다

01:03:29.680 --> 01:03:35.360
.
정렬에 대한 논의를 계속하기 전에,  ...

01:03:35.360 --> 01:03:38.560
위쪽에는 얼마나 많은 공간이 있나요?

01:03:38.560 --> 01:03:44.400
당신은 초지능에 대해 어떻게 생각하시나요?
이 학습 효율성 아이디어를 사용하면

01:03:44.400 --> 01:03:48.880
새로운 기술이나 새로운 지식을 배우는 속도가 매우 빨라질 수 있다고 생각하시나요?

01:03:48.880 --> 01:03:54.320
단순히 전략의 폭이 더 넓어진 것뿐인가요?
중앙에 더 강력하거나 더 큰 단일한 응집력 있는 "그것"이 있습니까

01:03:54.320 --> 01:04:01.600
?
그렇다면 이것이

01:04:01.600 --> 01:04:05.120
인류 문명의 나머지와 비교했을 때 일종의 신과 같다고 생각하시나요?
아니면 그저 또 다른

01:04:05.120 --> 01:04:10.000
존재나 존재들의 집합처럼 느껴지나요?
이는

01:04:10.000 --> 01:04:14.800
사람마다 직관이 다른 영역입니다.
저는 그것이 매우 강력할 것이라고 확신합니다.

01:04:16.240 --> 01:04:23.200
제 생각에 가장 일어날 가능성이 큰 일은 거의 동시에
여러 개의

01:04:23.200 --> 01:04:33.280
AI가 만들어지는 것입니다.  저는
클러스터가 충분히 크다면, 즉

01:04:33.280 --> 01:04:39.040
클러스터가 문자 그대로 대륙 크기라면, 그것은 실제로 매우
강력할 수 있다고 생각합니다.

01:04:39.040 --> 01:04:44.720
만약 문자 그대로 대륙 크기의
클러스터가 있다면, 그 AI는 매우 강력할 수 있습니다.   제가

01:04:46.640 --> 01:04:51.680
말씀드릴 수 있는 건, 만약 당신이
극도로 강력한 AI,

01:04:51.680 --> 01:04:59.760
정말로 극적으로 강력한 AI에 대해 이야기한다면,
어떤 면에서는 제한을 두거나

01:04:59.760 --> 01:05:11.200
어떤 종류의 합의나 그런 게 있으면 좋겠다는 겁니다.
초지능의 관심사는 무엇인가?   이러한

01:05:11.200 --> 01:05:16.800
우려를 설명하는 한 가지 방법은 무엇입니까?
충분히

01:05:16.800 --> 01:05:23.440
강력한, 정말로 충분히 강력한 시스템을 상상해 보세요. 그리고 매우 단일한 방식으로 지각 있는 생명을 돌보는 것과
같은 합리적인 일을 해야 한다고 말할 수 있습니다.

01:05:23.440 --> 01:05:29.440
우리는 그 결과를 좋아하지 않을 수도 있습니다.  그게 사실이에요

01:05:29.440 --> 01:05:35.840
.  그런데, 답은
일반적인 의미에서 RL 에이전트를 만들지 않는다는 것입니다

01:05:35.840 --> 01:05:42.800
.  몇 가지를 지적하겠습니다.  저는
인간이 반-RL 행위자라고 생각합니다.

01:05:43.600 --> 01:05:48.160
우리는 보상을 추구하고, 그러다가 감정
이나 다른 것이 우리를

01:05:48.160 --> 01:05:55.760
보상에 지치게 만들고, 우리는 다른 보상을 추구합니다.
시장은 매우 근시안적인 종류의

01:05:55.760 --> 01:05:59.600
대리인입니다.  진화도 마찬가지다.  진화
는 어떤 면에서는 매우 지적인 반면,

01:05:59.600 --> 01:06:03.040
다른 면에서는 매우 어리석습니다.
정부는 세 부분

01:06:03.040 --> 01:06:08.320
간의 끝없는 싸움으로 설계되었으며
, 이는 효과가 있습니다.

01:06:08.320 --> 01:06:13.120
그래서 저는 이런 생각을 합니다.
이 논의를 어렵게 만드는 또 다른 점은

01:06:13.120 --> 01:06:19.600
우리가 존재하지 않는 시스템에 대해 이야기하고 있다는 점입니다.
우리는 그런 시스템을 만드는 방법을 모릅니다.

01:06:19.600 --> 01:06:21.760
그게 또 다른 것이고,
사실 그게 제 믿음이에요.

01:06:21.760 --> 01:06:26.560
저는 사람들이 지금 하고 있는 일이
어느 정도 지속되다가 사라질 것이라고 생각합니다.

01:06:26.560 --> 01:06:30.080
계속해서 개선될 것이지만,
그렇다고 해서 "그것"이 될 수는 없을 것입니다.

01:06:30.080 --> 01:06:38.960
"그것"은 우리가 어떻게 만들어야 할지 모르고,
많은 부분이 신뢰할 수 있는 일반화를 이해하는 데 달려 있습니다

01:06:38.960 --> 01:06:47.120
.  다른 말도 할게요.

01:06:47.120 --> 01:06:55.760
정렬이 어려운 이유 중 하나는
인간의 가치를 배우는 능력이 취약하다는 것입니다.

01:06:55.760 --> 01:07:00.080
그러면 이를 최적화하는 능력이 취약하다는 뜻입니다.
실제로 최적화하는 법을 배우게 됩니다.

01:07:00.080 --> 01:07:06.640
그리고 "이것이 모두 신뢰할
수 없는 일반화의 사례가 아닌가?"라고 말할 수 없습니까?

01:07:06.640 --> 01:07:10.080
왜 인간은
일반화를 훨씬 더 잘하는 것처럼 보일까요?

01:07:10.080 --> 01:07:13.360
일반화가 훨씬 더 좋다면 어떨까요?
이런 경우에는 무슨 일이 일어날까요?

01:07:13.360 --> 01:07:18.560
그 효과는 무엇일까요?  하지만 그 질문들은 지금으로선
아직 답할 수 없습니다.

01:07:19.200 --> 01:07:24.640
AI가 잘 발전하는 모습은 어떤 모습일지 어떻게 생각하시나요?

01:07:24.640 --> 01:07:28.480
AI가 어떻게 진화할지 알아보았습니다.
우리는 이런 종류의 지속적인

01:07:28.480 --> 01:07:33.600
학습 에이전트를 갖게 될 것입니다.  AI는 매우 강력해질 것이다.
아마도 다양한 AI가 나올 수도 있겠죠.

01:07:33.600 --> 01:07:40.800
대륙 규모의 컴퓨팅 인텔리전스가 많이 생겨나는 것에 대해 어떻게 생각하시나요
?  얼마나 위험한 일

01:07:40.800 --> 01:07:49.840
인가요?  어떻게 하면 덜 위험하게 만들 수 있을까?
그러면 어떻게 하면

01:07:49.840 --> 01:07:56.240
잘못된
AI가 존재하고 나쁜 행위자가 존재하는 상황에서 균형을 보호할 수 있을까요?

01:07:56.240 --> 01:08:00.960
"
지각 있는 생명체를 돌보는 AI"를 제가 좋아하는 이유는 다음과 같습니다.

01:08:00.960 --> 01:08:09.520
우리는 그것이 좋은지 나쁜지에 대해 토론할 수 있습니다.
하지만 이러한 극적인 체계 중 첫 번째 N개가

01:08:09.520 --> 01:08:17.520
인간을 사랑하거나,
지각 있는 생명체를 돌보는 것이라면,

01:08:17.520 --> 01:08:23.840
분명히 이것도 달성되어야 합니다.  이것은
달성되어야 합니다.  따라서 이것이

01:08:23.840 --> 01:08:32.960
첫 번째 N개의 시스템에 의해 달성된다면,
적어도 한동안은 잘 될 것으로 봅니다.

01:08:32.960 --> 01:08:36.560
그러면
장기적으로 무슨 일이 일어날 것인가 하는 의문이 생깁니다.

01:08:36.560 --> 01:08:44.960
장기적인 균형을 어떻게 달성할 수 있나요?
저는 거기에 답이 있다고 생각합니다.

01:08:44.960 --> 01:08:49.200
저는 이 답변이 마음에 들지 않지만,
고려해 볼 필요가 있습니다.

01:08:51.760 --> 01:08:57.120
장기적으로 보면, "좋습니다.
강력한 AI가 존재하는 세상이라면

01:08:57.120 --> 01:09:01.200
단기적으로는
보편적인 고소득이 있다고 말할 수 있습니다.

01:09:01.200 --> 01:09:04.880
보편적인 고소득이 있고
우리 모두 잘 지내고 있습니다."라고 말할 수 있습니다.

01:09:04.880 --> 01:09:11.440
하지만 불교에서는 무엇이라고 말할까요?  "변화는
유일한 불변의 것입니다."  상황은 변합니다.  어떤

01:09:11.440 --> 01:09:18.160
종류의 정부, 정치 구조라는 것이 있는데,
이런 것들은 유효 기간이 있기 때문에 변합니다.

01:09:18.720 --> 01:09:22.560
새로운 정부 기관이 생겨나서 제대로
작동하다가 얼마 후에는 더 이상

01:09:22.560 --> 01:09:25.600
작동하지 않게 됩니다.
그건

01:09:25.600 --> 01:09:32.240
우리가 늘 보는 일이죠.
그래서 저는 장기적인 균형을 이루기 위한

01:09:32.240 --> 01:09:38.800
한 가지 접근 방식은 모든
사람이 자신의 명령에 따라 행동하는 AI를 갖게 되는 것이라고 생각합니다.

01:09:38.800 --> 01:09:41.040
그게 좋은 일이죠.
그것이

01:09:41.040 --> 01:09:47.040
무기한 유지될 수 있다면, 그것은 사실입니다.
하지만 그 단점은 AI가 그

01:09:47.040 --> 01:09:55.840
사람을 대신해 돈을 벌고
정치적 영역에서 그 사람의 요구를 옹호한 다음, "

01:09:55.840 --> 01:09:59.520
좋아요,
제가 한 일은 이렇습니다. 상황은 이렇습니다."라는 짧은 보고서를 쓰고

01:09:59.520 --> 01:10:05.760
그 사람이 "좋습니다. 계속하세요."라고 말하는 것입니다.
하지만 그 사람은 더 이상 참여자가 아닙니다.

01:10:05.760 --> 01:10:08.720
그러면 그것은 위태로운 상황이라고 말할 수 있습니다
. 저는

01:10:10.480 --> 01:10:16.880
이 해결책이 마음에 들지 않지만, 이것이 해결책이라는 말로 시작하겠습니다.

01:10:19.040 --> 01:10:23.680
해결책은 사람들이
Neuralink++의 어떤 종류로든 AI의 일부가 되는 것입니다.   그

01:10:23.680 --> 01:10:27.920
결과,
AI가 무언가를 이해하게 되고,

01:10:27.920 --> 01:10:34.160
우리도 그것을 이해하게 됩니다. 이제
이해가 대량으로 전달되기 때문입니다.

01:10:34.160 --> 01:10:41.920
따라서 이제 AI가 어떤 상황에 처하게 되면, 당신도
그 상황에 완전히 개입하게 됩니다.

01:10:41.920 --> 01:10:49.840
저는 이것이 균형에 대한 답이라고 생각합니다.

01:10:49.840 --> 01:10:56.160
수백만 년, 혹은 많은 경우 수십억
년 전 완전히 다른 환경에서 발달된 감정이

01:10:56.160 --> 01:11:03.520
여전히 우리의 행동을 강력하게 이끌고 있다는 사실이
정렬 성공의 한 예가 아닐까 싶습니다.

01:11:03.520 --> 01:11:11.520
제가 말하고자 하는 바를 자세히 설명하자면,

01:11:11.520 --> 01:11:15.760
가치 함수라고 부르는 게 더 정확한지 보상 함수라고 부르는 게 더 정확한지 모르겠지만,
뇌간에는

01:11:15.760 --> 01:11:19.600
"더 성공적인 사람과 어울리라"는 지시가 있습니다.
피질은

01:11:19.600 --> 01:11:25.200
현대적 맥락에서 성공이 무엇을 의미하는지 이해하는 부분입니다.
하지만 뇌간은 피질을 정렬하고

01:11:25.200 --> 01:11:29.840
"당신이 성공을 어떻게 인식하든 - 그리고
저는 그것이 무엇인지 이해할 만큼 똑똑하지 않지만 -

01:11:29.840 --> 01:11:36.560
당신은 여전히 ​​이 지시를 따를 것입니다."라고 말할 수 있습니다.
저는 좀 더 일반적인 관점이 있다고 생각합니다.

01:11:36.560 --> 01:11:46.960
저는
진화가 어떻게 고차원적 욕망을 인코딩하는지가 실제로 매우 신비롭다고 생각합니다.

01:11:46.960 --> 01:11:51.920
진화가 우리에게

01:11:51.920 --> 01:11:58.400
냄새가 좋은 음식에 대한 욕구를 부여한 이유를 이해하는 것은 매우 쉽습니다. 냄새는
화학 물질이기 때문에 그 화학 물질을 추구하면 됩니다.

01:11:58.400 --> 01:12:02.800
진화가 그런 일을 한다고 상상하는 건 매우 쉽습니다.

01:12:02.800 --> 01:12:08.880
하지만 진화는 또한
우리에게 이러한 모든 사회적 욕망을 부여했습니다.

01:12:08.880 --> 01:12:12.720
우리는
사회에서 긍정적으로 평가받는 것을 정말 중요하게 생각합니다.

01:12:12.720 --> 01:12:19.760
우리는 신용을 유지하는 데 관심이 있습니다.
우리가 가지고 있는 이 모든 사회적 직관은 우리 몸에 깊이

01:12:19.760 --> 01:12:26.240
새겨져 있다고 저는 강하게 느낍니다.
진화가 어떻게 이런 일을 했는지는 모르겠습니다.

01:12:26.240 --> 01:12:29.840
왜냐하면 그것은
뇌에 표현되는 고차원적 개념이기 때문입니다.   예를 들어,

01:12:31.360 --> 01:12:40.880
당신이 어떤 사회적인 것에 관심이 있다고 가정해 보겠습니다.
그것은 냄새와 같은 저수준 신호가 아닙니다.

01:12:40.880 --> 01:12:46.320
센서가 있어서 가능한 일이 아닙니다.
뇌는

01:12:46.320 --> 01:12:51.440
사회적으로 무슨 일이 일어나고 있는지 이해하기 위해 많은 양의 정보를 조각조각 모아 많은 처리 작업을 해야 합니다.

01:12:51.440 --> 01:12:56.640
어떻게 된 일인지 진화론은 "그게 바로 당신이
신경 써야 할 일이에요."라고 말했어요.  어떻게 가능했나요?  그것도

01:12:56.640 --> 01:13:04.400
아주 빠르게.  우리가 관심을 갖는 이 모든 정교한 사회적 사물들은
아주 최근에 진화했다고 생각합니다.

01:13:04.400 --> 01:13:08.560
진화론은
이러한 고수준의 욕구를 하드코딩하는 데 어려움을 겪었습니다.

01:13:12.000 --> 01:13:16.000
그것이 어떻게 이루어지는지에 대한 좋은 가설은 모르겠습니다.

01:13:16.000 --> 01:13:23.920
저는 몇 가지 아이디어를 생각해 냈지만, 그 중 어느 것도
만족스럽지 않았습니다.

01:13:24.560 --> 01:13:29.680
특히 인상적인 것은
당신이 평생 동안 욕망을 배웠다는 것입니다.

01:13:29.680 --> 01:13:32.560
당신의 뇌가 지능적이기 때문에 그것은 당연합니다.

01:13:32.560 --> 01:13:38.880
지적인 욕망을 배울 수 있는 이유가 무엇인지 알 수 있습니다.
이것이 당신의 요점이 아닐 수도 있지만,

01:13:38.880 --> 01:13:44.240
이를 이해하는 한 가지 방법은 욕망이 유전자에 내장되어 있고
, 유전자 자체가 지능적이지 않다는 것입니다.

01:13:44.240 --> 01:13:50.160
하지만 당신은 어떻게든 이 기능을 설명할 수 있을 것 같아요.
그 기능을 어떻게 정의하는지도 명확하지 않고,

01:13:50.160 --> 01:13:55.520
유전자에 내장할 수도 있습니다.
기본적으로 그렇습니다. 아니면 다르게 표현하겠습니다.   게놈이 사용할 수 있는

01:13:55.520 --> 01:14:01.280
도구에 대해 생각해 보면,

01:14:01.280 --> 01:14:05.760
"좋아요, 뇌를 만드는 방법이 여기에 있습니다."라고 말할 수 있습니다.
"

01:14:05.760 --> 01:14:10.560
도파민 뉴런을 후각 센서에 연결하는 방법은 다음과 같습니다."라고 말할 수 있습니다.
그리고 그 냄새가 어떤 종류

01:14:10.560 --> 01:14:15.680
의 좋은 냄새라면, 당신은 그것을 먹고 싶어할 것입니다.
나는 게놈이 그런 일을 할 수 있다고 상상할 수 있다.

01:14:15.680 --> 01:14:21.120
저는 그것이 상상하기 더 어렵다고 주장합니다.
게놈이

01:14:21.120 --> 01:14:28.080
당신의 뇌 전체, 즉 뇌의 큰 덩어리가 처리하는 복잡한 계산에 대해 신경 써야 한다고 말하는 것은 상상하기 어렵습니다

01:14:28.080 --> 01:14:33.760
.  제가 주장하는 것은 그게 전부입니다.
그것이 어떻게 이루어질 수 있을지에 대한 추측을 말씀드릴 수 있습니다.

01:14:33.760 --> 01:14:37.760
추측을 하나 해보겠습니다. 그리고
그 추측이 왜 거짓일 가능성이 큰지 설명하겠습니다.

01:14:37.760 --> 01:14:52.080
그러니까 뇌에는 뇌 영역이 있는 거예요.  우리에게는
피질이 있습니다.  뇌에는 모든 영역이 있습니다.

01:14:52.080 --> 01:14:57.040
피질은 균일하지만, 뇌의
영역과 피질의 뉴런은

01:14:57.040 --> 01:15:01.120
주로 이웃과 소통합니다.
그래서 뇌에 특정 영역이 생기는 거예요.

01:15:01.120 --> 01:15:04.640
어떤 종류의
음성 처리를 하려면 음성을 처리하는 모든 뉴런이

01:15:04.640 --> 01:15:08.000
서로 대화해야 합니다.
그리고 뉴런은 근처의 이웃과만 통신할 수 있기 때문에

01:15:08.000 --> 01:15:11.520
대부분 영역이어야 합니다.

01:15:11.520 --> 01:15:15.280
모든 지역은
사람마다 대체로 같은 장소에 위치해 있습니다.

01:15:15.280 --> 01:15:21.360
그러니까 진화론이
말 그대로 뇌의 특정 위치를 딱딱하게 코딩했을 수도 있겠죠.

01:15:21.360 --> 01:15:27.920
그러니까 "아, 뇌의 GPS 좌표가 이러이러할 때
, 그것이 작동할 때,

01:15:27.920 --> 01:15:30.720
그게 당신이 신경 써야 할 일이에요."라는 뜻이에요.
어쩌면 진화가 그런 일을 했을지도 모릅니다.

01:15:30.720 --> 01:15:36.000
진화의 도구 상자에 그런 것이 들어 있었을 테니까요.
네,

01:15:36.000 --> 01:15:44.160
예를 들어 시력을 잃은 사람
의 피질 부위가 다른 감각에 의해 채택되는 경우도 있습니다.

01:15:44.960 --> 01:15:53.200
저는 잘 모르겠지만, 피질의 다른 영역이 독점된 사람들에게는 시각적 신호를
필요로 하는 욕망이나 보상 기능이

01:15:53.200 --> 01:15:58.720
더 이상 작동하지 않을 것 같습니다
.

01:15:58.720 --> 01:16:05.680
예를 들어, 더 이상 시력이 없더라도 주변
사람들이 나를 좋아해 주기를 바라는 감정을 여전히 느낄 수 있습니까?

01:16:05.680 --> 01:16:10.000
대개는 시각적인 단서도 있습니다.

01:16:10.000 --> 01:16:14.320
저는 그 의견에 전적으로 동의합니다.
이 이론에는 더 강력한 반론이 있다고 생각합니다.

01:16:16.880 --> 01:16:22.560
어린 시절에 뇌의 절반을 제거했지만

01:16:23.360 --> 01:16:27.760
뇌의 모든 영역을 그대로 간직한 사람들이 있습니다.
하지만 그들은 모두 어떻게든 한쪽 반구로만 이동하는데, 이는

01:16:27.760 --> 01:16:32.240
뇌 영역과
그 위치가 고정되어 있지 않다는 것을 시사하며, 따라서

01:16:32.240 --> 01:16:34.240
이 이론은 사실이 아닙니다.

01:16:34.240 --> 01:16:37.680
사실이라면 멋졌겠지만, 사실이 아닙니다.
그래서 저는 그것이 미스터리라고 생각합니다.

01:16:37.680 --> 01:16:43.600
하지만 흥미로운 미스터리죠.  사실
진화는 어떻게든 우리에게

01:16:43.600 --> 01:16:49.840
사회적 문제에 관심을 갖게 하는 능력을 매우, 매우 확실하게 부여했습니다.
온갖 이상한 정신적

01:16:49.840 --> 01:16:54.240
상태와 결핍, 감정적
문제를 겪고 있는 사람들조차도 이에 대해 신경 쓰는 경향이 있습니다.

01:18:13.360 --> 01:18:18.080
SSI는 무엇을 다르게 계획하고 있나요?
아마도 당신의 계획은

01:18:18.080 --> 01:18:27.120
이때가 되면 최전선에 있는 기업 중 하나가 되는 것이겠죠.
아마도 당신은

01:18:27.120 --> 01:18:30.720
"다른 회사와는 달리 안전하게 이 일을 할 수 있는 방법이 있다고 생각한다

01:18:30.720 --> 01:18:37.760
"는 생각으로 SSI를 시작했을 겁니다.  그 차이점은 무엇인가?
제가 설명하고자 하는 바는,

01:18:37.760 --> 01:18:43.040
유망하다고 생각되는 몇 가지 아이디어가 있고,
이를 조사하여

01:18:43.040 --> 01:18:48.960
실제로 유망한지 아닌지 알아보고 싶다는 것입니다.  정말 그렇게
간단합니다.  그것은 시도입니다.  만약 우리가 일반화를 이해하기 위해 논의했던 아이디어가 옳다고 판명된다면,

01:19:01.040 --> 01:19:05.200
우리는 가치 있는 것을 얻을 수 있을 것이라고 생각합니다.
그들의 예측이 맞을까?  우리는

01:19:05.200 --> 01:19:10.720
연구를 하고 있습니다.  우리는 확실히 "연구의 시대
"에 있는 회사입니다.  우리는 진전을 이루고 있습니다.

01:19:10.720 --> 01:19:14.720
사실 우리는 지난 1년 동안 꽤 좋은 진전을 이루었
지만, 앞으로도 더 많은 진전과 연구가 필요합니다

01:19:14.720 --> 01:19:25.920
.  저는 그렇게 생각해요.  저는 그것을
목소리를 내고 참여하려는 시도로 봅니다.

01:19:29.840 --> 01:19:37.440
귀하의 공동 창업자이자 전임 CEO가
최근 Meta로 떠났고, 사람들은 "글쎄요,

01:19:37.440 --> 01:19:40.960
많은 획기적인 발전이 있었다면 그럴 일은

01:19:40.960 --> 01:19:49.200
없었을 것 같은데요."라고 물었습니다.  당신이 어떻게 반응할지 궁금하네요.
이를 위해, 잊었을지도 모르는 몇 가지 사실을 간단히 상기시켜드리겠습니다

01:19:49.200 --> 01:19:52.640
.
저는 이러한 사실들이

01:19:52.640 --> 01:19:59.200
맥락을 제공한다고 생각하며, 이것이 상황을 설명합니다.  당시
우리는

01:19:59.200 --> 01:20:10.720
320억 달러의 기업가치로 자금을 조달하고 있었고, 그때 메타가 들어와서
우리 인수를 제안했지만 저는 거절했습니다.

01:20:10.720 --> 01:20:19.920
하지만 어떤 면에서는 저의 전 공동 창업자가 그렇다고 말했어요.  그
결과 그는 단기 유동성도 많이 누릴 수 있었고

01:20:19.920 --> 01:20:24.880
,
SSI 출신으로 Meta에 합류한 유일한 사람이 되었습니다.

01:20:25.440 --> 01:20:31.200
SSI의 계획은

01:20:31.200 --> 01:20:35.600
인류 역사상 매우 중요한 시기인
초인적 지능이 나타나는 시기에 선두에 서는 회사가 되는 것 같습니다.

01:20:35.600 --> 01:20:39.360
당신은 초인적인 지능을 잘 발휘할 수 있는 방법에 대한 아이디어를 가지고 있습니다
.

01:20:39.360 --> 01:20:42.480
하지만 다른 회사들은 그들
만의 아이디어를 시도할 것입니다.

01:20:42.480 --> 01:20:48.400
SSI가
초지능을 성공적으로 구현하는 데 있어 갖는 독특한 점은 무엇입니까?   SSI를

01:20:48.400 --> 01:20:54.080
차별화하는 가장 중요한 요소는
기술적 접근 방식입니다.   저는 우리가 가치 있다고 생각하는

01:20:54.880 --> 01:21:01.520
다른 기술적 접근 방식을 가지고 있으며,
이를 추구하고 있습니다.

01:21:01.520 --> 01:21:06.160
저는 결국에는
전략의 융합이 이루어질 것이라고 주장합니다.   저는

01:21:06.160 --> 01:21:14.960
AI가 더욱 강력해짐에 따라 어느 순간 전략이 융합되어

01:21:14.960 --> 01:21:19.520
모든 사람이 전략이 무엇이어야 하는지 점점 더 명확하게 알게 될 것이라고 생각합니다.

01:21:19.520 --> 01:21:24.800
이는
서로 대화할 수 있는 방법을 찾아야 하며,

01:21:24.800 --> 01:21:37.920
최초의 실제 초지능 AI가
정렬되어 어떻게든 지각 있는 생명체를 돌보고,

01:21:37.920 --> 01:21:42.560
사람을 돌보고, 민주적이어야 한다는 것과 같은 내용이어야 합니다. 이러한 것들의
조합이어야 합니다.   저는

01:21:42.560 --> 01:21:50.400
이것이
모든 사람이 노력해야 할 조건이라고 생각합니다.

01:21:50.400 --> 01:21:57.040
SSI가 노력하는 것이 바로 그것입니다.
이번에는

01:21:57.040 --> 01:22:00.320
다른 모든 회사들이
자신들이 같은 목표를 향해 노력하고 있다는 것을 깨닫게 될 것이라고 생각합니다.    두고

01:22:00.320 --> 01:22:03.920
보자.  저는
AI가 더욱 강력해짐에 따라 세상이 정말로 바뀔 것이라고 생각합니다.

01:22:07.920 --> 01:22:11.680
저는 상황이 정말 달라질 것이고
사람들의 행동도 정말 달라질 것이라고 생각합니다.

01:22:12.480 --> 01:22:16.560
예측에 대해 말하자면,
당신이 설명하고 있는 이 시스템에 대한 예측은 어떤가요? 이 시스템은

01:22:16.560 --> 01:22:23.840
인간만큼 잘 학습할 수 있고,
결과적으로 초인이 될 수 있을까요?

01:22:23.840 --> 01:22:27.600
5~20년 정도 될 것 같아요.
5~20년?

01:22:27.600 --> 01:22:29.760
음.
저는

01:22:29.760 --> 01:22:35.680
당신이 세상이 어떻게 전개될지 보고 싶습니다.

01:22:35.680 --> 01:22:40.000
다른 회사들이
현재의 접근 방식을 계속 고수하고 몇 년 더 지나면 정체될 겁니다.

01:22:40.000 --> 01:22:44.720
여기서 "멈추다"는 것은 그들이
수천억 달러 이하의 수익을 올리지 못한다는 뜻인가요?

01:22:44.720 --> 01:22:57.200
당신은 정체가 무엇을 의미하는지 어떻게 생각하시나요?
저는 정체가 일어날 것 같다고 생각합니다. 모든 회사가

01:22:57.200 --> 01:23:00.788
매우 비슷하게 보일 것입니다
.   이런 내용일 수도 있겠네요

01:23:00.788 --> 01:23:02.640
.
확신할 수는 없지만, 제 생각에는

01:23:05.360 --> 01:23:10.320
정체되어 있더라도 이
회사들이 엄청난 수익을 낼 수 있을 것 같습니다.

01:23:10.320 --> 01:23:15.280
이익은 아닐지 몰라도
서로 차별화하기 위해 열심히 노력해야 하므로

01:23:15.280 --> 01:23:22.880
수익은 확실히 아닐 것입니다.
하지만 귀하의 모델에는

01:23:23.760 --> 01:23:27.920
올바른 솔루션이 등장하면
모든 회사가 융합할 것이라는 의미가 내포되어 있습니다.

01:23:27.920 --> 01:23:31.440
왜 그렇게 생각하는지 궁금하네요.
저는 그들의 정렬 전략에 대한 융합에 대해 더 많이 이야기했습니다

01:23:31.440 --> 01:23:34.720
.  기술적 접근 방식
에 있어서도 결국 융합이

01:23:34.720 --> 01:23:38.800
일어날 가능성이 높다고 생각하지만, 제가 언급한 융합은

01:23:38.800 --> 01:23:43.680
정렬 전략에 대한 융합을 뜻합니다.
정확히 무엇을 해야 할까요?

01:23:43.680 --> 01:23:46.880
저는 미래가 어떻게 전개될지 더 잘 이해하고 싶습니다
.

01:23:46.880 --> 01:23:50.320
현재 우리는 이렇게 다양한 회사를 운영하고 있으며,
이들의 접근 방식으로는 수익을 계속 창출할 수 있겠지만,

01:23:50.320 --> 01:23:56.560
인간과 같은 학습 능력을 갖추지는 못할 것으로 예상합니다.
그래서 이제 우리는 이렇게 다양한 종류의 회사를 갖게 되었습니다.

01:23:56.560 --> 01:23:59.520
우리는 여러분을 가지고 있고, 생각하는 기계도 있고, 그 외에도
많은 실험실이 있습니다.

01:24:00.160 --> 01:24:03.120
어쩌면 그들 중 한 명이
올바른 접근 방식을 알아낼 수도 있습니다.

01:24:03.120 --> 01:24:07.600
하지만 그들의 제품이 출시되면서
다른 사람들도 이 일을 하는 방법을 명확하게 알게 되었습니다.

01:24:07.600 --> 01:24:11.920
어떻게 해야 할지는 명확하지 않겠지만,
다른 것이

01:24:11.920 --> 01:24:17.600
가능하다는 것은 분명할 것이고, 그것이 정보입니다.
그러면 사람들은

01:24:17.600 --> 01:24:26.560
그것이 어떻게 작동하는지 알아내려고 노력할 것입니다.
하지만

01:24:26.560 --> 01:24:34.320
여기서 언급되지 않고 논의되지 않은 것 중 하나는
AI의 역량이 증가할 때마다

01:24:34.320 --> 01:24:40.960
어떤 종류의 변화가 있을 것이라고 생각하지만,
일이 처리되는 방식에 어떤 변화가 있는지는 정확히 모르겠습니다.

01:24:42.960 --> 01:24:47.280
중요한 내용일 것 같지만,
정확히 무엇인지는 잘 모르겠습니다.

01:24:50.320 --> 01:24:55.360
기본적으로,
해당 모델을 보유한 회사가 이러한 모든 이익을 얻을 것으로 기대할 수 있습니다.

01:24:55.360 --> 01:25:02.480
왜냐하면 그들은
세상에서 축적해 온 기술과 지식을 갖춘 모델을 보유하고 있기 때문입니다.   그

01:25:02.480 --> 01:25:05.760
혜택이 널리 분산될 것이라고 생각하는 이유는 무엇이며,

01:25:05.760 --> 01:25:11.200
지속적인 학습 루프를 처음 시작한 모델 회사에만 국한되지 않을 것이라고 생각하는 이유는 무엇입니까?

01:25:14.880 --> 01:25:25.680
제 생각에는 이런 일이 일어날 것 같습니다.
첫째,

01:25:25.680 --> 01:25:32.000
과거의 AI가 지금까지 어떤 방향으로 발전해 왔는지 살펴보겠습니다.
한 회사가 선제적으로 생산을 시작했고,

01:25:32.000 --> 01:25:40.400
다른 회사가 경쟁적으로 비슷한
제품을 생산한 뒤 일정 시간이 지나

01:25:40.400 --> 01:25:48.640
시장에서 경쟁을 시작하면서 가격이 낮아졌습니다.
그래서 저는 시장 관점에서 볼 때,

01:25:48.640 --> 01:25:54.800
거기서도 비슷한 일이 일어날 것이라고 생각합니다.
그런데 우리는 좋은 세상에 대해 이야기하고 있는 거예요.

01:25:56.640 --> 01:26:08.000
좋은 세상이란 무엇인가?
인간과 유사한 강력한 학습자가 있는 곳이기도 합니다.

01:26:08.000 --> 01:26:13.760
그런데, 초지능 AI의 사양에 관해 아직 논의하지 않은 또 다른 사항이 있는데,

01:26:13.760 --> 01:26:20.160
고려해 볼 가치가 있다고 생각합니다.
좁게 만들면

01:26:20.160 --> 01:26:24.320
유용하면서도 좁을 수 있습니다.
여러분은 수많은 좁은 범위의 초지능 AI를 가질 수 있습니다.

01:26:24.320 --> 01:26:31.600
하지만 여러분이 그런 시스템을 많이 가지고 있고,

01:26:32.720 --> 01:26:35.440
그것으로부터 많은 수익을 창출하는 회사가 있다고 가정해 보겠습니다.
그러면 또 다른

01:26:35.440 --> 01:26:40.240
회사가 들어와서 경쟁을 시작합니다.
경쟁은

01:26:40.240 --> 01:26:52.800
전문화를 통해 이루어집니다.  경쟁은 전문화를 좋아한다
.  시장에서도 볼 수 있고,

01:26:52.800 --> 01:26:55.280
진화에서도 볼 수 있습니다.
여러분은 다양한 틈새 시장을 갖게 될 것이고,

01:26:55.280 --> 01:26:59.520
각 틈새 시장을 점유하는 다양한 회사도 갖게 될 것입니다.

01:26:59.520 --> 01:27:08.640
이 세상에서는 어떤 AI 회사가

01:27:08.640 --> 01:27:13.360
정말 복잡한 경제 활동의 어떤 분야에서는 훨씬 더 뛰어나고,
다른 회사가 다른 분야에서는 더 뛰어나다고 말할 수도 있습니다.

01:27:13.360 --> 01:27:15.200
그리고 세 번째 회사는
소송에 정말 능숙합니다.

01:27:15.200 --> 01:27:19.840
이것이 인간과 같은
학습이 의미하는 바와 모순되지 않습니까?  그것은 배울 수 있다는 것입니다... 배울 수는 있지만,

01:27:19.840 --> 01:27:25.120
여러분은
학습을 축적해 왔습니다.  당신은 큰 투자를 했습니다.

01:27:25.120 --> 01:27:30.960
당신은 이 일에 정말, 정말 능숙하고, 정말 놀라울 정도로 능숙해지기 위해 많은 컴퓨팅을 투자했습니다
.

01:27:30.960 --> 01:27:34.080
다른 누군가가 엄청난 양
의 컴퓨팅과 엄청난 양의

01:27:34.080 --> 01:27:38.000
경험을 투자해 다른 분야에서 정말 능숙해졌습니다.
당신은 거기에 도달하기 위해 많은 인간적 학습을 적용했지만,

01:27:38.000 --> 01:27:44.160
지금은
다른 사람이 "보세요, 저는

01:27:44.160 --> 01:27:47.600
당신이 배운 것을 배우고 싶지 않습니다."라고 말할 만큼 높은 지점에 있습니다.
그러려면 여러

01:27:47.600 --> 01:27:53.360
회사가 동시에 인간과 유사한 지속적
학습 에이전트를 시작해서 서로

01:27:53.360 --> 01:27:58.000
다른 분기에서 서로 다른 트리 검색을 시작해야 할 것 같습니다.

01:27:58.000 --> 01:28:07.280
하지만 한 회사가 먼저 해당 에이전트를 확보하거나,
먼저 해당 학습자를 확보한다면... 글쎄요, 경제

01:28:09.120 --> 01:28:15.760
의 모든 직업을 생각해 보면
, 각 직업을 학습하는 인스턴스를 갖는 것이

01:28:15.760 --> 01:28:21.200
회사에는 쉬운 것처럼 보입니다.
타당한 주장이네요.  저는 그게 올바른 방향으로 흘러가지

01:28:21.200 --> 01:28:28.720
않을 거라고 강하게 직감합니다.
이런 주장은 이런 방향으로 갈 것이라고 하지만, 내

01:28:28.720 --> 01:28:36.240
강한 직감으로는 이런 방향으로 가지 않을 것 같다.
이론상으로는 이론과 실제 사이에 차이가 없습니다

01:28:36.240 --> 01:28:39.440
.  실제로는 그렇습니다.  저는
그것이 그 중 하나일 것이라고 생각합니다.

01:28:39.440 --> 01:28:44.000
많은 사람들의 재귀적
자기 계발 모델은 문자 그대로, 명확하게

01:28:44.000 --> 01:28:49.360
서버에 다양한 아이디어를 내놓는 백만 명의 일리아가 있을 것이며
, 이로 인해

01:28:49.360 --> 01:28:52.800
매우 빠르게 초지능이 등장하게 될 것이라고 말합니다.

01:28:52.800 --> 01:29:00.800
당신이 하고 있는 일이 얼마나 병렬화될 수 있는지에 대한 직감이 있나요?
일리아를 복사하는 데에는 어떤 이점이 있나요?   모르겠습니다

01:29:00.800 --> 01:29:09.440
.  저는 똑같은 생각을 하는 사람보다는 다르게 생각하는
사람을 원하기 때문에 수익은 확실히 줄어들 것이라고 생각합니다

01:29:09.440 --> 01:29:14.320
.
만약 나의 문자 그대로의 사본이 있다면,

01:29:14.320 --> 01:29:21.920
당신이 얼마나 더 많은 증분 가치를 얻을 수 있을지 모르겠습니다.
다르게 생각하는 사람,

01:29:21.920 --> 01:29:25.360
그게 바로 당신이 원하는 사람이에요.

01:29:25.360 --> 01:29:30.800
완전히 다른 회사에서 출시한, 서로

01:29:30.800 --> 01:29:35.920
겹치지 않는 데이터세트로 훈련된 다양한 모델을 살펴보면,
LLM이 서로 얼마나 유사한지 실제로 놀라울 정도로 잘 드러난다는 이유는 무엇일까요?

01:29:35.920 --> 01:29:39.520
아마도 데이터 세트가
보이는 것만큼 중복되지 않을 수도 있습니다.

01:29:39.520 --> 01:29:44.080
하지만
개인이

01:29:44.080 --> 01:29:46.960
미래의 AI보다 생산성이 낮을지라도
인간 팀이

01:29:46.960 --> 01:29:53.600
AI 팀보다 더 다양한 측면이 있다는 사실은 어떤 면에서는 타당할 수도 있습니다.
AI 간에 의미 있는 다양성을 어떻게 이끌어낼 수 있을까?   저는

01:29:53.600 --> 01:29:56.720
온도만 높이는 것은
횡설수설일 뿐이라고 생각합니다.

01:29:56.720 --> 01:30:01.360
여러분은 다양한 과학자들이 서로
다른 편견이나 생각을 가지고 있다는 것을 원합니다.

01:30:01.360 --> 01:30:04.720
AI 에이전트에서 그런 종류의 다양성을 어떻게 얻을 수 있나요?

01:30:04.720 --> 01:30:10.720
그래서 다양성이 없었던 이유는
사전 훈련 때문이라고 생각합니다.

01:30:10.720 --> 01:30:16.800
모든 사전 학습된 모델은
동일한 데이터로 사전 학습되었기 때문에 거의 동일합니다.

01:30:16.800 --> 01:30:20.960
이제 RL과 훈련 이후에는
차별화가 나타나기 시작합니다.

01:30:20.960 --> 01:30:24.960
왜냐하면 사람마다
다른 RL 훈련을 하기 때문입니다.   저는

01:30:26.400 --> 01:30:31.520
이전에 학습을 시작하기 위해

01:30:31.520 --> 01:30:38.000
데이터를 얻거나 다른 에이전트와
동등한 지능을 가진 에이전트를 매치하는 방법으로 셀프 플레이에 대한 힌트를 들은 적이 있습니다.

01:30:38.000 --> 01:30:46.800
LLM과 함께 이런 종류의 작업에 대한 공개 제안이 없는 이유를 어떻게 생각해야 할까요?   말씀드릴 것은

01:30:46.800 --> 01:30:52.320
두 가지입니다.
제가 셀프 플레이가 흥미롭다고 생각한 이유는

01:30:52.320 --> 01:31:00.800
데이터 없이 컴퓨팅만을 사용하여 모델을 만드는 방법을 제공했기 때문입니다.

01:31:00.800 --> 01:31:06.320
데이터가 궁극적인 병목 현상이라고 생각한다면,
컴퓨팅만을 사용하는 것은 매우 흥미로운 일입니다.

01:31:06.320 --> 01:31:15.760
그래서 흥미로운 거죠.
문제는 셀프 플레이, 적어도

01:31:15.760 --> 01:31:21.520
과거에 했던 방식, 즉 에이전트들이
어떻게든 서로 경쟁하는 방식은

01:31:21.520 --> 01:31:29.040
특정 기술 세트를 개발하는 데만 유용하다는 것입니다.
너무 좁아요.  협상,

01:31:29.040 --> 01:31:35.040
갈등, 특정 사회적 기술,
전략 수립 등과 같은 것에만 유용합니다.

01:31:35.040 --> 01:31:38.800
만약 당신이 그런 기술에 관심이 있다면,
셀프 플레이가 유용할 것입니다.

01:31:39.360 --> 01:31:46.880
사실, 저는 셀프플레이가 자리를 찾았다고 생각하지만
, 단지 다른 형태로만 가능할 뿐이라고 생각합니다.

01:31:48.080 --> 01:31:55.680
그래서 토론, 증명자-검증자와 같은 것들이
있는데, 일종의 LLM-as-a-Judge가 있어서

01:31:55.680 --> 01:32:00.080
자신의 작업에서 실수를 찾아내도록 동기를 부여받습니다.
이것이 정확히 자기 플레이라고 할 수는 없지만,

01:32:00.080 --> 01:32:04.720
사람들이 하고 있는 것과 관련된 적대적 설정이라고 생각합니다.

01:32:04.720 --> 01:32:12.800
실제로 셀프 플레이는
에이전트 간의 보다 일반적인 경쟁의 특별한 경우입니다.

01:32:13.600 --> 01:32:16.560
경쟁에 대한 자연스러운 반응은
다르려고 노력하는 것입니다.

01:32:16.560 --> 01:32:21.360
따라서 여러 에이전트를 한자리에 모아 놓고
"여러분 모두 어떤

01:32:21.360 --> 01:32:26.560
문제를 해결해야 하고, 당신은 에이전트로서
다른 사람들이 하는 일을 검사하고 있습니다."라고

01:32:26.560 --> 01:32:31.520
말하면, 그들은 "이미 이런
접근 방식을 취하고 있다면, 제가 그 방식을 따라야 할지 확실하지 않습니다.

01:32:31.520 --> 01:32:36.000
차별화된 것을 추구해야 합니다."라고 말할 것입니다.  그래서 저는 이와
같은 것이

01:32:36.000 --> 01:32:44.400
다양한 접근 방식에 대한 인센티브를 만들어낼 수 있다고 생각합니다.
마지막 질문: 연구의 맛이란 무엇인가?

01:32:44.400 --> 01:32:51.040
당신은 분명
세계에서

01:32:51.040 --> 01:33:01.280
AI 연구에 가장 뛰어난 감각을 가진 사람으로 여겨지는군요.
당신은

01:33:01.280 --> 01:33:05.040
AlexNet부터 GPT-3까지 딥 러닝 역사상 가장 중요한 사건들의 공동 저자였습니다.

01:33:05.040 --> 01:33:11.120
무슨 말씀이신가요?
이런 아이디어가 어떻게 떠오르는지 설명해주세요.   이 문제에 대해서는

01:33:11.120 --> 01:33:18.720
제가 직접 말씀드리겠습니다.
사람마다 다르게 하는 것 같아요.

01:33:18.720 --> 01:33:29.680
개인적으로 저를 이끄는 한 가지는 사람들이 어떤 사람인지 생각하되 올바르게 생각
함으로써 AI가 어떻게 되어야 하는지에 대한 미학입니다

01:33:29.680 --> 01:33:35.520
.
사람들이 잘못 생각하고 있다는 생각은 매우 쉽지만

01:33:35.520 --> 01:33:40.320
,
사람들에 대해 올바르게 생각한다는 것은 무엇을 의미할까요?  몇 가지

01:33:40.320 --> 01:33:48.080
예를 들어보겠습니다.  인공 뉴런이라는 아이디어는
뇌에서 직접 영감을 받은 것으로,

01:33:48.080 --> 01:33:52.720
훌륭한 아이디어입니다.  왜?  뇌에는
이렇게 다양한 기관이 있고, 주름도 있지만,

01:33:52.720 --> 01:33:56.400
주름 자체는 아마 중요하지 않을 겁니다.
왜 우리는 뉴런이 중요하다고 생각할까?

01:33:56.400 --> 01:34:01.280
왜냐하면 그 수가 많기 때문이죠.
그게 맞는 것 같아서 뉴런이 필요한 거죠.

01:34:01.280 --> 01:34:09.600
뉴런 간의 연결을 변경하는 로컬 학습 규칙이 필요합니다.

01:34:10.240 --> 01:34:15.600
뇌가 그렇게 한다고 생각됩니다.
분산 표현의 개념.

01:34:15.600 --> 01:34:19.920
뇌가
경험에 반응한다는 생각에 따라 우리의

01:34:19.920 --> 01:34:23.200
신경망은 경험으로부터 학습해야 합니다.
뇌는 경험으로부터 배우고,

01:34:24.480 --> 01:34:29.280
신경망도 경험으로부터 배워야 합니다.
당신은 스스로에게 뭔가 근본적인 것이 있는지, 아니면

01:34:29.280 --> 01:34:35.600
근본적이지 않은지 묻습니다.  사물이 마땅히 있어야 할 모습.
저는 그것이 제게 상당히 많은 영향을 미쳤다고 생각합니다.

01:34:35.600 --> 01:34:41.440
여러 각도에서 생각하고
아름다움, 아름다움과 단순함을 추구하면서요.

01:34:41.440 --> 01:34:46.400
추함, 추함이 있을 자리는 없어.
그것은 아름다움, 단순함, 우아함,

01:34:46.400 --> 01:34:49.360
뇌로부터 얻은 올바른 영감입니다.
이 모든 것들이

01:34:49.360 --> 01:34:53.120
동시에 존재해야 합니다.  이러한 요소가
많을수록

01:34:53.120 --> 01:34:58.640
상향식 믿음에 대한 자신감이 커집니다.

01:34:58.640 --> 01:35:04.560
실험이 당신의 생각과 모순될 때 당신을 지탱해주는 것은 상향식 믿음입니다.  항상
데이터를 신뢰한다면

01:35:04.560 --> 01:35:07.520
때로는
올바른 일을 하고 있을 수도 있지만 버그가 있을 수도 있습니다.

01:35:07.520 --> 01:35:11.040
하지만 버그가 있다는 사실을 모르죠.
버그가 있다는 것을 어떻게 알 수 있나요?

01:35:11.040 --> 01:35:14.960
디버깅을 계속해야 할지, 아니면
잘못된 방향이라는 결론을 내려야 할지 어떻게 알 수 있나요?  그것은

01:35:14.960 --> 01:35:20.720
상향식입니다.  일이 이렇게 되어야만 한다고 말할 수 있다.  이런 일이 반드시
일어나야 하므로,

01:35:20.720 --> 01:35:24.880
우리는 계속 노력해야 합니다.
이것이 상향식이며,

01:35:25.520 --> 01:35:31.600
뇌의 다면적인 아름다움과 영감에 기초하고 있습니다.
좋습니다. 여기서 마치겠습니다.

01:35:31.600 --> 01:35:34.960
매우 감사합니다.
일리아, 정말 고맙습니다.

01:35:34.960 --> 01:35:36.480
괜찮은.  감사합니다.
정말 좋았어요.   네

01:35:36.480 --> 01:35:39.040
, 즐거웠어요.
네, 저도요.

