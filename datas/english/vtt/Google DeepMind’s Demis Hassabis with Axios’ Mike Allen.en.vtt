WEBVTT

00:00:00.080 --> 00:00:05.200
Thank you very much. Big finish. I'm

00:00:02.320 --> 00:00:07.759
Mike Allen, co-founder of Axio on bealf

00:00:05.200 --> 00:00:10.000
of my co-founders, Roy Schwarz, Jim

00:00:07.759 --> 00:00:12.320
Vanhey. Thank you to all of you who for

00:00:10.000 --> 00:00:15.040
coming up nine years now have been fans

00:00:12.320 --> 00:00:17.039
of Axios and thank you for turning out

00:00:15.040 --> 00:00:19.840
here in San Francisco in this historic

00:00:17.039 --> 00:00:23.439
bank, this very cool uh setting for this

00:00:19.840 --> 00:00:26.240
Axios AI plus SF summit. Uh welcome to

00:00:23.439 --> 00:00:29.760
all of you around the world. for our big

00:00:26.240 --> 00:00:33.520
finish. Deis Hassabis, PhD co-founder

00:00:29.760 --> 00:00:37.120
and CEO of Google Deep Mind. He's a

00:00:33.520 --> 00:00:40.640
neuroscientist and entrepreneur and AI

00:00:37.120 --> 00:00:44.640
pioneer. Demis was a chess prodigy at

00:00:40.640 --> 00:00:47.280
five, a Nobel laureate at 48. He's a

00:00:44.640 --> 00:00:51.745
Britishborn genius. He's been kned.

00:00:47.280 --> 00:00:53.765
Deisabas, welcome to Axios.

00:00:51.745 --> 00:00:53.765
[applause]

00:00:56.960 --> 00:01:00.079
Thank you so much. Thanks for having me.

00:00:58.640 --> 00:01:00.559
>> We've been looking forward to this. We

00:01:00.079 --> 00:01:01.680
appreciate

00:01:00.559 --> 00:01:04.879
>> to be here.

00:01:01.680 --> 00:01:08.240
>> It was just over 400 days ago that you

00:01:04.879 --> 00:01:10.320
found out you were a Nobel laureate. And

00:01:08.240 --> 00:01:11.600
you said in that moment, you said this

00:01:10.320 --> 00:01:13.200
is surreal.

00:01:11.600 --> 00:01:14.159
>> This is the big one.

00:01:13.200 --> 00:01:16.000
>> Yeah.

00:01:14.159 --> 00:01:17.360
>> What has changed since then about your

00:01:16.000 --> 00:01:18.640
life and work? What has it made

00:01:17.360 --> 00:01:20.080
possible?

00:01:18.640 --> 00:01:21.920
>> Um, well, look, it's still pretty

00:01:20.080 --> 00:01:24.799
surreal actually. still hasn't fully

00:01:21.920 --> 00:01:26.080
sunk in, but uh it has made quite a big

00:01:24.799 --> 00:01:27.759
difference. The the thing it makes a

00:01:26.080 --> 00:01:30.960
difference to is when you speak to

00:01:27.759 --> 00:01:32.479
people not in your field, including you

00:01:30.960 --> 00:01:33.920
know big government people, things like

00:01:32.479 --> 00:01:36.479
that who maybe don't know that much

00:01:33.920 --> 00:01:38.960
about AI. If you, you know, you have the

00:01:36.479 --> 00:01:41.040
Nobel Prize, it's a sort of shortcut to

00:01:38.960 --> 00:01:42.479
almost anyone to to know that you're,

00:01:41.040 --> 00:01:43.920
you know, you're expert in your field.

00:01:42.479 --> 00:01:47.040
So, it's it's going to be useful, I

00:01:43.920 --> 00:01:50.960
think, in the future. and [laughter]

00:01:47.040 --> 00:01:52.880
and you had endless resources at your

00:01:50.960 --> 00:01:55.200
disposal. Are there new resources that

00:01:52.880 --> 00:01:56.000
you have or that you think you can tap

00:01:55.200 --> 00:01:57.439
now?

00:01:56.000 --> 00:01:59.439
>> Not really. I mean, you're right. We

00:01:57.439 --> 00:02:01.119
we're lucky at at Google at Deep Mind.

00:01:59.439 --> 00:02:03.040
We have we have a lot of resources.

00:02:01.119 --> 00:02:05.200
They're not endless. We always need more

00:02:03.040 --> 00:02:07.759
more compute. Uh no matter how much

00:02:05.200 --> 00:02:09.119
compute you have, but um but we have,

00:02:07.759 --> 00:02:10.800
you know, a lot of great things, which

00:02:09.119 --> 00:02:13.120
is why we're able to do such a broad

00:02:10.800 --> 00:02:15.840
portfolio of things. But it's mostly

00:02:13.120 --> 00:02:18.080
again this uh this platform it gives you

00:02:15.840 --> 00:02:20.319
to basically speak out about things that

00:02:18.080 --> 00:02:21.920
you care about. And I haven't done a lot

00:02:20.319 --> 00:02:23.280
of that yet, but I think it will be

00:02:21.920 --> 00:02:25.280
important. Maybe we're going to talk

00:02:23.280 --> 00:02:27.599
about AI safety and other things. I

00:02:25.280 --> 00:02:30.400
think uh the Nobel and the platform that

00:02:27.599 --> 00:02:32.560
gives you uh could be useful for that.

00:02:30.400 --> 00:02:34.080
>> And what's on the short list of in

00:02:32.560 --> 00:02:36.720
addition to AI safety that you think

00:02:34.080 --> 00:02:37.360
you'll be talking more about using your

00:02:36.720 --> 00:02:40.000
platform?

00:02:37.360 --> 00:02:42.080
>> Yeah, well it's not just about uh safety

00:02:40.000 --> 00:02:44.000
in the long term. AGI safety obviously I

00:02:42.080 --> 00:02:47.440
think we think a lot about that but it's

00:02:44.000 --> 00:02:49.120
also about responsible use of AI today.

00:02:47.440 --> 00:02:51.840
Uh what are the kinds of things we

00:02:49.120 --> 00:02:54.800
should be using AI to to improve and to

00:02:51.840 --> 00:02:56.080
to power up and to accelerate and maybe

00:02:54.800 --> 00:02:59.120
you know what sorts of things we should

00:02:56.080 --> 00:03:00.720
be careful about um uh uh in the even in

00:02:59.120 --> 00:03:03.040
the near term. So I think that's one

00:03:00.720 --> 00:03:06.000
thing I think also just getting society

00:03:03.040 --> 00:03:07.519
ready for what's to come. you know, AGI

00:03:06.000 --> 00:03:11.120
probably the most transformative moment

00:03:07.519 --> 00:03:14.560
in human history is on the horizon and

00:03:11.120 --> 00:03:16.879
um we need to get prepared as a society

00:03:14.560 --> 00:03:18.800
um and as a species and I think of

00:03:16.879 --> 00:03:21.519
course governments and other important

00:03:18.800 --> 00:03:24.080
people uh uh other important leaders are

00:03:21.519 --> 00:03:25.680
going to be critical in that and I think

00:03:24.080 --> 00:03:28.800
having having something like the Nobel

00:03:25.680 --> 00:03:31.200
platform opens pretty much any door. One

00:03:28.800 --> 00:03:34.879
of the things that distinguishes you is

00:03:31.200 --> 00:03:37.760
you're deep in the science and yet you

00:03:34.879 --> 00:03:40.080
also are on the front line of this fight

00:03:37.760 --> 00:03:43.599
and this race among companies,

00:03:40.080 --> 00:03:46.400
hyperscalers, superpowers and you sort

00:03:43.599 --> 00:03:49.920
of in the mold of Steve Jobs, you also

00:03:46.400 --> 00:03:52.000
have a product mind. You want to create

00:03:49.920 --> 00:03:55.040
delightful things for people, but you

00:03:52.000 --> 00:03:56.799
always say you're a scientist first.

00:03:55.040 --> 00:03:58.480
>> Yeah, science. I'm a scientist first.

00:03:56.799 --> 00:04:00.319
The reason I say that is that's the

00:03:58.480 --> 00:04:03.360
that's the sort of default approach I

00:04:00.319 --> 00:04:05.200
take to everything. So um and what I

00:04:03.360 --> 00:04:08.080
mean by that is the scientific method

00:04:05.200 --> 00:04:09.680
really that way of thinking. Um I really

00:04:08.080 --> 00:04:11.760
love the I mean I think it's the most

00:04:09.680 --> 00:04:14.159
the scientific method is is the most

00:04:11.760 --> 00:04:16.000
important maybe idea humanity's ever

00:04:14.159 --> 00:04:17.919
had. Um you know created the

00:04:16.000 --> 00:04:19.680
enlightenment and then modern science.

00:04:17.919 --> 00:04:22.320
So basically, modern civilization

00:04:19.680 --> 00:04:24.560
depends on on on this on this idea of

00:04:22.320 --> 00:04:26.720
scientific method and experimentation

00:04:24.560 --> 00:04:28.639
and then updating your hypothesis and so

00:04:26.720 --> 00:04:30.560
on. And I think it's an incredibly

00:04:28.639 --> 00:04:32.080
powerful method, but I think it can be

00:04:30.560 --> 00:04:33.600
applied to more than just science. I

00:04:32.080 --> 00:04:36.320
think it can be applied to everyday

00:04:33.600 --> 00:04:38.240
living and indeed business. Um, and

00:04:36.320 --> 00:04:41.600
that's what I've tried to do is sort of

00:04:38.240 --> 00:04:44.320
take that uh uh to its limit. And I

00:04:41.600 --> 00:04:46.000
think that's what gives us um you know

00:04:44.320 --> 00:04:48.080
advantage in some ways as a as a

00:04:46.000 --> 00:04:50.160
research organization as an engineering

00:04:48.080 --> 00:04:52.080
organization. Yes, we're in the middle

00:04:50.160 --> 00:04:54.479
of this ferocious probably the most

00:04:52.080 --> 00:04:57.919
ferocious competitive battle maybe tech

00:04:54.479 --> 00:04:59.440
has ever seen. Um and uh but one of the

00:04:57.919 --> 00:05:01.759
things that I think gives us an edge is

00:04:59.440 --> 00:05:04.639
is the rigor and precision we bring to

00:05:01.759 --> 00:05:06.160
our work. um because um we have a

00:05:04.639 --> 00:05:08.720
scientific method sort of at the heart

00:05:06.160 --> 00:05:10.400
of it and we blend world-class research

00:05:08.720 --> 00:05:12.240
with world-class engineering with

00:05:10.400 --> 00:05:14.479
world-class infrastructure and I think

00:05:12.240 --> 00:05:16.639
you need all three of those things to be

00:05:14.479 --> 00:05:18.320
at the frontier of something like AI and

00:05:16.639 --> 00:05:20.400
I think you know we we're we're sort of

00:05:18.320 --> 00:05:23.600
pretty unique in having uh worldclass

00:05:20.400 --> 00:05:26.560
capabilities in in all those areas. Um

00:05:23.600 --> 00:05:28.639
yeah so in Axio fashion we're going to

00:05:26.560 --> 00:05:31.680
divide our conversation between zoom out

00:05:28.639 --> 00:05:35.199
and zoom in. So, zoom out uh getting

00:05:31.680 --> 00:05:37.039
your priceless uh mind on the state of

00:05:35.199 --> 00:05:39.199
AI. So, we're going to talk about the

00:05:37.039 --> 00:05:41.840
blunt state of AI. And what I'm going to

00:05:39.199 --> 00:05:43.840
ask from you is given the known knowns

00:05:41.840 --> 00:05:46.639
today, be blunt,

00:05:43.840 --> 00:05:47.360
>> clinical, no hype, no soft selling. Can

00:05:46.639 --> 00:05:48.240
we do that?

00:05:47.360 --> 00:05:50.639
>> I'll do my best.

00:05:48.240 --> 00:05:53.680
>> All right. Um

00:05:50.639 --> 00:05:55.759
what does the next 12 months of progress

00:05:53.680 --> 00:05:57.759
look like? What do you believe that if

00:05:55.759 --> 00:05:59.440
if we sit here a year from today and I

00:05:57.759 --> 00:06:00.639
would love to uh what will have changed

00:05:59.440 --> 00:06:02.400
in the world?

00:06:00.639 --> 00:06:06.000
>> Um I think the things that that we're

00:06:02.400 --> 00:06:09.520
we're pressing hard on are um uh the

00:06:06.000 --> 00:06:11.280
convergence of modalities. So you Gemini

00:06:09.520 --> 00:06:12.479
which is our main foundation model has

00:06:11.280 --> 00:06:14.880
always been multimodal from the

00:06:12.479 --> 00:06:17.600
beginning. It takes images, video, uh

00:06:14.880 --> 00:06:20.080
text, audio and then can produce now

00:06:17.600 --> 00:06:22.160
increasingly produce those uh uh types

00:06:20.080 --> 00:06:25.120
of outputs as well. Um, and I think

00:06:22.160 --> 00:06:27.680
we're getting some really interesting uh

00:06:25.120 --> 00:06:29.280
cross-pollination by being multimodal.

00:06:27.680 --> 00:06:32.319
One the best example of that is our

00:06:29.280 --> 00:06:34.240
latest image model NO Banana Pro which

00:06:32.319 --> 00:06:36.560
um I think shows some astonishing sort

00:06:34.240 --> 00:06:38.000
of understanding of visuals and it can

00:06:36.560 --> 00:06:40.160
kind of you know create infographics

00:06:38.000 --> 00:06:41.520
that are really accurate and so on. So I

00:06:40.160 --> 00:06:44.160
think over the next year you're going to

00:06:41.520 --> 00:06:46.319
see that uh uh progress a lot and I

00:06:44.160 --> 00:06:47.919
think for example in video when that

00:06:46.319 --> 00:06:49.199
converges with the language models

00:06:47.919 --> 00:06:51.039
you're going to be see some very

00:06:49.199 --> 00:06:52.720
interesting combinations of capabilities

00:06:51.039 --> 00:06:54.000
there. I think the other things we're

00:06:52.720 --> 00:06:55.919
going to see over the next year and I'm

00:06:54.000 --> 00:06:58.479
personally working on is world models.

00:06:55.919 --> 00:07:00.960
So uh we have this um uh uh system

00:06:58.479 --> 00:07:02.880
called Genie Genie 3 which is like an

00:07:00.960 --> 00:07:04.160
interactive video model you can think

00:07:02.880 --> 00:07:05.759
about. So you can sort of generate a

00:07:04.160 --> 00:07:07.199
video but then you can start walking

00:07:05.759 --> 00:07:09.280
around it like you're in a game or

00:07:07.199 --> 00:07:12.240
simulation and it stays coherent for a

00:07:09.280 --> 00:07:14.800
minute. I think that's very exciting. Um

00:07:12.240 --> 00:07:17.039
and then uh you know maybe the other

00:07:14.800 --> 00:07:18.160
thing is a agent based systems. So we I

00:07:17.039 --> 00:07:19.919
think the field's been talking a lot

00:07:18.160 --> 00:07:22.080
about agents but then they're not

00:07:19.919 --> 00:07:22.800
reliable yet enough to do full tasks.

00:07:22.080 --> 00:07:24.400
But I think over the next

00:07:22.800 --> 00:07:26.400
>> we've heard a lot about that today here

00:07:24.400 --> 00:07:28.479
on the Axia stage. What would you say a

00:07:26.400 --> 00:07:31.120
year from now? How will agents have

00:07:28.479 --> 00:07:32.720
progressed? What's an example of how it

00:07:31.120 --> 00:07:33.520
will work in everyday life a year from

00:07:32.720 --> 00:07:35.440
now?

00:07:33.520 --> 00:07:36.960
>> Well, look, I we we have this concept of

00:07:35.440 --> 00:07:39.919
a universal assistant that we want

00:07:36.960 --> 00:07:41.280
Gemini eventually to become. Uh I think

00:07:39.919 --> 00:07:42.479
this is also you're going to see from us

00:07:41.280 --> 00:07:45.680
over the next year. This will be on on

00:07:42.479 --> 00:07:47.280
on more devices as well. By universal,

00:07:45.680 --> 00:07:49.440
we mean it's not just on your computer

00:07:47.280 --> 00:07:50.960
or your laptop or your or your phone,

00:07:49.440 --> 00:07:54.720
but maybe comes around with you on

00:07:50.960 --> 00:07:56.240
glasses or other devices. And um I think

00:07:54.720 --> 00:07:58.639
it needs you know we want to create

00:07:56.240 --> 00:08:01.199
something that is useful to you in your

00:07:58.639 --> 00:08:03.120
everyday life that you consult many

00:08:01.199 --> 00:08:04.720
times a day. it becomes a part of the

00:08:03.120 --> 00:08:06.639
fabric of your life and it just improves

00:08:04.720 --> 00:08:08.479
your productivity but also your personal

00:08:06.639 --> 00:08:10.800
life you know recommendations for books

00:08:08.479 --> 00:08:13.680
and films and other or activities that

00:08:10.800 --> 00:08:15.120
you'd like and but yeah so but agents at

00:08:13.680 --> 00:08:18.560
the moment they can't comp you can't

00:08:15.120 --> 00:08:20.080
delegate to them uh a whole task and be

00:08:18.560 --> 00:08:23.280
sure they're going to complete that

00:08:20.080 --> 00:08:25.039
entire task uh uh completely reliably

00:08:23.280 --> 00:08:26.479
>> but a year from now you think they will

00:08:25.039 --> 00:08:29.599
>> I think a year from now we'll start

00:08:26.479 --> 00:08:34.159
having agents that uh are close uh to

00:08:29.599 --> 00:08:37.680
doing that and bullcase, barecase, what

00:08:34.159 --> 00:08:40.959
is the best case for what AI can do for

00:08:37.680 --> 00:08:43.039
the world and what do you fear most?

00:08:40.959 --> 00:08:44.480
>> Well, look, the the the the the best

00:08:43.039 --> 00:08:45.839
case scenario that that I've always

00:08:44.480 --> 00:08:48.240
dreamed about and why I've worked my

00:08:45.839 --> 00:08:49.920
whole life on on on AI and you know

00:08:48.240 --> 00:08:52.080
getting closer to this moment we've been

00:08:49.920 --> 00:08:56.000
working towards for decades now, many of

00:08:52.080 --> 00:08:57.920
us is um uh a kind of I somes call it

00:08:56.000 --> 00:09:00.080
radical abundance. So this idea we

00:08:57.920 --> 00:09:02.640
solved a lot of the biggest issues

00:09:00.080 --> 00:09:05.600
confronting uh society and humanity

00:09:02.640 --> 00:09:08.000
today. So whether that's free uh uh

00:09:05.600 --> 00:09:10.160
renewable clean energy, maybe we sold

00:09:08.000 --> 00:09:13.120
fusion or better battery optimal

00:09:10.160 --> 00:09:15.120
batteries and and solar uh materials,

00:09:13.120 --> 00:09:17.600
semiconductors, you know, material

00:09:15.120 --> 00:09:19.760
science. We've solved a lot of diseases.

00:09:17.600 --> 00:09:21.360
So then we're in a situation where, you

00:09:19.760 --> 00:09:24.880
know, we're in this new era, post

00:09:21.360 --> 00:09:27.120
scarcity era, and we're potentially, you

00:09:24.880 --> 00:09:29.120
know, humanity's is is flourishing and

00:09:27.120 --> 00:09:30.880
traveling to the stars and spreading

00:09:29.120 --> 00:09:33.680
consciousness to the to the galaxy.

00:09:30.880 --> 00:09:37.600
>> And what do you fear most?

00:09:33.680 --> 00:09:39.839
>> Well, even that utopian kind of view has

00:09:37.600 --> 00:09:42.720
some questions around it about what will

00:09:39.839 --> 00:09:44.480
be um our purpose as humans if there are

00:09:42.720 --> 00:09:45.600
these technologies and that are out

00:09:44.480 --> 00:09:47.279
there that are solving all these

00:09:45.600 --> 00:09:48.720
problems. all be left to solve. You

00:09:47.279 --> 00:09:50.399
know, I worry about that as a scientist

00:09:48.720 --> 00:09:52.320
and you know, the scientific method

00:09:50.399 --> 00:09:55.200
even. So, there's that, but there's also

00:09:52.320 --> 00:09:57.360
obviously the the well-known uh down

00:09:55.200 --> 00:10:01.120
challenges and risks with AI of well,

00:09:57.360 --> 00:10:04.480
twofold. One is bad actors um uh using

00:10:01.120 --> 00:10:06.240
AI for harmful ends um or the AI itself

00:10:04.480 --> 00:10:09.040
as it gets closer to AGI and becomes

00:10:06.240 --> 00:10:11.120
more gentic um it goes off the rails in

00:10:09.040 --> 00:10:13.760
some way that harms humanity.

00:10:11.120 --> 00:10:17.120
>> So, you mentioned going off the rails.

00:10:13.760 --> 00:10:19.120
Um, how worried are you about these

00:10:17.120 --> 00:10:20.800
catastrophic outcomes? Your level of

00:10:19.120 --> 00:10:24.160
concern? I'm just going to rattle them

00:10:20.800 --> 00:10:25.440
off. One, pathogens created by an evil

00:10:24.160 --> 00:10:26.640
actor using AI.

00:10:25.440 --> 00:10:28.560
>> Mhm.

00:10:26.640 --> 00:10:30.399
>> I think that's definitely one of the one

00:10:28.560 --> 00:10:31.920
of the bad use case scenarios that we

00:10:30.399 --> 00:10:35.600
have to guard against for sure.

00:10:31.920 --> 00:10:36.959
>> Energy or water cyber terror using AI by

00:10:35.600 --> 00:10:38.720
a foreign actor.

00:10:36.959 --> 00:10:40.720
>> Yeah, that that's probably almost

00:10:38.720 --> 00:10:42.720
already happening now, I would say.

00:10:40.720 --> 00:10:45.200
Maybe not with very sophisticated AI

00:10:42.720 --> 00:10:47.519
yet, but I think that's the most obvious

00:10:45.200 --> 00:10:49.600
vulnerable vector. Um, and which is why

00:10:47.519 --> 00:10:51.600
we focus quite a lot and we are focusing

00:10:49.600 --> 00:10:54.160
quite a lot as Google and as DeepMind on

00:10:51.600 --> 00:10:56.240
on AI for cyber security. So, so to

00:10:54.160 --> 00:10:57.200
power up the defensive side of that

00:10:56.240 --> 00:11:00.160
equation,

00:10:57.200 --> 00:11:01.839
>> AI operating outside human control on

00:11:00.160 --> 00:11:03.360
its own.

00:11:01.839 --> 00:11:05.680
Well, this goes back to the agentic

00:11:03.360 --> 00:11:07.600
stuff where I think as that becomes more

00:11:05.680 --> 00:11:09.120
sophisticated and it's clear why the

00:11:07.600 --> 00:11:10.880
industry will build those things because

00:11:09.120 --> 00:11:13.040
they'll be more useful as things like

00:11:10.880 --> 00:11:15.680
assistance. Um, so they're definitely

00:11:13.040 --> 00:11:17.760
going to happen, but the more aentic and

00:11:15.680 --> 00:11:20.560
autonomous they are, the more room there

00:11:17.760 --> 00:11:22.720
is for these things to uh deviate from

00:11:20.560 --> 00:11:24.880
what you maybe had intended when you

00:11:22.720 --> 00:11:26.959
gave the initial instruction or the

00:11:24.880 --> 00:11:28.880
initial goal. So this is a very active

00:11:26.959 --> 00:11:31.920
area of research which is to how to make

00:11:28.880 --> 00:11:34.160
sure that systems that maybe are capable

00:11:31.920 --> 00:11:37.279
of continual learning or online learning

00:11:34.160 --> 00:11:39.279
stay uh within the guard rails that that

00:11:37.279 --> 00:11:42.959
you set. I mean, I think the good news

00:11:39.279 --> 00:11:45.519
is um because AI is become such so big

00:11:42.959 --> 00:11:48.000
commercially and for enterprises, if you

00:11:45.519 --> 00:11:50.399
think about renting or selling one of

00:11:48.000 --> 00:11:52.880
your agents as a model provider, leading

00:11:50.399 --> 00:11:55.600
model provider to another big business,

00:11:52.880 --> 00:11:57.680
those businesses will want guarantees

00:11:55.600 --> 00:12:00.160
around the agents behavior, what it does

00:11:57.680 --> 00:12:02.959
with their data, what it does with their

00:12:00.160 --> 00:12:04.000
the customers. And if those things go

00:12:02.959 --> 00:12:06.000
wrong, they're not going to be

00:12:04.000 --> 00:12:08.240
existential in any way, but you'll lose

00:12:06.000 --> 00:12:10.160
the business for sure. So because why

00:12:08.240 --> 00:12:11.440
would that business enterprise go with

00:12:10.160 --> 00:12:12.560
that provider? They would choose a

00:12:11.440 --> 00:12:14.560
different provider that was more

00:12:12.560 --> 00:12:16.800
responsible and had better guarantees.

00:12:14.560 --> 00:12:18.240
So I think what's great about that is um

00:12:16.800 --> 00:12:21.600
that that will it will sort of

00:12:18.240 --> 00:12:24.240
capitalism will reward sort of naturally

00:12:21.600 --> 00:12:27.839
uh ideally more responsible actors

00:12:24.240 --> 00:12:29.680
>> but it's possible that the AI could jump

00:12:27.839 --> 00:12:31.360
the moat, jump the guard rail

00:12:29.680 --> 00:12:33.600
>> potentially if done wrong. I mean it's

00:12:31.360 --> 00:12:36.240
there was always a possibility. We're we

00:12:33.600 --> 00:12:37.680
nobody really knows what the um that's

00:12:36.240 --> 00:12:40.560
one of the big unknowns. I think it's

00:12:37.680 --> 00:12:42.720
non zero that potential. Uh so it's

00:12:40.560 --> 00:12:45.440
worth very seriously considering and

00:12:42.720 --> 00:12:47.200
mitigating against but um you know I

00:12:45.440 --> 00:12:48.720
hear people talk you know give very

00:12:47.200 --> 00:12:50.399
precise percentages about what the

00:12:48.720 --> 00:12:52.160
chances of these poom

00:12:50.399 --> 00:12:54.079
>> a p doom which I think is kind of

00:12:52.160 --> 00:12:55.519
nonsense because no one knows what it

00:12:54.079 --> 00:12:57.279
is. What I know is it's

00:12:55.519 --> 00:12:58.399
>> so you don't you don't quantify it but

00:12:57.279 --> 00:13:01.839
you say it's

00:12:58.399 --> 00:13:04.480
>> it's non zero. So clearly if your PDM is

00:13:01.839 --> 00:13:06.639
non zero then you you you know you must

00:13:04.480 --> 00:13:07.920
put significant resources and and and

00:13:06.639 --> 00:13:10.639
attention on that.

00:13:07.920 --> 00:13:13.440
>> Where is the US winning the AI race

00:13:10.639 --> 00:13:16.079
against China and where are we losing?

00:13:13.440 --> 00:13:19.760
>> Um I I I think that we're still in the

00:13:16.079 --> 00:13:21.519
in the US and in the west um in the lead

00:13:19.760 --> 00:13:24.720
uh if you look at the at the latest

00:13:21.519 --> 00:13:26.480
benchmarks and um the latest systems but

00:13:24.720 --> 00:13:27.600
they're not you know China is not far

00:13:26.480 --> 00:13:28.959
behind. If you look at the latest

00:13:27.600 --> 00:13:30.639
DeepSseek or the latest smallers,

00:13:28.959 --> 00:13:32.800
they're very good and they there are

00:13:30.639 --> 00:13:34.800
some very capable teams there. So maybe

00:13:32.800 --> 00:13:37.279
we're, you know, the lead is only a

00:13:34.800 --> 00:13:37.760
matter of months as opposed to years at

00:13:37.279 --> 00:13:40.160
this point.

00:13:37.760 --> 00:13:42.399
>> Because when you put chips aside, AI,

00:13:40.160 --> 00:13:45.040
China probably is winning.

00:13:42.399 --> 00:13:47.040
>> Um, no, I think chips is one thing, but

00:13:45.040 --> 00:13:48.639
I think algorithmically, innovation

00:13:47.040 --> 00:13:51.200
wise, I think the West still has the

00:13:48.639 --> 00:13:54.639
edge. So I don't think any of the

00:13:51.200 --> 00:13:56.880
Chinese models or or companies have

00:13:54.639 --> 00:14:00.480
shown they can innovate on

00:13:56.880 --> 00:14:02.800
algorithmically something new that um

00:14:00.480 --> 00:14:06.399
beyond the state-of-the-art they they

00:14:02.800 --> 00:14:08.639
they've been very good at um uh fast

00:14:06.399 --> 00:14:09.519
sort of following the the current uh

00:14:08.639 --> 00:14:11.360
state-of-the-art.

00:14:09.519 --> 00:14:13.040
>> Our last zoom out question and you're

00:14:11.360 --> 00:14:15.839
going to like this one. What's the most

00:14:13.040 --> 00:14:18.560
astonishing thing about AI that you

00:14:15.839 --> 00:14:20.399
think gets shockingly little attention?

00:14:18.560 --> 00:14:22.079
The most astonishing thing about AI that

00:14:20.399 --> 00:14:23.680
gets shocking little little attention.

00:14:22.079 --> 00:14:25.360
>> Wow. Yeah. I think if I think of the

00:14:23.680 --> 00:14:28.560
things we're working on and already have

00:14:25.360 --> 00:14:30.959
working, it's the um multimodal

00:14:28.560 --> 00:14:31.279
understanding these models have. Like if

00:14:30.959 --> 00:14:33.199
you

00:14:31.279 --> 00:14:35.760
>> and multimodal video,

00:14:33.199 --> 00:14:37.760
>> yes, video uh image and and I mean

00:14:35.760 --> 00:14:40.480
audio, but I'm thinking specifically

00:14:37.760 --> 00:14:43.279
video actually. So if you if you give

00:14:40.480 --> 00:14:44.880
Gemini a YouTube video to process, you

00:14:43.279 --> 00:14:47.040
can ask it all sorts of incredible

00:14:44.880 --> 00:14:48.880
things about the video that it's just

00:14:47.040 --> 00:14:51.120
sort of mind-blowing to me that it can

00:14:48.880 --> 00:14:53.279
understand sort of conceptually in a lot

00:14:51.120 --> 00:14:54.560
of cases like not always but in many

00:14:53.279 --> 00:14:55.839
really impressive cases what's

00:14:54.560 --> 00:14:57.680
happening. Can understand

00:14:55.839 --> 00:15:00.399
>> example of a question. Um well I've

00:14:57.680 --> 00:15:01.839
asked questions on on like um you know

00:15:00.399 --> 00:15:03.279
one of I mean look this was just

00:15:01.839 --> 00:15:06.320
something I tested Gemini on the other

00:15:03.279 --> 00:15:09.040
day was was um I love the film Fight

00:15:06.320 --> 00:15:10.800
Club and uh there's some scene in it I

00:15:09.040 --> 00:15:12.320
think where Brad Pitt or or or maybe

00:15:10.800 --> 00:15:16.480
it's Ed Norton I can't remember takes

00:15:12.320 --> 00:15:19.120
off his ring uh uh before having a fight

00:15:16.480 --> 00:15:21.199
and the sort of um I asked you know

00:15:19.120 --> 00:15:23.360
Gemini like what's the significance of

00:15:21.199 --> 00:15:24.720
of of of that of that [snorts] action

00:15:23.360 --> 00:15:26.079
and you know he came up with a very

00:15:24.720 --> 00:15:28.399
interesting sort of philosophical point

00:15:26.079 --> 00:15:30.079
about leaving behind uh everyday life

00:15:28.399 --> 00:15:33.120
and and just sort of symbolically

00:15:30.079 --> 00:15:35.120
showing that um was you know very

00:15:33.120 --> 00:15:37.440
interesting kind of meta insight that

00:15:35.120 --> 00:15:38.720
that you know these systems have now and

00:15:37.440 --> 00:15:39.920
I think if you use it the other thing

00:15:38.720 --> 00:15:41.279
that's sort of not appreciated is like

00:15:39.920 --> 00:15:43.120
we have this thing called Gemini Live

00:15:41.279 --> 00:15:45.760
where you can point your phone at

00:15:43.120 --> 00:15:47.680
something and say you're a mechanic uh

00:15:45.760 --> 00:15:49.519
uh it can actually just help you with

00:15:47.680 --> 00:15:51.360
whatever you know task you have in front

00:15:49.519 --> 00:15:52.800
of you ideally that should be glasses

00:15:51.360 --> 00:15:55.040
because you want to have your hands free

00:15:52.800 --> 00:15:58.240
really for that um but I think people

00:15:55.040 --> 00:16:00.399
don't realize how um how powerful that

00:15:58.240 --> 00:16:01.839
multimodality capability is yet.

00:16:00.399 --> 00:16:04.000
>> All right, you've given us the perfect

00:16:01.839 --> 00:16:06.720
bridge in transition to zooming in. Uh

00:16:04.000 --> 00:16:09.600
congratulations on Gemini 3 last month.

00:16:06.720 --> 00:16:12.959
Uh your gamechanging uh model, you say

00:16:09.600 --> 00:16:14.880
it reasons with unprecedented depth and

00:16:12.959 --> 00:16:17.600
nuance. Tell us what's unique about the

00:16:14.880 --> 00:16:21.120
nuance part of Gemini 3. Yeah, I think

00:16:17.600 --> 00:16:23.040
it's just um uh uh we're really pleased

00:16:21.120 --> 00:16:24.880
with the the the the almost the

00:16:23.040 --> 00:16:27.360
personality of it, the style of it as

00:16:24.880 --> 00:16:29.920
well as its capability. I I I I like the

00:16:27.360 --> 00:16:31.519
way um that it answers succinctly. It

00:16:29.920 --> 00:16:33.279
pushes back a little bit if you're

00:16:31.519 --> 00:16:35.440
doesn't just agree with whatever you're

00:16:33.279 --> 00:16:36.880
saying. It pushes back gently on some

00:16:35.440 --> 00:16:38.800
ideas that if they're not if they don't

00:16:36.880 --> 00:16:41.199
make sense. And I think people are

00:16:38.800 --> 00:16:43.279
appreciating uh it seems you know sort

00:16:41.199 --> 00:16:45.279
of I feel like it's a you can feel it's

00:16:43.279 --> 00:16:47.600
a bit of a step change in its kind of

00:16:45.279 --> 00:16:49.519
intelligence and therefore usefulness.

00:16:47.600 --> 00:16:52.000
>> And what's something that Gemini has

00:16:49.519 --> 00:16:54.240
answered or produced where you said I

00:16:52.000 --> 00:16:56.480
didn't know it could do that or I didn't

00:16:54.240 --> 00:16:58.320
know it would do that.

00:16:56.480 --> 00:17:00.240
>> Well actually this is the the amazing

00:16:58.320 --> 00:17:02.880
thing of when you why we love what what

00:17:00.240 --> 00:17:04.880
we're doing so much is that the this era

00:17:02.880 --> 00:17:06.880
we're now in with research connected to

00:17:04.880 --> 00:17:10.000
product. The great thing about that is

00:17:06.880 --> 00:17:12.079
that you get millions and potentially at

00:17:10.000 --> 00:17:14.319
Google billions of users immediately

00:17:12.079 --> 00:17:16.640
take advantage of the new technology you

00:17:14.319 --> 00:17:19.120
put out there. And uh we're continually

00:17:16.640 --> 00:17:21.600
surprised by the cool things that people

00:17:19.120 --> 00:17:24.000
figure out very quickly um to use these

00:17:21.600 --> 00:17:25.679
models for. Um and a lot of those things

00:17:24.000 --> 00:17:27.760
sort of, you know, tend to go viral. But

00:17:25.679 --> 00:17:30.960
the thing I I most enjoyed with Gemini 3

00:17:27.760 --> 00:17:32.480
was oneshotting uh games. So back to my

00:17:30.960 --> 00:17:34.160
very first career of making AI for

00:17:32.480 --> 00:17:35.520
games, I think we're very close now with

00:17:34.160 --> 00:17:37.200
these models. maybe the next version

00:17:35.520 --> 00:17:40.320
models where you could start really

00:17:37.200 --> 00:17:43.039
creating perhaps commercial grade games

00:17:40.320 --> 00:17:45.280
uh you know vibe coding them uh with you

00:17:43.039 --> 00:17:45.840
know in a few hours which used to take

00:17:45.280 --> 00:17:48.080
years

00:17:45.840 --> 00:17:50.480
>> and that shows nuance. What does that

00:17:48.080 --> 00:17:53.760
show about the model? Well, I think it's

00:17:50.480 --> 00:17:57.200
just incredible uh uh depth and and and

00:17:53.760 --> 00:17:59.919
capability of these models to understand

00:17:57.200 --> 00:18:02.320
very high level instructions and and

00:17:59.919 --> 00:18:05.120
produce you know very detailed outputs

00:18:02.320 --> 00:18:06.799
and the other things that uh uh Gemini 3

00:18:05.120 --> 00:18:08.799
particularly is good at is front-end

00:18:06.799 --> 00:18:11.200
work and developing you know websites

00:18:08.799 --> 00:18:13.760
and it's it's pretty good aesthetically

00:18:11.200 --> 00:18:15.520
and creatively as well as um

00:18:13.760 --> 00:18:18.160
technically. Something we've written a

00:18:15.520 --> 00:18:20.480
fair amount about at Axios is that even

00:18:18.160 --> 00:18:22.640
the authors, creators of these models

00:18:20.480 --> 00:18:24.400
don't totally understand them. What's

00:18:22.640 --> 00:18:26.320
something about Gemini 3? Yeah.

00:18:24.400 --> 00:18:26.880
>> That you feel like you don't totally

00:18:26.320 --> 00:18:28.799
get?

00:18:26.880 --> 00:18:31.919
>> Well, actually I feel like with all

00:18:28.799 --> 00:18:33.360
these models um and and maybe all of the

00:18:31.919 --> 00:18:36.160
the audience are feeling this too is

00:18:33.360 --> 00:18:39.360
that it there's such a fast pace of of

00:18:36.160 --> 00:18:41.520
of innovation and improvement. Um we're

00:18:39.360 --> 00:18:43.600
spending almost all of our time building

00:18:41.520 --> 00:18:45.039
these things. We have we don't even have

00:18:43.600 --> 00:18:46.880
I I have to have this feeling every time

00:18:45.039 --> 00:18:49.200
we release a new version that I haven't

00:18:46.880 --> 00:18:50.720
even explored a tenth had time to even

00:18:49.200 --> 00:18:52.559
explore a tenth of probably what the

00:18:50.720 --> 00:18:54.559
existing systems can do because of

00:18:52.559 --> 00:18:56.080
course we're on to immediately you know

00:18:54.559 --> 00:18:58.080
we're referencing back to the ferocious

00:18:56.080 --> 00:18:59.600
race and competition we're in we're

00:18:58.080 --> 00:19:01.760
immediately focusing on the next

00:18:59.600 --> 00:19:03.760
innovation uh and obviously making sure

00:19:01.760 --> 00:19:07.600
it's safe and reliable and all those

00:19:03.760 --> 00:19:10.320
things. So again, our users end up uh uh

00:19:07.600 --> 00:19:12.240
taking them much further than often uh

00:19:10.320 --> 00:19:15.200
we we've tried internally.

00:19:12.240 --> 00:19:18.960
>> And one more question on Gemini 3, a

00:19:15.200 --> 00:19:22.240
little back story and you had a number

00:19:18.960 --> 00:19:26.080
of irons in the fire, but LLM's the

00:19:22.240 --> 00:19:28.799
textbased uh large language models. uh

00:19:26.080 --> 00:19:30.640
you didn't necessarily go all in on that

00:19:28.799 --> 00:19:33.039
as the holy grail. Something that Walter

00:19:30.640 --> 00:19:35.360
Isacson, the great author and thinker

00:19:33.039 --> 00:19:38.640
and your friend said to me is that when

00:19:35.360 --> 00:19:42.000
you saw the power of the LLM, you did a

00:19:38.640 --> 00:19:44.799
pivot, a pureette, as Walter said it,

00:19:42.000 --> 00:19:47.440
and were able to leapfrog to great

00:19:44.799 --> 00:19:49.440
success. And Walter's point was that

00:19:47.440 --> 00:19:51.360
most business people would have been

00:19:49.440 --> 00:19:53.840
stubborn, might have doubled, triple

00:19:51.360 --> 00:19:56.559
down on their other bets. How did you

00:19:53.840 --> 00:19:57.200
make this decision to go allin on your

00:19:56.559 --> 00:19:59.120
LLM?

00:19:57.200 --> 00:20:00.480
>> Well, I think this is again the the the

00:19:59.120 --> 00:20:01.919
beauty of and the strength of the

00:20:00.480 --> 00:20:04.640
scientific method. If you're a true

00:20:01.919 --> 00:20:07.280
scientist, you can't get too dogmatic

00:20:04.640 --> 00:20:09.039
about some idea you have. You you need

00:20:07.280 --> 00:20:11.360
to go with where the empirical evidence

00:20:09.039 --> 00:20:12.880
is taking you. So, first of all, this is

00:20:11.360 --> 00:20:16.320
this is Walter is probably referring

00:20:12.880 --> 00:20:18.160
back to the 2017 2018 era. So, there we

00:20:16.320 --> 00:20:19.919
had a lot of irons in the fire. As we

00:20:18.160 --> 00:20:21.600
said, we had our own very capable

00:20:19.919 --> 00:20:23.360
language models. They were called

00:20:21.600 --> 00:20:24.720
Chinchilla and then Sparrow and we had

00:20:23.360 --> 00:20:26.480
these various different code names for

00:20:24.720 --> 00:20:28.159
them. Um they weren't publicly released

00:20:26.480 --> 00:20:29.600
but they were internal. In fact, some of

00:20:28.159 --> 00:20:30.880
the scaling laws were originally figured

00:20:29.600 --> 00:20:33.679
out by our team. They're called the

00:20:30.880 --> 00:20:35.760
Chinchilla scaling laws. Um but we also

00:20:33.679 --> 00:20:37.120
had other types of programs alpha zero

00:20:35.760 --> 00:20:39.200
things that were building on Alpha Go

00:20:37.120 --> 00:20:40.799
pure RL systems and we also had some

00:20:39.200 --> 00:20:42.720
cognitive science more neuroscience

00:20:40.799 --> 00:20:46.080
inspired architectures as well. And at

00:20:42.720 --> 00:20:49.280
the time all we weren't sure my job is

00:20:46.080 --> 00:20:50.960
to make sure we build AGI uh first fast

00:20:49.280 --> 00:20:53.200
and safely, right? That's always been

00:20:50.960 --> 00:20:55.919
our our solve intelligence, our mission

00:20:53.200 --> 00:20:58.320
at DeepMind. And and so I'm kind of

00:20:55.919 --> 00:21:00.159
agnostic actually to the to the approach

00:20:58.320 --> 00:21:01.679
that's taken. I'm pretty pragmatic on

00:21:00.159 --> 00:21:04.080
that. That's maybe my engineering side

00:21:01.679 --> 00:21:06.080
of me is I have some theories as as a

00:21:04.080 --> 00:21:06.960
good scientist would, but I'm I'm I'm at

00:21:06.080 --> 00:21:09.360
the end of the day, it's got to

00:21:06.960 --> 00:21:11.360
pragmatically work. And so when we

00:21:09.360 --> 00:21:13.760
started seeing the beginnings of scaling

00:21:11.360 --> 00:21:16.000
working, then we increasingly put more

00:21:13.760 --> 00:21:17.840
and more resources onto that branch of

00:21:16.000 --> 00:21:19.600
the of the of the research tree.

00:21:17.840 --> 00:21:21.280
>> Something that's refreshing about your

00:21:19.600 --> 00:21:24.559
approach is with artificial general

00:21:21.280 --> 00:21:26.480
intelligence, human capable uh AI. You

00:21:24.559 --> 00:21:28.320
don't shy away from it. Some other

00:21:26.480 --> 00:21:30.000
people say, "Well, we won't know or

00:21:28.320 --> 00:21:32.400
we're already there or it doesn't

00:21:30.000 --> 00:21:34.960
matter." You say that it does matter and

00:21:32.400 --> 00:21:35.360
we will know. And you say it's not far

00:21:34.960 --> 00:21:37.679
off.

00:21:35.360 --> 00:21:39.280
>> Yeah, we're definitely not there now.

00:21:37.679 --> 00:21:40.559
So, and and I and

00:21:39.280 --> 00:21:41.840
>> actually quite close is how you say.

00:21:40.559 --> 00:21:43.600
>> Yes, quite close. I think we're like

00:21:41.840 --> 00:21:45.440
five to 10 years away if you were to ask

00:21:43.600 --> 00:21:48.400
me. I'm sorry. I think Say that again.

00:21:45.440 --> 00:21:49.919
>> Five to 10 years away. I think my bar

00:21:48.400 --> 00:21:52.240
though is quite high. So, this is the

00:21:49.919 --> 00:21:54.000
the we define AGI as you know the a

00:21:52.240 --> 00:21:56.080
system that that exhibits all the

00:21:54.000 --> 00:21:58.480
cognitive capabilities we have and that

00:21:56.080 --> 00:22:00.640
includes uh inventive and creative

00:21:58.480 --> 00:22:02.159
capabilities. I think there are missing

00:22:00.640 --> 00:22:04.240
there's as all of you have used the

00:22:02.159 --> 00:22:05.520
current LLMs there are they're they're

00:22:04.240 --> 00:22:06.880
amazing in some ways. They're really

00:22:05.520 --> 00:22:09.200
impressive in some senses in some

00:22:06.880 --> 00:22:12.159
they've got incredible almost PhD levels

00:22:09.200 --> 00:22:13.919
uh key skills in some areas IMO gold

00:22:12.159 --> 00:22:15.679
medals and so on but in other areas

00:22:13.919 --> 00:22:17.919
they're very flawed still and so they're

00:22:15.679 --> 00:22:20.000
these sort of jagged intelligences so

00:22:17.919 --> 00:22:22.159
the you would expect across the board

00:22:20.000 --> 00:22:23.760
consistency from a true AGI and they're

00:22:22.159 --> 00:22:25.600
missing other capabilities like

00:22:23.760 --> 00:22:27.840
continual learning online learning

00:22:25.600 --> 00:22:29.840
long-term planning and reasoning they

00:22:27.840 --> 00:22:31.760
can't do any of these things currently I

00:22:29.840 --> 00:22:32.880
think they will be able to but maybe one

00:22:31.760 --> 00:22:33.600
or two more breakthroughs are going to

00:22:32.880 --> 00:22:35.280
be required

00:22:33.600 --> 00:22:37.440
>> and a question from the great Ena

00:22:35.280 --> 00:22:40.240
Frereded who we've uh seen today and

00:22:37.440 --> 00:22:43.919
whose uh coverage from day zero of Axios

00:22:40.240 --> 00:22:46.080
has helped make Axios what it is. Uh she

00:22:43.919 --> 00:22:50.880
says you're obviously

00:22:46.080 --> 00:22:53.600
um uh you've said that AI might be one

00:22:50.880 --> 00:22:54.159
advance two advances away from AGI.

00:22:53.600 --> 00:22:57.679
>> Yes.

00:22:54.159 --> 00:22:59.679
>> Will we get there just by improving LLM

00:22:57.679 --> 00:23:01.360
and generative AI or do you think that

00:22:59.679 --> 00:23:03.760
there might be a different approach

00:23:01.360 --> 00:23:05.440
that's needed to hit a GI in your 5 to

00:23:03.760 --> 00:23:07.919
10 years? I think I think again this is

00:23:05.440 --> 00:23:10.159
an empirical question but what I do know

00:23:07.919 --> 00:23:13.039
this is this would be my best guess is

00:23:10.159 --> 00:23:15.520
um the scaling of the current systems

00:23:13.039 --> 00:23:17.760
you we must push that to the maximum

00:23:15.520 --> 00:23:20.400
because at the minimum it will be a key

00:23:17.760 --> 00:23:22.960
component of the final AGI system it

00:23:20.400 --> 00:23:24.640
could be the entirety of the AGI system

00:23:22.960 --> 00:23:26.720
there's a chance that just scaling will

00:23:24.640 --> 00:23:28.080
get you there but my guess is if I was

00:23:26.720 --> 00:23:30.080
to guess from where I my vantage point

00:23:28.080 --> 00:23:32.000
now is that one or two more big

00:23:30.080 --> 00:23:33.440
breakthroughs when I mean there's

00:23:32.000 --> 00:23:35.760
innovation going on all the time by the

00:23:33.440 --> 00:23:37.520
way even including in scaling um

00:23:35.760 --> 00:23:40.159
existing techniques but I'm talking like

00:23:37.520 --> 00:23:42.080
a transformer level or alpho level type

00:23:40.159 --> 00:23:45.039
of breakthrough. I think we might I

00:23:42.080 --> 00:23:46.720
suspect when we look back in once AGI is

00:23:45.039 --> 00:23:48.720
done that one or two of those things

00:23:46.720 --> 00:23:49.200
were still required in addition to

00:23:48.720 --> 00:23:50.799
scaling.

00:23:49.200 --> 00:23:53.200
>> We're about to get the hook. So a super

00:23:50.799 --> 00:23:56.400
rapid round. Another question from uh

00:23:53.200 --> 00:23:58.480
Ena. you obviously are a big believer in

00:23:56.400 --> 00:24:00.559
AI, but if you look at what's being

00:23:58.480 --> 00:24:02.159
spent, that doesn't mean that there

00:24:00.559 --> 00:24:04.320
might not be a big enough bubble to

00:24:02.159 --> 00:24:05.120
rattle the economy. How worried are you

00:24:04.320 --> 00:24:08.000
about that?

00:24:05.120 --> 00:24:09.919
>> Um, I think we there I think it's not a

00:24:08.000 --> 00:24:12.080
binary. I think some parts of the AI

00:24:09.919 --> 00:24:13.360
industry are probably in a bubble like,

00:24:12.080 --> 00:24:15.919
you know, I don't know, like the seed

00:24:13.360 --> 00:24:17.679
rounds of, you know, you know, $50

00:24:15.919 --> 00:24:20.080
billion seed rounds and things like that

00:24:17.679 --> 00:24:22.080
seems a little bit unsustainable. But um

00:24:20.080 --> 00:24:24.000
on the other hand, of course, I more

00:24:22.080 --> 00:24:26.799
than anyone believes that AI is the most

00:24:24.000 --> 00:24:28.960
transformative uh technology ever. So I

00:24:26.799 --> 00:24:31.360
think in the fullness of time, this is

00:24:28.960 --> 00:24:34.000
all going to be uh more than justified.

00:24:31.360 --> 00:24:36.159
And my job as head of Google Deep Mine

00:24:34.000 --> 00:24:39.279
and and the engine room of Google is to

00:24:36.159 --> 00:24:41.600
make sure we win either way. If if the

00:24:39.279 --> 00:24:43.120
bubble the so-called bubble bursts or if

00:24:41.600 --> 00:24:44.480
things continue to be good like they are

00:24:43.120 --> 00:24:46.640
now, we're in a strong position.

00:24:44.480 --> 00:24:49.039
>> The AI recruiting wars, what's the end

00:24:46.640 --> 00:24:50.480
state of this competition for talent?

00:24:49.039 --> 00:24:52.000
Well, look, it's gone pretty crazy

00:24:50.480 --> 00:24:53.520
recently. Things like what Meta have

00:24:52.000 --> 00:24:55.600
been doing and, you know, everyone's got

00:24:53.520 --> 00:24:58.559
to do what what makes sense for them.

00:24:55.600 --> 00:25:00.799
Uh, what we found for us is that we want

00:24:58.559 --> 00:25:02.640
people who are missiondriven. We have, I

00:25:00.799 --> 00:25:04.720
think, the best mission. We have the

00:25:02.640 --> 00:25:06.480
full stack. So, I think if you want to

00:25:04.720 --> 00:25:08.400
do the most impactful work and have the

00:25:06.480 --> 00:25:10.320
most positive impact on the world, then

00:25:08.400 --> 00:25:13.120
I think there's nowhere better uh than

00:25:10.320 --> 00:25:14.720
than at Google DeepMind. And in the end,

00:25:13.120 --> 00:25:16.320
I think the best scientists, the best

00:25:14.720 --> 00:25:17.919
researchers, the best engineers, they

00:25:16.320 --> 00:25:19.840
want to work on the most cutting edge

00:25:17.919 --> 00:25:21.840
stuff. So if you're the sort of top of

00:25:19.840 --> 00:25:25.120
the leaderboards with the best systems,

00:25:21.840 --> 00:25:26.400
uh that's that's sort of a self fueling.

00:25:25.120 --> 00:25:28.320
This is a question from James

00:25:26.400 --> 00:25:30.320
Vanderhigh, an entrepreneurial young

00:25:28.320 --> 00:25:32.000
mind at High Point University in North

00:25:30.320 --> 00:25:35.120
Carolina. He says, "There's a lot of

00:25:32.000 --> 00:25:37.919
conversation about AI gaining a mind of

00:25:35.120 --> 00:25:40.960
its own. Is there a scenario where AI

00:25:37.919 --> 00:25:42.320
could act in its selfinterest?"

00:25:40.960 --> 00:25:44.480
Well, that's a great question and and

00:25:42.320 --> 00:25:47.200
it's related to the some of the the the

00:25:44.480 --> 00:25:49.120
the more sort of catastrophic outcomes

00:25:47.200 --> 00:25:50.880
is if that went wrong, that would be one

00:25:49.120 --> 00:25:52.880
of the issues that with agentbased

00:25:50.880 --> 00:25:55.039
systems or very autonomous systems if

00:25:52.880 --> 00:25:57.600
somehow they developed a self-interest

00:25:55.039 --> 00:26:00.320
that was some in some sense sense

00:25:57.600 --> 00:26:03.200
conflicting with what the designers or

00:26:00.320 --> 00:26:05.919
even perhaps humanity wanted it to do.

00:26:03.200 --> 00:26:08.880
>> And finishing with a fun thing, you're

00:26:05.919 --> 00:26:11.279
still a gamer. What does gaming teach us

00:26:08.880 --> 00:26:13.120
about the world and what does gaming

00:26:11.279 --> 00:26:13.760
teach us about where these machines are

00:26:13.120 --> 00:26:16.159
headed?

00:26:13.760 --> 00:26:18.159
>> Well, look, I think uh my chess

00:26:16.159 --> 00:26:20.080
background and and my training in that

00:26:18.159 --> 00:26:22.400
and then other games subsequently has

00:26:20.080 --> 00:26:24.799
been critical to how I do my work and

00:26:22.400 --> 00:26:26.400
both in business and in science. Uh I

00:26:24.799 --> 00:26:27.600
think the thing I love about games and

00:26:26.400 --> 00:26:29.760
there's many things I've loved about

00:26:27.600 --> 00:26:31.520
them, but I love the creativity of

00:26:29.760 --> 00:26:33.039
making them. But I also just playing

00:26:31.520 --> 00:26:35.440
them I think is the best way to train

00:26:33.039 --> 00:26:38.080
your mind because the best games whether

00:26:35.440 --> 00:26:39.919
that's chess or go or whatever or poker

00:26:38.080 --> 00:26:42.320
they're microcosms of something in the

00:26:39.919 --> 00:26:44.400
real world right but in general you

00:26:42.320 --> 00:26:46.640
don't get in the real world to have

00:26:44.400 --> 00:26:48.720
several practice goes at making the

00:26:46.640 --> 00:26:50.640
decision correctly in that moment. Maybe

00:26:48.720 --> 00:26:52.320
in the real life you only get a dozen of

00:26:50.640 --> 00:26:55.039
those critical moments, but you can

00:26:52.320 --> 00:26:58.000
practice your decision-m capabilities as

00:26:55.039 --> 00:27:00.000
much as you want uh w within the the the

00:26:58.000 --> 00:27:02.159
the almost the simulation really of the

00:27:00.000 --> 00:27:04.000
world with games. Um and as long as you

00:27:02.159 --> 00:27:05.440
take the games very seriously, so you

00:27:04.000 --> 00:27:07.679
put you put a lot of thought into your

00:27:05.440 --> 00:27:09.039
decision-m, it really does train your

00:27:07.679 --> 00:27:10.880
your decision-m and planning

00:27:09.039 --> 00:27:13.279
capabilities in my opinion. Now, you've

00:27:10.880 --> 00:27:16.640
pointed out that our squishy brains uh

00:27:13.279 --> 00:27:18.400
evolved uh to be hunter gatherers and

00:27:16.640 --> 00:27:20.159
yet we're facing a disruption that as

00:27:18.400 --> 00:27:22.240
you put it to the Guardian will be 10

00:27:20.159 --> 00:27:25.039
times bigger than the industrial

00:27:22.240 --> 00:27:27.360
revolution and maybe 10 times faster.

00:27:25.039 --> 00:27:30.720
Are we facing a situation where most

00:27:27.360 --> 00:27:32.720
humans can't keep up and maybe no human

00:27:30.720 --> 00:27:34.880
including you can keep up?

00:27:32.720 --> 00:27:37.360
>> Well, the good news is and I think my

00:27:34.880 --> 00:27:39.919
point on the hunt gather was look how

00:27:37.360 --> 00:27:42.320
adaptive our brains have been. We we

00:27:39.919 --> 00:27:44.480
evolved to be hunter gatherers and yet

00:27:42.320 --> 00:27:46.159
here we are sitting in our modern

00:27:44.480 --> 00:27:50.400
cities, modern civilization with all the

00:27:46.159 --> 00:27:52.320
technology around us and um you know the

00:27:50.400 --> 00:27:54.960
same human brain pretty much has been

00:27:52.320 --> 00:27:58.399
able to adapt to that. So I'm a really

00:27:54.960 --> 00:28:00.320
uh big believer in uh human ingenuity

00:27:58.399 --> 00:28:02.320
and um and I think we're infinitely

00:28:00.320 --> 00:28:03.760
adaptable. We are the only existence

00:28:02.320 --> 00:28:05.919
proof our brains are the only existence

00:28:03.760 --> 00:28:07.760
proof of general intelligence perhaps in

00:28:05.919 --> 00:28:09.679
the known you know universe that we know

00:28:07.760 --> 00:28:12.080
of so far. So we are general

00:28:09.679 --> 00:28:14.000
intelligences ourselves and so we should

00:28:12.080 --> 00:28:17.039
be able to infinitely adapt. There is a

00:28:14.000 --> 00:28:18.640
question about when AGI post AGI what

00:28:17.039 --> 00:28:20.720
kinds of technologies can we create

00:28:18.640 --> 00:28:24.080
brain computer interfaces other things

00:28:20.720 --> 00:28:25.919
that some of us may choose to to use in

00:28:24.080 --> 00:28:27.760
addition to our existing technologies

00:28:25.919 --> 00:28:28.159
and that could be one way for us to keep

00:28:27.760 --> 00:28:30.080
up.

00:28:28.159 --> 00:28:32.159
>> And as we say goodbye you're a lifelong

00:28:30.080 --> 00:28:36.960
Liverpool fan. You've helped them with

00:28:32.159 --> 00:28:39.279
their analytics. How will AI affect and

00:28:36.960 --> 00:28:40.320
inform the World Cup here in North

00:28:39.279 --> 00:28:41.679
America?

00:28:40.320 --> 00:28:43.600
>> Well, we've had a lot of we've had a lot

00:28:41.679 --> 00:28:46.000
of teams approach us for for help, too.

00:28:43.600 --> 00:28:47.679
And um and I have to be try and be equal

00:28:46.000 --> 00:28:49.360
with that, but it's hard having a

00:28:47.679 --> 00:28:50.720
lifelong spot of Liverpool. But I'm

00:28:49.360 --> 00:28:51.919
looking forward to trying to make it out

00:28:50.720 --> 00:28:52.480
here maybe at least for the World Cup

00:28:51.919 --> 00:28:54.799
final.

00:28:52.480 --> 00:28:57.039
>> But but let's be serious. What what what

00:28:54.799 --> 00:28:59.760
will it change between now and then?

00:28:57.039 --> 00:29:00.559
It's a it's a lifetime in AI between now

00:28:59.760 --> 00:29:02.799
and then, right?

00:29:00.559 --> 00:29:03.520
>> Yeah. Well, what in AI or AI for sport

00:29:02.799 --> 00:29:05.440
or just in

00:29:03.520 --> 00:29:07.279
>> Yes. Yeah. Well, I mean, look, sport has

00:29:05.440 --> 00:29:09.600
immense amount of data and it's all

00:29:07.279 --> 00:29:11.679
about uh extreme elite performance. So,

00:29:09.600 --> 00:29:14.159
it's actually a natural bed fellow for

00:29:11.679 --> 00:29:16.240
for AI to to come in and and help

00:29:14.159 --> 00:29:18.000
optimize that process even further.

00:29:16.240 --> 00:29:19.600
>> And without giving away a trade secret,

00:29:18.000 --> 00:29:21.039
what will it be able to do for a World

00:29:19.600 --> 00:29:23.120
Cup team?

00:29:21.039 --> 00:29:24.799
>> Uh maybe score more headers from from

00:29:23.120 --> 00:29:26.080
corners, you know, if you place the

00:29:24.799 --> 00:29:27.840
that's one of the things I think our

00:29:26.080 --> 00:29:30.640
system found out like precise

00:29:27.840 --> 00:29:33.840
positioning of the players. Deus, thanks

00:29:30.640 --> 00:29:33.840
for making a

