WEBVTT

00:00:00.000 --> 00:00:03.919
I would love if you could compare the

00:00:01.760 --> 00:00:05.759
moment that we are in right now in 2025

00:00:03.919 --> 00:00:07.600
to the early days of the internet and

00:00:05.759 --> 00:00:10.960
I'll ask it as a as a sort of question

00:00:07.600 --> 00:00:13.519
where it's like AI is to 2025 as the

00:00:10.960 --> 00:00:17.119
internet is to what year?

00:00:13.519 --> 00:00:20.000
>> Um okay internet history I guess it was

00:00:17.119 --> 00:00:22.960
arponet and whatnot in the 70s. Uh but

00:00:20.000 --> 00:00:24.560
the web kind of I think of it as born in

00:00:22.960 --> 00:00:27.039
uh

00:00:24.560 --> 00:00:29.199
Mosaic was launched in 93 or so if I

00:00:27.039 --> 00:00:33.840
remember correctly and then Netscape

00:00:29.199 --> 00:00:36.880
however many years after that. Um

00:00:33.840 --> 00:00:38.559
I think um in some ways you can draw

00:00:36.880 --> 00:00:41.600
parallels.

00:00:38.559 --> 00:00:44.719
Um I don't know you could say with sort

00:00:41.600 --> 00:00:48.320
of I don't know transformers in 2017 the

00:00:44.719 --> 00:00:50.480
first inklings of our uh new kind of

00:00:48.320 --> 00:00:53.760
language models

00:00:50.480 --> 00:01:00.239
but I think it's really different

00:00:53.760 --> 00:01:02.320
>> in many ways. I mean for one thing um

00:01:00.239 --> 00:01:05.680
the internet

00:01:02.320 --> 00:01:09.520
was brilliant and enabled many things

00:01:05.680 --> 00:01:11.280
uh but it wasn't like technically

00:01:09.520 --> 00:01:13.920
revolutionary

00:01:11.280 --> 00:01:16.640
um in the sense um you know like uh with

00:01:13.920 --> 00:01:18.960
the web Tim Berners Lee at CERN they

00:01:16.640 --> 00:01:23.439
were just organizing the scientists data

00:01:18.960 --> 00:01:25.840
and stuff and sharing it and um they did

00:01:23.439 --> 00:01:27.280
a great job and it took off as a viral

00:01:25.840 --> 00:01:29.200
organizational thing and it's been

00:01:27.280 --> 00:01:31.920
phenomenal, don't get me wrong. Uh but

00:01:29.200 --> 00:01:33.520
it wasn't like

00:01:31.920 --> 00:01:35.040
um

00:01:33.520 --> 00:01:37.439
nobody would have like questioned

00:01:35.040 --> 00:01:40.400
whether that was physically possible

00:01:37.439 --> 00:01:43.119
>> 5 years before. There was no real

00:01:40.400 --> 00:01:46.320
limitations. Um in this case, we don't

00:01:43.119 --> 00:01:48.479
really know what intelligence is.

00:01:46.320 --> 00:01:51.680
>> We don't know how far we can take it.

00:01:48.479 --> 00:01:54.000
Um, I think a lot of people, myself

00:01:51.680 --> 00:01:56.000
included, are just surprised how quickly

00:01:54.000 --> 00:01:59.200
and how far it is ramped.

00:01:56.000 --> 00:02:00.479
>> Um, and

00:01:59.200 --> 00:02:03.360
uh, so that's just an important

00:02:00.479 --> 00:02:06.640
distinction. Like we don't even know

00:02:03.360 --> 00:02:07.759
what the possible peak is.

00:02:06.640 --> 00:02:09.920
>> Like with the internet, you could

00:02:07.759 --> 00:02:11.680
imagine everybody could communicate at

00:02:09.920 --> 00:02:14.160
high speed with everybody else. There'd

00:02:11.680 --> 00:02:16.480
be, you know, every company would have a

00:02:14.160 --> 00:02:18.640
website, you know, which they do now. Um

00:02:16.480 --> 00:02:22.080
but you could you could have

00:02:18.640 --> 00:02:25.440
realistically imagined that in 1990

00:02:22.080 --> 00:02:28.000
let's say or um

00:02:25.440 --> 00:02:30.640
and even uh there were things before

00:02:28.000 --> 00:02:32.959
that like gopher I don't probably dating

00:02:30.640 --> 00:02:33.920
myself but um there were things before

00:02:32.959 --> 00:02:36.959
the web

00:02:33.920 --> 00:02:40.000
>> uh that were kind of like that um but

00:02:36.959 --> 00:02:43.040
yeah with with AI you don't know where

00:02:40.000 --> 00:02:45.920
the peak is or if there's a peak at all

00:02:43.040 --> 00:02:47.920
>> um so that's one important difference Um

00:02:45.920 --> 00:02:53.599
the other important difference and for

00:02:47.920 --> 00:02:56.879
better or for worse um this has now

00:02:53.599 --> 00:02:58.400
gained you know profound international

00:02:56.879 --> 00:03:01.040
attention. I mean the amount of

00:02:58.400 --> 00:03:03.200
resources and you know money and compute

00:03:01.040 --> 00:03:05.360
and energy that are flowing towards AI

00:03:03.200 --> 00:03:06.959
is extraordinary. you know, the early

00:03:05.360 --> 00:03:11.120
days of web, we were a startup, you

00:03:06.959 --> 00:03:13.840
know, we whatever we we we got uh a

00:03:11.120 --> 00:03:15.840
little uh seed loan, less than a million

00:03:13.840 --> 00:03:18.000
dollars, we were off to the races, you

00:03:15.840 --> 00:03:19.920
know, I think we got like $10 million in

00:03:18.000 --> 00:03:22.640
our venture round and that was that.

00:03:19.920 --> 00:03:24.080
>> Um, you know, these days companies are

00:03:22.640 --> 00:03:25.920
spending billions and billions of

00:03:24.080 --> 00:03:30.319
dollars building, you know, the best AI

00:03:25.920 --> 00:03:32.720
models in the world. Um and um

00:03:30.319 --> 00:03:34.400
that's a good and bad thing, but uh but

00:03:32.720 --> 00:03:34.720
it's definitely different.

00:03:34.400 --> 00:03:39.400
>> Yeah.

00:03:34.720 --> 00:03:39.400
>> Uh so I just think it's um

00:03:39.440 --> 00:03:42.720
>> in as much as there are parallels, I

00:03:41.280 --> 00:03:43.599
just think we have no idea where this is

00:03:42.720 --> 00:03:48.000
going.

00:03:43.599 --> 00:03:52.159
>> So to that end then, do you believe that

00:03:48.000 --> 00:03:54.080
AI is fundamentally more of a discovery

00:03:52.159 --> 00:03:55.920
than an invention?

00:03:54.080 --> 00:03:57.519
Do you feel like this is some emergent

00:03:55.920 --> 00:03:59.360
property of the universe and we're just

00:03:57.519 --> 00:04:02.560
stumbling upon it or is this like the

00:03:59.360 --> 00:04:04.159
ultimate test of human creation?

00:04:02.560 --> 00:04:05.280
>> Well, I I mean I guess both of those

00:04:04.159 --> 00:04:05.760
things that you said.

00:04:05.280 --> 00:04:07.599
>> Yeah.

00:04:05.760 --> 00:04:09.840
>> Um

00:04:07.599 --> 00:04:11.920
are distinct

00:04:09.840 --> 00:04:13.439
um from from what the web might have

00:04:11.920 --> 00:04:15.599
been. I mean, yeah, I think it's a

00:04:13.439 --> 00:04:17.919
discovery in the sense like we simply do

00:04:15.599 --> 00:04:20.720
not know what is the limit to

00:04:17.919 --> 00:04:24.240
intelligence. There's no

00:04:20.720 --> 00:04:26.240
law that says, you know, can you be 100

00:04:24.240 --> 00:04:29.040
times smarter than Einstein? Can you be

00:04:26.240 --> 00:04:30.639
a billion times smarter? Can you

00:04:29.040 --> 00:04:33.360
>> be a Google time smarter? You know,

00:04:30.639 --> 00:04:36.000
there's no um I don't I I think we have

00:04:33.360 --> 00:04:37.199
just no idea what the laws governing

00:04:36.000 --> 00:04:40.880
that are.

00:04:37.199 --> 00:04:43.680
>> And um so yeah, I guess I guess you

00:04:40.880 --> 00:04:48.000
could call it a discovery of sorts. Uh

00:04:43.680 --> 00:04:51.840
maybe an analogy is sort of um quantum

00:04:48.000 --> 00:04:53.600
computing where you don't really know

00:04:51.840 --> 00:04:54.960
how much computation are you really

00:04:53.600 --> 00:04:56.000
going to be able to get out of the

00:04:54.960 --> 00:04:57.680
universe.

00:04:56.000 --> 00:05:00.160
>> Um kind of the basic laws of quantum

00:04:57.680 --> 00:05:02.160
mechanics suggests it's extremely high.

00:05:00.160 --> 00:05:03.600
Um but

00:05:02.160 --> 00:05:05.680
you don't know in practice if there are

00:05:03.600 --> 00:05:06.639
just other limitations you don't know

00:05:05.680 --> 00:05:08.800
about right now.

00:05:06.639 --> 00:05:10.560
>> Yeah. Yeah. Yeah. No, that's

00:05:08.800 --> 00:05:14.880
fascinating. But you ultimately think

00:05:10.560 --> 00:05:16.479
that AI is a more momentous

00:05:14.880 --> 00:05:17.759
discovery or invention than the

00:05:16.479 --> 00:05:23.479
internet.

00:05:17.759 --> 00:05:23.479
Yeah, I I think the internet was um

00:05:24.160 --> 00:05:30.400
it was definitely very important.

00:05:26.720 --> 00:05:32.240
Uh but it was sort of as much a uh kind

00:05:30.400 --> 00:05:34.080
of social

00:05:32.240 --> 00:05:36.720
development like everybody agreeing to

00:05:34.080 --> 00:05:38.800
use these protocols and then kind of

00:05:36.720 --> 00:05:41.280
making their data and systems available

00:05:38.800 --> 00:05:44.320
for everybody else with you know

00:05:41.280 --> 00:05:46.400
>> TCP and IP and then um

00:05:44.320 --> 00:05:48.720
>> you know HTML, HP just agreeing on

00:05:46.400 --> 00:05:52.800
protocols and allowing it to grow and

00:05:48.720 --> 00:05:55.520
flourish. um you know, maybe akin to how

00:05:52.800 --> 00:05:57.199
money was an invention uh thousands of

00:05:55.520 --> 00:05:58.639
years ago that let people, you know,

00:05:57.199 --> 00:06:01.120
really trade and stuff,

00:05:58.639 --> 00:06:05.440
>> but it's not like

00:06:01.120 --> 00:06:08.720
um it's not neither money nor the

00:06:05.440 --> 00:06:10.479
internet are testing the limits of the

00:06:08.720 --> 00:06:11.360
universe.

00:06:10.479 --> 00:06:14.479
>> But AI is

00:06:11.360 --> 00:06:16.400
>> But AI is Yeah. because we just um we

00:06:14.479 --> 00:06:19.440
don't know how intelligent things can be

00:06:16.400 --> 00:06:21.840
and we don't know you know we know some

00:06:19.440 --> 00:06:24.479
things about the brain. There's maybe

00:06:21.840 --> 00:06:28.319
whatever 100 billion neurons

00:06:24.479 --> 00:06:29.840
>> 100 uh trillion sinapses uh and they run

00:06:28.319 --> 00:06:31.840
so fast but

00:06:29.840 --> 00:06:34.720
>> you know with our computers can we you

00:06:31.840 --> 00:06:36.960
know simulate that can we go beyond that

00:06:34.720 --> 00:06:37.600
>> um and how far and what would that be

00:06:36.960 --> 00:06:38.080
like?

00:06:37.600 --> 00:06:39.759
>> Yeah.

00:06:38.080 --> 00:06:42.080
>> Um

00:06:39.759 --> 00:06:44.639
>> we just don't know. I feel like in that

00:06:42.080 --> 00:06:46.639
in that way the question of you know how

00:06:44.639 --> 00:06:48.080
do we approach this what do we build who

00:06:46.639 --> 00:06:49.919
should work on this are all questions

00:06:48.080 --> 00:06:52.720
that are philosophical just as much as

00:06:49.919 --> 00:06:53.840
they are um you know technical or

00:06:52.720 --> 00:06:56.319
economic

00:06:53.840 --> 00:06:57.759
>> 100% yeah well consciousness is another

00:06:56.319 --> 00:06:58.639
kind of thing that gets brought into it

00:06:57.759 --> 00:07:00.400
you know

00:06:58.639 --> 00:07:02.000
>> the internet didn't raise questions of

00:07:00.400 --> 00:07:03.840
consciousness for example but right

00:07:02.000 --> 00:07:06.080
>> you know if this AI is smart enough and

00:07:03.840 --> 00:07:08.080
self-aware enough

00:07:06.080 --> 00:07:08.639
>> does that matter what does that mean I

00:07:08.080 --> 00:07:11.919
don't know

00:07:08.639 --> 00:07:14.080
>> yeah um You started Google famously in a

00:07:11.919 --> 00:07:16.240
garage in Menllo Park with Larry Pageige

00:07:14.080 --> 00:07:17.599
in 1998. You were just two guys trying

00:07:16.240 --> 00:07:19.280
to build something because you saw an

00:07:17.599 --> 00:07:20.960
opportunity. Now Google is a two

00:07:19.280 --> 00:07:24.400
trillion dollar company which I hope I

00:07:20.960 --> 00:07:24.800
have this right 180,000 employees. um

00:07:24.400 --> 00:07:27.280
maybe

00:07:24.800 --> 00:07:28.800
>> plus or minus plus or minus you track it

00:07:27.280 --> 00:07:31.039
as well as I

00:07:28.800 --> 00:07:33.440
>> um I'm sure riding the AI wave so to

00:07:31.039 --> 00:07:37.759
speak with all that infrastructure has

00:07:33.440 --> 00:07:39.840
many uh and many um advantages but I'm

00:07:37.759 --> 00:07:42.800
curious if there's any part of you that

00:07:39.840 --> 00:07:44.479
maybe just 1% wishes you were 20 years

00:07:42.800 --> 00:07:46.319
old again just graduated from Stanford

00:07:44.479 --> 00:07:48.560
it was just two guys in a garage.

00:07:46.319 --> 00:07:50.960
>> Um

00:07:48.560 --> 00:07:53.919
oh that's a good question. Um, I mean,

00:07:50.960 --> 00:07:55.919
look, I'm just grateful

00:07:53.919 --> 00:08:00.479
as computer scientists to be alive at

00:07:55.919 --> 00:08:02.479
any age, uh, during this time. Um,

00:08:00.479 --> 00:08:04.240
I think if you were to walk across the

00:08:02.479 --> 00:08:05.599
street, I don't know, maybe you can talk

00:08:04.240 --> 00:08:08.960
some folks into letting you know that

00:08:05.599 --> 00:08:11.039
later. Uh, you know, and you see how all

00:08:08.960 --> 00:08:13.520
of the AI researchers are all gathered

00:08:11.039 --> 00:08:15.360
kind of around around the coffee machine

00:08:13.520 --> 00:08:16.800
and whatnot, and, you know, everybody's

00:08:15.360 --> 00:08:18.319
excited. I mean, it is a very

00:08:16.800 --> 00:08:21.919
startuplike feel. Mhm.

00:08:18.319 --> 00:08:23.360
>> Uh it's not really a garage obviously.

00:08:21.919 --> 00:08:24.800
>> Uh though technically, by the way, when

00:08:23.360 --> 00:08:25.840
we started, we had the garage and a

00:08:24.800 --> 00:08:26.560
couple bedrooms.

00:08:25.840 --> 00:08:28.160
>> Okay, good.

00:08:26.560 --> 00:08:31.919
>> It was sort of like But we did have the

00:08:28.160 --> 00:08:32.479
garage. Uh but it wasn't just a garage.

00:08:31.919 --> 00:08:33.839
Um

00:08:32.479 --> 00:08:35.490
>> my greatest fear was getting a

00:08:33.839 --> 00:08:37.120
historical detail wrong, so I'm glad

00:08:35.490 --> 00:08:38.959
[laughter] that's done. Now we

00:08:37.120 --> 00:08:40.800
>> No, no, we we we tell the story kind of

00:08:38.959 --> 00:08:42.880
like a garage. I mean, but uh but but

00:08:40.800 --> 00:08:44.720
there were a couple uh rooms in

00:08:42.880 --> 00:08:45.360
addition, which helped a lot. That's

00:08:44.720 --> 00:08:49.920
good.

00:08:45.360 --> 00:08:54.320
>> Um but um I mean there is a very

00:08:49.920 --> 00:08:57.839
entrepreneurial sense I think

00:08:54.320 --> 00:09:00.160
given the kinds of um

00:08:57.839 --> 00:09:01.680
compute requirements um that are

00:09:00.160 --> 00:09:04.880
required to compete at the forefront

00:09:01.680 --> 00:09:07.120
right now. Um

00:09:04.880 --> 00:09:08.800
and um

00:09:07.120 --> 00:09:11.600
kind of the amount of science that goes

00:09:08.800 --> 00:09:15.839
into it. it would be really hard to try

00:09:11.600 --> 00:09:18.399
to um make a lot of headway,

00:09:15.839 --> 00:09:21.040
>> at least on the foundation model side as

00:09:18.399 --> 00:09:23.040
a couple guys in the garage. Uh lots of

00:09:21.040 --> 00:09:25.120
folks in garages

00:09:23.040 --> 00:09:28.800
can use these models to create new and

00:09:25.120 --> 00:09:30.160
amazing things. Um and um I don't want

00:09:28.800 --> 00:09:32.160
to discount the possibility that

00:09:30.160 --> 00:09:35.519
somebody's just going to have some idea

00:09:32.160 --> 00:09:37.600
that's so brilliant uh that even in a

00:09:35.519 --> 00:09:39.440
sort of a couple people in the garage

00:09:37.600 --> 00:09:43.200
could pull it off.

00:09:39.440 --> 00:09:48.480
Um, but it it seems like

00:09:43.200 --> 00:09:50.000
the the frontier is being pushed by,

00:09:48.480 --> 00:09:52.000
you know, pretty big companies like

00:09:50.000 --> 00:09:54.240
ours. I think we're now at the frontier.

00:09:52.000 --> 00:09:55.839
I'm very proud of the product progress

00:09:54.240 --> 00:09:59.200
we made over the last

00:09:55.839 --> 00:10:00.800
>> year. Um, so honestly, I'm really

00:09:59.200 --> 00:10:01.839
grateful to be able to be a part of

00:10:00.800 --> 00:10:04.320
that. So,

00:10:01.839 --> 00:10:06.560
>> I don't think I would take that uh

00:10:04.320 --> 00:10:07.040
teleportation to my younger self just

00:10:06.560 --> 00:10:10.080
yet.

00:10:07.040 --> 00:10:13.440
>> Yeah. What is the most sci-fi sounding

00:10:10.080 --> 00:10:15.440
thing that you actually believe has like

00:10:13.440 --> 00:10:17.519
a decent chance of becoming real let's

00:10:15.440 --> 00:10:19.279
say in the next 10 years?

00:10:17.519 --> 00:10:21.040
I [snorts] think the most exciting will

00:10:19.279 --> 00:10:23.680
be

00:10:21.040 --> 00:10:26.680
uh

00:10:23.680 --> 00:10:26.680
Gemini

00:10:27.920 --> 00:10:32.560
making some really substantial

00:10:29.839 --> 00:10:36.320
contribution to itself

00:10:32.560 --> 00:10:38.720
>> in terms of uh you know machine learning

00:10:36.320 --> 00:10:41.440
idea that it comes up with maybe

00:10:38.720 --> 00:10:43.600
implements uh and to develop the next

00:10:41.440 --> 00:10:47.279
version of itself. We already use Gemini

00:10:43.600 --> 00:10:49.360
a lot during like pieces like some AI

00:10:47.279 --> 00:10:53.680
researcher will be like oh I need to you

00:10:49.360 --> 00:10:56.160
know debug this code for me or um you

00:10:53.680 --> 00:10:57.920
know help me with this math uh or

00:10:56.160 --> 00:11:01.839
something like that as sort of one-offs

00:10:57.920 --> 00:11:04.320
but uh as a sort of really substantive

00:11:01.839 --> 00:11:07.600
some kind of new

00:11:04.320 --> 00:11:11.440
um significant breakthrough

00:11:07.600 --> 00:11:14.160
um that the AI itself makes. Um, I think

00:11:11.440 --> 00:11:15.760
that to me that's science fiction and um

00:11:14.160 --> 00:11:18.079
then I think it could well happen.

00:11:15.760 --> 00:11:20.399
>> Yeah. If you had to ballpark guess when

00:11:18.079 --> 00:11:21.760
do you think Gemini will create the next

00:11:20.399 --> 00:11:24.240
version of Gemini?

00:11:21.760 --> 00:11:27.279
>> Uh, I mean I I like I said it's already

00:11:24.240 --> 00:11:30.480
assisting that's already happening. Um,

00:11:27.279 --> 00:11:32.720
but from like kind of some kind of from

00:11:30.480 --> 00:11:36.320
scratch sort of rewrite. I don't know.

00:11:32.720 --> 00:11:38.959
That's a tough question. I don't know if

00:11:36.320 --> 00:11:43.200
I don't know how high of a priority that

00:11:38.959 --> 00:11:44.959
is to some extent because we sort of

00:11:43.200 --> 00:11:47.120
can

00:11:44.959 --> 00:11:49.760
guide it you know at what point is it

00:11:47.120 --> 00:11:51.279
possible

00:11:49.760 --> 00:11:53.360
um

00:11:51.279 --> 00:11:56.560
maybe in a

00:11:53.360 --> 00:11:58.640
3 years let's say three four years I I

00:11:56.560 --> 00:12:00.240
don't know if it would the vision the

00:11:58.640 --> 00:12:02.473
virtue it would make just by itself

00:12:00.240 --> 00:12:05.120
would be quite as good as itself

00:12:02.473 --> 00:12:08.240
[snorts] But um you know if you think

00:12:05.120 --> 00:12:09.920
about it with our new um video model the

00:12:08.240 --> 00:12:12.160
V3 let's I mean

00:12:09.920 --> 00:12:13.680
>> which by the way brought me to tears in

00:12:12.160 --> 00:12:15.360
the [laughter] demo area just a few

00:12:13.680 --> 00:12:17.839
>> in a good way I hope in a very good way.

00:12:15.360 --> 00:12:18.720
>> Okay just yeah the sound something about

00:12:17.839 --> 00:12:20.800
it just came

00:12:18.720 --> 00:12:22.560
>> yeah no sound is such a huge deal

00:12:20.800 --> 00:12:24.160
>> I didn't realize how much that was

00:12:22.560 --> 00:12:25.600
missing until it and it was all there

00:12:24.160 --> 00:12:26.240
and it just yeah it hit me like a ton of

00:12:25.600 --> 00:12:29.200
bricks.

00:12:26.240 --> 00:12:31.600
>> Oh well thank you. Um,

00:12:29.200 --> 00:12:33.680
but you know, in theory, I guess I've

00:12:31.600 --> 00:12:35.360
never tried this, but well, first of

00:12:33.680 --> 00:12:37.360
all, you don't I don't know if we

00:12:35.360 --> 00:12:40.320
support this in the user interface, but

00:12:37.360 --> 00:12:42.560
you don't have to give it a prompt. Um,

00:12:40.320 --> 00:12:43.839
like it will just generate a video. Our

00:12:42.560 --> 00:12:46.240
user interface might not actually

00:12:43.839 --> 00:12:48.320
support not having a prompt. Uh, but

00:12:46.240 --> 00:12:48.959
then you would have no idea what it's

00:12:48.320 --> 00:12:49.360
going to make.

00:12:48.959 --> 00:12:53.200
>> Yeah.

00:12:49.360 --> 00:12:56.240
>> Um, you could just say make a good

00:12:53.200 --> 00:12:58.639
video, I guess. um that would be sort of

00:12:56.240 --> 00:13:00.880
the model just by itself going but

00:12:58.639 --> 00:13:03.200
generally I think when it's directed by

00:13:00.880 --> 00:13:06.399
a person and presumably that's what you

00:13:03.200 --> 00:13:07.839
did you gave it some prompt some target

00:13:06.399 --> 00:13:10.240
>> uh you know you get really good results

00:13:07.839 --> 00:13:12.480
I guess I'm just saying

00:13:10.240 --> 00:13:15.519
if Gemini creates the next great version

00:13:12.480 --> 00:13:17.920
of Gemini I think

00:13:15.519 --> 00:13:20.160
>> for the sort of foreseeable future it

00:13:17.920 --> 00:13:22.800
will probably do better if there's a

00:13:20.160 --> 00:13:24.480
human that sort of guides it at at least

00:13:22.800 --> 00:13:26.720
at some high

00:13:24.480 --> 00:13:30.320
way. I mean, it it could conceivably

00:13:26.720 --> 00:13:32.160
someday blank slate, just go to town, do

00:13:30.320 --> 00:13:35.040
everything without any guidance.

00:13:32.160 --> 00:13:36.639
>> Um, but yeah, that's that's level of

00:13:35.040 --> 00:13:37.040
sci-fi I don't think we've gotten to

00:13:36.639 --> 00:13:39.360
yet.

00:13:37.040 --> 00:13:42.160
>> Got it. So, for the foreseeable future,

00:13:39.360 --> 00:13:44.959
you do foresee a world in which it's the

00:13:42.160 --> 00:13:46.639
Google employees helping the AI along to

00:13:44.959 --> 00:13:47.760
build the next versions of Gemini, the

00:13:46.639 --> 00:13:48.720
next the future that we're going to

00:13:47.760 --> 00:13:51.200
>> Yeah. Yeah, that's right.

00:13:48.720 --> 00:13:53.040
>> Yeah. Yeah. [snorts] Um, how much I'm

00:13:51.200 --> 00:13:55.680
curious how much of your time and energy

00:13:53.040 --> 00:13:57.680
you spend on sort of these like the

00:13:55.680 --> 00:14:01.199
deeper more meaty philosophical

00:13:57.680 --> 00:14:03.040
questions of AI versus I'm sure all of

00:14:01.199 --> 00:14:04.639
the the the technical questions, the

00:14:03.040 --> 00:14:06.800
practical questions, the business

00:14:04.639 --> 00:14:08.480
questions.

00:14:06.800 --> 00:14:10.480
How Yeah. How much of your energy goes

00:14:08.480 --> 00:14:12.560
towards all of that given that it is

00:14:10.480 --> 00:14:14.560
such a you know feels like we're

00:14:12.560 --> 00:14:16.880
discovering some fundamental underlying

00:14:14.560 --> 00:14:18.560
capability of the universe at the same

00:14:16.880 --> 00:14:20.480
time as building cool tech.

00:14:18.560 --> 00:14:22.720
>> Yeah.

00:14:20.480 --> 00:14:24.480
I mean,

00:14:22.720 --> 00:14:26.240
probably not that much goes to the

00:14:24.480 --> 00:14:28.720
philosophical questions just as a

00:14:26.240 --> 00:14:31.120
practical matter. It's just like there's

00:14:28.720 --> 00:14:32.959
so many technical details you need to

00:14:31.120 --> 00:14:33.440
get right on the on the way there.

00:14:32.959 --> 00:14:36.240
>> Yeah.

00:14:33.440 --> 00:14:38.720
>> Um, you know, I'm ftting about our being

00:14:36.240 --> 00:14:41.120
able to sign up for whatever Ultra and

00:14:38.720 --> 00:14:42.800
VO3 and all the all the things that

00:14:41.120 --> 00:14:45.120
aren't quite working as I'd hoped. And

00:14:42.800 --> 00:14:46.959
I'm uh right now hassling the engineers

00:14:45.120 --> 00:14:49.920
and the product managers about all the

00:14:46.959 --> 00:14:52.320
little um snafuss.

00:14:49.920 --> 00:14:55.600
Um I mean it's definitely nice to take a

00:14:52.320 --> 00:15:00.000
step back.

00:14:55.600 --> 00:15:03.680
Um some of the philosophical questions

00:15:00.000 --> 00:15:06.000
do kind of im emerge

00:15:03.680 --> 00:15:08.959
out of the technical details

00:15:06.000 --> 00:15:11.519
>> like um you know we have some new model

00:15:08.959 --> 00:15:13.440
how are we how are we going to evaluate

00:15:11.519 --> 00:15:15.440
it let's say

00:15:13.440 --> 00:15:17.279
>> um like what does it mean for the model

00:15:15.440 --> 00:15:19.440
to be good we have sort of standard

00:15:17.279 --> 00:15:21.360
benchmarks and things that at some point

00:15:19.440 --> 00:15:23.199
the models tend to get really good at

00:15:21.360 --> 00:15:24.880
those benchmarks

00:15:23.199 --> 00:15:26.959
um

00:15:24.880 --> 00:15:30.399
And uh you know every time you need to

00:15:26.959 --> 00:15:32.399
kind of redesign that you do take a step

00:15:30.399 --> 00:15:34.560
back kind of philosophically and try to

00:15:32.399 --> 00:15:38.720
figure out you know what is important.

00:15:34.560 --> 00:15:40.560
Um when you have new kinds of AI models

00:15:38.720 --> 00:15:42.720
uh the diffusion model for example that

00:15:40.560 --> 00:15:44.720
you can now uh play with

00:15:42.720 --> 00:15:46.880
>> um

00:15:44.720 --> 00:15:48.800
text diffusion that's not like an apples

00:15:46.880 --> 00:15:52.639
to apples thing. Mhm.

00:15:48.800 --> 00:15:54.639
>> So now we're kind of asking, well,

00:15:52.639 --> 00:15:56.160
how do we compare a thing that doesn't,

00:15:54.639 --> 00:15:58.560
you know, go left to right that's

00:15:56.160 --> 00:15:59.920
actually kind of spinning the whole

00:15:58.560 --> 00:16:00.399
thing out at once?

00:15:59.920 --> 00:16:04.720
>> Yeah.

00:16:00.399 --> 00:16:08.079
>> Um, how do we measure

00:16:04.720 --> 00:16:11.120
that compared to our uh kind of normal

00:16:08.079 --> 00:16:13.199
auto reagive models? So, a lot of these

00:16:11.120 --> 00:16:14.720
things bring up some philosophical

00:16:13.199 --> 00:16:17.199
questions.

00:16:14.720 --> 00:16:19.360
Um

00:16:17.199 --> 00:16:22.399
but you are pretty grounded and knows

00:16:19.360 --> 00:16:23.600
the grindstone uh no pun intended

00:16:22.399 --> 00:16:25.600
>> uh answering them.

00:16:23.600 --> 00:16:27.199
>> Yeah. Yeah. Well, you're it's it's

00:16:25.600 --> 00:16:29.120
rooted in such practicality. You

00:16:27.199 --> 00:16:30.560
actually it's not you know it's not

00:16:29.120 --> 00:16:31.920
theory about what other people are

00:16:30.560 --> 00:16:33.040
working on when you're actually in the

00:16:31.920 --> 00:16:35.279
lab building.

00:16:33.040 --> 00:16:36.959
>> Yeah. Yeah. So, I don't know. Probably

00:16:35.279 --> 00:16:38.399
be great for me to be able to spend more

00:16:36.959 --> 00:16:39.680
time on some of the philosophical

00:16:38.399 --> 00:16:42.560
questions.

00:16:39.680 --> 00:16:43.600
>> Um but there's there's a lot going on.

00:16:42.560 --> 00:16:45.920
>> There's a lot there.

00:16:43.600 --> 00:16:47.839
>> Yeah. Um, I don't know how much time we

00:16:45.920 --> 00:16:49.440
have left, but one question I do want to

00:16:47.839 --> 00:16:52.000
make sure I get the chance to ask is,

00:16:49.440 --> 00:16:54.480
what's a question or a topic that you

00:16:52.000 --> 00:16:55.360
wish people like me, interviewers, asked

00:16:54.480 --> 00:16:57.600
you more about?

00:16:55.360 --> 00:17:01.920
>> A question or a topic that I wish people

00:16:57.600 --> 00:17:04.720
would ask me about? Oh my gosh. Um,

00:17:01.920 --> 00:17:06.640
h

00:17:04.720 --> 00:17:08.480
okay to formulate this as a question or

00:17:06.640 --> 00:17:10.079
I guess I guess I could just sort of

00:17:08.480 --> 00:17:13.520
answer the question. Maybe that'd be

00:17:10.079 --> 00:17:16.520
easier. Um, uh, I I I guess it's this

00:17:13.520 --> 00:17:16.520
overall

00:17:16.640 --> 00:17:22.400
idea

00:17:19.280 --> 00:17:24.400
that people sort of react to, let's say,

00:17:22.400 --> 00:17:27.839
new AI announcements, whatever. We we

00:17:24.400 --> 00:17:30.240
announced a bunch of things yesterday

00:17:27.839 --> 00:17:32.160
and now, you know, what can you do with

00:17:30.240 --> 00:17:33.840
those things?

00:17:32.160 --> 00:17:35.280
Um,

00:17:33.840 --> 00:17:37.440
and there's a bunch of cool stuff you

00:17:35.280 --> 00:17:39.760
can do with those things. Um, and then

00:17:37.440 --> 00:17:42.400
there's a bunch of stuff you can't or

00:17:39.760 --> 00:17:44.160
it's not quite right.

00:17:42.400 --> 00:17:47.679
But I think the interesting question is

00:17:44.160 --> 00:17:51.440
what are you going to be able to do with

00:17:47.679 --> 00:17:56.160
kind of those things next generations

00:17:51.440 --> 00:17:57.840
in one year in two years. Um, and that's

00:17:56.160 --> 00:17:59.600
that brings uh, you know, all kinds of

00:17:57.840 --> 00:18:01.840
exciting questions. M

00:17:59.600 --> 00:18:03.919
>> uh I mean

00:18:01.840 --> 00:18:05.840
language models

00:18:03.919 --> 00:18:07.600
2 years ago made so many very

00:18:05.840 --> 00:18:08.559
embarrassing

00:18:07.600 --> 00:18:09.120
uh errors.

00:18:08.559 --> 00:18:12.400
>> Mhm.

00:18:09.120 --> 00:18:14.000
>> It was kind of like,

00:18:12.400 --> 00:18:17.600
oh wow, this thing actually did this

00:18:14.000 --> 00:18:19.360
correctly and that's super cool.

00:18:17.600 --> 00:18:22.080
That's very different than, oh my gosh,

00:18:19.360 --> 00:18:24.320
I can actually use this as a tool for

00:18:22.080 --> 00:18:26.160
whatever it is because I, you know, if

00:18:24.320 --> 00:18:28.720
it's, you know, going to be right 20% of

00:18:26.160 --> 00:18:30.559
the time, I can, you know, whatever,

00:18:28.720 --> 00:18:32.720
post a tweet with it, which is like,

00:18:30.559 --> 00:18:35.280
wow, that's cool. But I can't actually

00:18:32.720 --> 00:18:38.559
use it dayto-day.

00:18:35.280 --> 00:18:40.000
uh and um

00:18:38.559 --> 00:18:41.840
nevertheless when you kind of look at

00:18:40.000 --> 00:18:44.480
the trend

00:18:41.840 --> 00:18:48.160
uh you probably will be able to use it

00:18:44.480 --> 00:18:50.320
dayto-day with reasonable reliability.

00:18:48.160 --> 00:18:51.919
>> Um so

00:18:50.320 --> 00:18:53.440
I think as people kind of think about

00:18:51.919 --> 00:18:55.120
where they're going to plug these tools

00:18:53.440 --> 00:18:57.919
in

00:18:55.120 --> 00:18:59.760
uh to whatever it is they're trying to

00:18:57.919 --> 00:19:00.320
do, you know, you're trying to make a

00:18:59.760 --> 00:19:00.640
movie.

00:19:00.320 --> 00:19:02.799
>> Yeah.

00:19:00.640 --> 00:19:04.320
>> Uh VO is cool and that has sound now.

00:19:02.799 --> 00:19:05.600
you know, maybe a year ago you've been

00:19:04.320 --> 00:19:09.200
like, well, it doesn't have a sound will

00:19:05.600 --> 00:19:11.039
be a pain. Um, we've done some work for

00:19:09.200 --> 00:19:14.080
continuity of characters and things like

00:19:11.039 --> 00:19:15.440
that, but it's uh it's and we we

00:19:14.080 --> 00:19:18.080
actually have some movies we are making

00:19:15.440 --> 00:19:20.720
with it, but it's probably still not

00:19:18.080 --> 00:19:22.480
ideal for making

00:19:20.720 --> 00:19:25.360
like some kind of two-hour film.

00:19:22.480 --> 00:19:27.520
>> Sure. Um [snorts] but nevertheless I

00:19:25.360 --> 00:19:29.760
think when you look at how far these

00:19:27.520 --> 00:19:31.440
tools have come in the last couple years

00:19:29.760 --> 00:19:33.440
you know all the video models not just

00:19:31.440 --> 00:19:36.320
ours but you know

00:19:33.440 --> 00:19:37.760
>> um all of them and you forecast in a

00:19:36.320 --> 00:19:39.280
couple more years boy you're going to be

00:19:37.760 --> 00:19:40.799
able to do a lot of really interesting

00:19:39.280 --> 00:19:42.080
things with them. M

00:19:40.799 --> 00:19:43.919
>> um so I

00:19:42.080 --> 00:19:45.760
>> can you give some examples of that

00:19:43.919 --> 00:19:48.400
because one of the arguments I've heard

00:19:45.760 --> 00:19:49.840
is like by far video is the most exp if

00:19:48.400 --> 00:19:51.440
you look at all the different modalities

00:19:49.840 --> 00:19:53.200
here video is the most computationally

00:19:51.440 --> 00:19:55.520
expensive and what's the practical

00:19:53.200 --> 00:19:58.640
application fun videos on YouTube you

00:19:55.520 --> 00:20:01.440
know AI rot on Tik Tok like there's a

00:19:58.640 --> 00:20:04.160
lot of um controversy or conversation

00:20:01.440 --> 00:20:05.760
amongst uh you know folks about why are

00:20:04.160 --> 00:20:07.200
we investing so much time and energy

00:20:05.760 --> 00:20:08.799
into doing it besides the fact that it's

00:20:07.200 --> 00:20:09.840
really really cool. Can you share what

00:20:08.799 --> 00:20:12.320
are some of those other practical

00:20:09.840 --> 00:20:14.000
applications of these video models?

00:20:12.320 --> 00:20:18.400
>> Yeah, I mean like I say, I mean, I think

00:20:14.000 --> 00:20:20.400
it's like the difference between a cool

00:20:18.400 --> 00:20:23.600
toy

00:20:20.400 --> 00:20:25.520
and sort of a useful tool,

00:20:23.600 --> 00:20:29.280
you know, sort of a matter of of time

00:20:25.520 --> 00:20:30.880
and it happens gradually. And um

00:20:29.280 --> 00:20:34.720
uh you know, we're trying to aim for the

00:20:30.880 --> 00:20:38.000
useful tool. Um we have some uh film

00:20:34.720 --> 00:20:39.679
producers that are here. Um, I think

00:20:38.000 --> 00:20:41.360
Darren Aronowski might have already

00:20:39.679 --> 00:20:42.960
spoken to I don't know if he was on

00:20:41.360 --> 00:20:46.400
panel or something, but he's making a

00:20:42.960 --> 00:20:50.000
video. Um, I've um a close friend of

00:20:46.400 --> 00:20:52.000
mine, Dustin, is making a video. Um, but

00:20:50.000 --> 00:20:54.000
like you know, some real artists are

00:20:52.000 --> 00:20:55.520
using these tools,

00:20:54.000 --> 00:20:57.840
>> but it's early days. I mean, they're

00:20:55.520 --> 00:21:00.159
obviously

00:20:57.840 --> 00:21:03.840
dealing with something

00:21:00.159 --> 00:21:06.000
that two years from now movie directors

00:21:03.840 --> 00:21:08.240
will think was kind of a joke. Mhm.

00:21:06.000 --> 00:21:10.400
>> Uh but they're putting up with it to be,

00:21:08.240 --> 00:21:14.159
you know, on the frontier

00:21:10.400 --> 00:21:15.760
and I I think these models will be

00:21:14.159 --> 00:21:19.200
capable of producing really compelling

00:21:15.760 --> 00:21:21.760
videos. Um they'll be able to do it, you

00:21:19.200 --> 00:21:23.679
know, in concert with, you know, human

00:21:21.760 --> 00:21:24.000
directors, human actors and so forth.

00:21:23.679 --> 00:21:26.559
Yeah,

00:21:24.000 --> 00:21:29.520
>> I think that Areronowski film does have

00:21:26.559 --> 00:21:32.559
it has a combination of like you know

00:21:29.520 --> 00:21:35.840
real life sort of acting combined with

00:21:32.559 --> 00:21:37.760
AI generation in in a pretty cool way.

00:21:35.840 --> 00:21:41.039
Um

00:21:37.760 --> 00:21:44.640
but I you know look we today obviously

00:21:41.039 --> 00:21:46.640
we have the um industrial light magic

00:21:44.640 --> 00:21:48.480
all the you know uh Lucasfilm you know

00:21:46.640 --> 00:21:50.320
they do all the special effects you know

00:21:48.480 --> 00:21:53.360
we already use technology to generate

00:21:50.320 --> 00:21:56.720
film. um this is just sort of a new

00:21:53.360 --> 00:21:59.520
dimension in that and obviously

00:21:56.720 --> 00:22:02.000
early days and maybe it's not, you know,

00:21:59.520 --> 00:22:05.360
the best resolution. It's not like the

00:22:02.000 --> 00:22:07.120
best um continuity over a long period of

00:22:05.360 --> 00:22:08.720
time or whatnot, but

00:22:07.120 --> 00:22:11.760
>> I I think you'll see all those things

00:22:08.720 --> 00:22:14.080
come along. Uh so we're trying to push

00:22:11.760 --> 00:22:15.280
the envelope so these things become real

00:22:14.080 --> 00:22:16.480
tools,

00:22:15.280 --> 00:22:17.840
>> uh not just toys.

00:22:16.480 --> 00:22:19.840
>> Yeah. Yeah. Yeah. The one of the things

00:22:17.840 --> 00:22:21.520
I always say on on my platform is

00:22:19.840 --> 00:22:23.520
whatever you think about it today, this

00:22:21.520 --> 00:22:24.799
is the worst that it will ever look.

00:22:23.520 --> 00:22:25.120
>> Yeah, that's right. That's exactly

00:22:24.799 --> 00:22:26.720
correct.

00:22:25.120 --> 00:22:28.640
>> Um I Okay, so I got the signal for one

00:22:26.720 --> 00:22:31.679
more question. Um so I I'll I'll ask

00:22:28.640 --> 00:22:34.320
this one on on behalf of all of the, you

00:22:31.679 --> 00:22:37.039
know, the builders out there who are so

00:22:34.320 --> 00:22:38.640
excited about this moment, but are not a

00:22:37.039 --> 00:22:41.520
Google employee. They're not working at

00:22:38.640 --> 00:22:42.960
a Frontier Lab. Is there any sort of

00:22:41.520 --> 00:22:44.080
direction that you would point these

00:22:42.960 --> 00:22:45.919
people into? Whether they have a

00:22:44.080 --> 00:22:48.480
software engineering background or are

00:22:45.919 --> 00:22:50.080
just simply somebody who is understands

00:22:48.480 --> 00:22:52.159
the depth and the importance of this

00:22:50.080 --> 00:22:53.440
moment and wants to be involved. Is

00:22:52.159 --> 00:22:56.880
there a direction that you would point

00:22:53.440 --> 00:22:58.559
them into to say start here? Or maybe is

00:22:56.880 --> 00:23:00.000
there a direction you would say don't go

00:22:58.559 --> 00:23:01.760
in that way. Don't don't waste your

00:23:00.000 --> 00:23:04.080
time.

00:23:01.760 --> 00:23:05.840
>> Um

00:23:04.080 --> 00:23:08.000
well, there's so many great ideas, so I

00:23:05.840 --> 00:23:09.760
would be reluctant to stop anybody from

00:23:08.000 --> 00:23:10.320
doing anything cuz you you never know.

00:23:09.760 --> 00:23:12.559
Yeah.

00:23:10.320 --> 00:23:15.280
>> Um

00:23:12.559 --> 00:23:18.400
uh I think there's actually an

00:23:15.280 --> 00:23:20.240
increasing amount of really interesting

00:23:18.400 --> 00:23:23.679
academic work even you know outside of

00:23:20.240 --> 00:23:26.159
the big labs. Um and I think it has sort

00:23:23.679 --> 00:23:28.400
of happened with this um advent of

00:23:26.159 --> 00:23:30.320
reasoning models that that happens more

00:23:28.400 --> 00:23:32.400
in the what we call post-raining kind of

00:23:30.320 --> 00:23:35.840
reinforcement learning

00:23:32.400 --> 00:23:39.039
>> um step which is more manageable with

00:23:35.840 --> 00:23:41.679
the amount of compute resources that

00:23:39.039 --> 00:23:44.960
lots of um academic institutions or

00:23:41.679 --> 00:23:47.120
small companies have. Um and there are

00:23:44.960 --> 00:23:49.360
lots of actually open weight models

00:23:47.120 --> 00:23:51.760
including our Java models that you can

00:23:49.360 --> 00:23:55.679
use to experiment with that. Um,

00:23:51.760 --> 00:23:58.240
increasingly I think you'll see um some

00:23:55.679 --> 00:24:00.880
kinds of reinforcement learning APIs

00:23:58.240 --> 00:24:04.240
from the top models so that you know you

00:24:00.880 --> 00:24:06.400
can send us your problems um maybe with

00:24:04.240 --> 00:24:09.520
correct solutions or graders and you

00:24:06.400 --> 00:24:11.520
know we can contribute uh your problems

00:24:09.520 --> 00:24:14.240
the mix of like if you want the model to

00:24:11.520 --> 00:24:18.000
be good at that. Um so I think that kind

00:24:14.240 --> 00:24:20.799
of thing is coming. Uh yeah, I actually

00:24:18.000 --> 00:24:24.880
I think it's pretty good time

00:24:20.799 --> 00:24:28.000
um to be able to make an impact

00:24:24.880 --> 00:24:30.000
uh without having to train up a

00:24:28.000 --> 00:24:31.360
foundation model.

00:24:30.000 --> 00:24:34.640
>> Amazing. Well, thank you so much for

00:24:31.360 --> 00:24:34.640
your time. Thank you.

