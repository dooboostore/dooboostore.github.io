WEBVTT

00:00:00.080 --> 00:00:05.200
매우 감사합니다.  훌륭한 마무리.  저는

00:00:05.200 --> 00:00:10.000
공동 창립자인 로이 슈워츠, 짐

00:00:07.759 --> 00:00:12.320
반히를 대신하여 Axio의 공동 창립자인 마이크 앨런입니다.  Axios의 팬이 되어주신 모든 분들께 9년 동안 감사드리고,

00:00:15.040 --> 00:00:19.840
이 역사적인

00:00:17.039 --> 00:00:23.439
은행, 아주 멋진 장소인 샌프란시스코에서

00:00:19.840 --> 00:00:26.240
Axios AI와 SF 정상회담을 위해 참석해 주셔서 감사합니다.

00:00:23.439 --> 00:00:29.760
전 세계의 여러분 모두를 환영합니다.  우리의 큰

00:00:26.240 --> 00:00:33.520
마무리를 위해.  데이스 하사비스,

00:00:29.760 --> 00:00:37.120
구글 딥마인드의 공동 창립자이자 CEO.  그는

00:00:33.520 --> 00:00:40.640
신경과학자이자 기업가이며 AI

00:00:37.120 --> 00:00:44.640
선구자입니다.  데미스는 5살에 체스 신동이 되었고

00:00:40.640 --> 00:00:47.280
, 48살에 노벨상을 수상했습니다. 그는

00:00:44.640 --> 00:00:51.745
영국에서 태어난 천재입니다.  그는 죽었어.

00:00:47.280 --> 00:00:53.765
데이사바스, Axios에 오신 것을 환영합니다.

00:00:51.745 --> 00:00:53.765
[박수]

00:00:56.960 --> 00:01:00.079
정말 감사합니다.  초대해 주셔서 감사합니다.

00:00:58.640 --> 00:01:00.559
우리는 이것을 기대하고 있었습니다.

00:01:00.559 --> 00:01:04.879
여기에 참석하게 되어 감사합니다.

00:01:04.879 --> 00:01:10.320
당신이 노벨상 수상자라는 사실을 알게 된 지 불과 400일이 넘었습니다.  그리고

00:01:08.240 --> 00:01:11.600
당신은 그 순간에 이게

00:01:10.320 --> 00:01:13.200
초현실적이라고 말했어요.

00:01:11.600 --> 00:01:14.159
이게 가장 큰 문제예요.

00:01:13.200 --> 00:01:16.000
응.   그 이후로

00:01:14.159 --> 00:01:17.360
당신의

00:01:16.000 --> 00:01:18.640
삶과 일은 어떻게 바뀌었나요?  그것이 무엇을

00:01:17.360 --> 00:01:20.080
가능하게 했는가?

00:01:18.640 --> 00:01:21.920
음, 글쎄요, 사실 아직도 꽤

00:01:20.080 --> 00:01:24.799
초현실적이죠.  아직은 완전히

00:01:21.920 --> 00:01:26.080
실감나지 않지만, 어, 꽤 큰

00:01:24.799 --> 00:01:27.759
차이가 생겼어요.  가장 큰

00:01:26.080 --> 00:01:30.960
차이점은 같은 분야가

00:01:27.759 --> 00:01:32.479
아닌 사람들, 예를 들어

00:01:30.960 --> 00:01:33.920
대규모 정부 관계자 등

00:01:33.920 --> 00:01:38.960
AI에 대해 잘 모르는 사람들과 이야기할 때입니다.

00:01:36.479 --> 00:01:41.040
노벨상을 받았다는 것은,

00:01:38.960 --> 00:01:42.479
당신이

00:01:41.040 --> 00:01:43.920
해당 분야의 전문가라는 것을 다른 사람들에게 알리는 지름길과 같습니다.

00:01:42.479 --> 00:01:47.040
그러니까

00:01:43.920 --> 00:01:50.960
앞으로 유용할 것 같아요.  그리고 [웃음]

00:01:47.040 --> 00:01:52.880
그리고 여러분은 무한한 자원을

00:01:50.960 --> 00:01:55.200
활용할 수 있었습니다.

00:01:52.880 --> 00:01:56.000
당신이 가지고 있는 새로운 자원이나 지금 활용할 수 있다고 생각하는 자원이 있나요

00:01:55.200 --> 00:01:57.439
?

00:01:56.000 --> 00:01:59.439
설마.  그렇죠.  우리는

00:01:57.439 --> 00:02:01.119
구글의 딥마인드에서 일할 수 있어서 운이 좋았어요.

00:01:59.439 --> 00:02:03.040
우리는 많은 자원을 가지고 있습니다.

00:02:01.119 --> 00:02:05.200
그것들은 무한하지 않습니다.  우리는 항상 더 많은

00:02:03.040 --> 00:02:07.759
컴퓨팅을 필요로 합니다.  음,

00:02:05.200 --> 00:02:09.119
컴퓨팅 능력이 아무리 뛰어나도 상관없지만, 음, 우리는

00:02:07.759 --> 00:02:10.800
훌륭한 것들을 많이 가지고 있기

00:02:09.119 --> 00:02:13.120
때문에 그렇게 광범위한

00:02:10.800 --> 00:02:15.840
포트폴리오를 구축할 수 있는 겁니다.  하지만

00:02:13.120 --> 00:02:18.080
이 플랫폼은

00:02:15.840 --> 00:02:20.319
기본적으로 여러분이 관심 있는 것에 대해 말할 수 있는 공간을 제공합니다

00:02:18.080 --> 00:02:21.920
.  저는 아직 그런 일을 많이 하지는 않았지만

00:02:20.319 --> 00:02:23.280
, 그것이 중요할 것이라고 생각합니다

00:02:21.920 --> 00:02:25.280
.  아마도

00:02:23.280 --> 00:02:27.599
AI 안전과 다른 것들에 대해서도 이야기해 볼 수 있을 것 같아요.  저는

00:02:25.280 --> 00:02:30.400
노벨상과 그것을

00:02:27.599 --> 00:02:32.560
뒷받침하는 플랫폼이 그런 데에 유용할 수 있다고 생각합니다.

00:02:30.400 --> 00:02:34.080
그리고

00:02:32.560 --> 00:02:36.720
AI 안전 외에 플랫폼을

00:02:34.080 --> 00:02:37.360
사용해서 더 많이 이야기할 것으로 생각되는 짧은 목록에는 무엇이 있나요

00:02:36.720 --> 00:02:40.000
?

00:02:37.360 --> 00:02:42.080
네, 장기적으로 안전하다는 것만이 중요한 건 아닙니다

00:02:40.000 --> 00:02:44.000
.  AGI 안전은 당연히

00:02:42.080 --> 00:02:47.440
우리가 많이 생각하는 부분입니다. 하지만

00:02:44.000 --> 00:02:49.120
오늘날에는 AI를 책임감 있게 사용하는 것에 대한 것도 중요합니다.

00:02:47.440 --> 00:02:51.840
음,

00:02:49.120 --> 00:02:54.800
AI를 사용해서 개선하고

00:02:51.840 --> 00:02:56.080
강화하고 가속화해야 할 일의 종류는 무엇일까요? 그리고

00:02:59.120 --> 00:03:03.040
단기적으로 조심해야 할 일의 종류는 무엇일까요?  그래서 저는 그것이

00:03:00.720 --> 00:03:06.000
사회가

00:03:03.040 --> 00:03:07.519
다가올 일에 대비하도록 하는 하나의 방법이라고 생각합니다.  아시다시피, AGI는

00:03:07.519 --> 00:03:14.560
인류 역사상 가장 변혁적인 순간이 다가오고 있으며,

00:03:11.120 --> 00:03:16.879
우리는 사회적으로,

00:03:14.560 --> 00:03:18.800
그리고 종으로서 준비해야 합니다. 그리고 저는

00:03:16.879 --> 00:03:21.519
정부와 다른 중요한

00:03:18.800 --> 00:03:24.080
사람들, 다른 중요한 지도자들이 이 분야에서

00:03:21.519 --> 00:03:25.680
중요한 역할을 할 것이라고 생각합니다. 그리고

00:03:24.080 --> 00:03:28.800
노벨상

00:03:25.680 --> 00:03:31.200
플랫폼 같은 것이 있으면 거의 모든 문이 열릴 것이라고 생각합니다.

00:03:28.800 --> 00:03:34.879
당신을 특별하게 만드는 것 중 하나는

00:03:31.200 --> 00:03:37.760
당신이 과학에 깊이 관여하고 있으면서도 동시에

00:03:37.760 --> 00:03:43.599
기업,

00:03:40.080 --> 00:03:46.400
하이퍼스케일러, 초강대국 간의 싸움과 경쟁의 최전선에 서 있다는 것입니다. 당신은

00:03:43.599 --> 00:03:49.920
스티브 잡스와 같은 사람이고,

00:03:46.400 --> 00:03:52.000
제품에 대한 사고방식도 가지고 있습니다.  당신은

00:03:49.920 --> 00:03:55.040
사람들을 위해 즐거운 것들을 만들고 싶어하지만, 당신은

00:03:52.000 --> 00:03:56.799
항상 먼저 과학자라고 말합니다.

00:03:55.040 --> 00:03:58.480
네, 과학이죠.  저는 원래 과학자예요.

00:03:56.799 --> 00:04:00.319
제가 그렇게 말하는 이유는

00:03:58.480 --> 00:04:03.360
제가

00:04:00.319 --> 00:04:05.200
모든 일에 기본적으로 취하는 접근 방식이기 때문입니다.  그러니까 제가 말하고자 하는 것은

00:04:03.360 --> 00:04:08.080
과학적 방법이란

00:04:05.200 --> 00:04:09.680
바로 그런 사고방식을 뜻한다는 겁니다.  음, 저는 과학적 방법을 정말

00:04:08.080 --> 00:04:11.760
좋아합니다. 제 생각에는

00:04:09.680 --> 00:04:14.159
과학적 방법이

00:04:11.760 --> 00:04:16.000
인류가 지금까지 생각해 낸 가장 중요한 아이디어일 겁니다

00:04:14.159 --> 00:04:17.919
.  음,

00:04:16.000 --> 00:04:19.680
계몽주의와 현대 과학이 탄생했죠.

00:04:17.919 --> 00:04:22.320
그러니까 기본적으로 현대 문명은

00:04:22.320 --> 00:04:26.720
과학적 방법과 실험,

00:04:24.560 --> 00:04:28.639
그리고 가설을 업데이트하는 등의 아이디어에 의존합니다

00:04:26.720 --> 00:04:30.560
.  저는 이것이 엄청나게

00:04:28.639 --> 00:04:32.080
강력한 방법이라고 생각하는데,

00:04:30.560 --> 00:04:33.600
과학에만 적용되는 게 아니라고 생각합니다.

00:04:32.080 --> 00:04:36.320
일상

00:04:33.600 --> 00:04:38.240
생활은 물론이고 사업에도 적용될 수 있다고 생각합니다.  음, 제가 하려고 했던 건 그걸

00:04:38.240 --> 00:04:44.320
한계까지 끌어올리는 거였어요.  그리고 저는 그것이

00:04:46.000 --> 00:04:50.160
연구 기관으로서, 그리고 엔지니어링

00:04:48.080 --> 00:04:52.080
기관으로서 우리에게 어떤 면에서 이점을 제공한다고 생각합니다.  그렇습니다. 우리는 지금

00:04:50.160 --> 00:04:54.479
이 치열하고 아마도

00:04:52.080 --> 00:04:57.919
기술 역사상 가장 치열한 경쟁의 한가운데에 있습니다

00:04:54.479 --> 00:04:59.440
.  음, 음, 하지만

00:04:57.919 --> 00:05:01.759
제 생각에 우리에게 우위를 제공하는 것 중 하나는

00:04:59.440 --> 00:05:04.639
우리가 작업에 엄격함과 정밀성을 적용한다는 것입니다

00:05:01.759 --> 00:05:06.160
.  음, 왜냐하면 우리는

00:05:04.639 --> 00:05:08.720
과학적 방법을 핵심으로 두고 있고,

00:05:06.160 --> 00:05:10.400
세계적 수준의 연구,

00:05:08.720 --> 00:05:12.240
세계적 수준의 엔지니어링,

00:05:10.400 --> 00:05:14.479
세계적 수준의 인프라를 결합하고 있습니다.

00:05:14.479 --> 00:05:18.320
AI와 같은 분야의 최전선에 서려면 이 세 가지가 모두 필요하다고 생각합니다. 그리고 우리는

00:05:20.400 --> 00:05:26.560
그 모든 분야에서 세계적 수준의 역량을 갖추고 있다는 점에서 매우 독특하다고 생각합니다.

00:05:23.600 --> 00:05:28.639
음, Axio 방식으로

00:05:26.560 --> 00:05:31.680
대화를 확대

00:05:28.639 --> 00:05:35.199
와 축소로 나누어 보겠습니다. 그러니까, 확대해서 AI

00:05:31.680 --> 00:05:37.039
의 상태에 대해 여러분의 소중한 마음을 담아보세요

00:05:35.199 --> 00:05:39.199
.  그럼, 우리는

00:05:37.039 --> 00:05:41.840
AI의 둔탁한 상태에 대해 이야기해 보도록 하겠습니다.  그리고 제가

00:05:39.199 --> 00:05:43.840
여러분께 부탁드리고 싶은 것은

00:05:41.840 --> 00:05:46.639
오늘날 알려진 사실을 바탕으로 솔직하고,

00:05:43.840 --> 00:05:47.360
임상적이며, 과장이나 부드러운 판매 없이 말씀해 주시라는 것입니다.

00:05:46.639 --> 00:05:48.240
우리가 그렇게 할 수 있나요?

00:05:47.360 --> 00:05:50.639
최선을 다하겠습니다.

00:05:48.240 --> 00:05:53.680
괜찮은.  음,

00:05:50.639 --> 00:05:55.759
앞으로 12개월 동안의 진행 상황은 어떻게 될까요

00:05:53.680 --> 00:05:57.759
?

00:05:55.759 --> 00:05:59.440
오늘로부터 1년 후 우리가 여기 앉아

00:05:59.440 --> 00:06:02.400
세상이 어떻게 바뀌었을지 궁금하시죠?   음, 우리가

00:06:02.400 --> 00:06:09.520
열심히 추진하고 있는 것은

00:06:06.000 --> 00:06:11.280
모달리티의 융합이라고 생각합니다.  그래서

00:06:09.520 --> 00:06:12.479
우리의 주요 기반 모델인 제미니는

00:06:12.479 --> 00:06:17.600
처음부터 항상 멀티모달이었습니다.  이미지, 비디오,

00:06:14.880 --> 00:06:20.080
텍스트, 오디오를 입력받아 이제는

00:06:17.600 --> 00:06:22.160
점점 더 다양한 유형

00:06:20.080 --> 00:06:25.120
의 결과물을 만들어낼 수 있습니다.  음, 저는

00:06:22.160 --> 00:06:27.680
우리가

00:06:25.120 --> 00:06:29.280
다중 모드를 통해 정말 흥미로운 교차 수분을 얻고 있다고 생각합니다.

00:06:27.680 --> 00:06:32.319
그 중 가장 좋은 예가

00:06:29.280 --> 00:06:34.240
최신 이미지 모델인 NO Banana Pro입니다. 이 모델은

00:06:34.240 --> 00:06:38.000
시각적 요소에 대한 놀라운 이해력을 보여주고,

00:06:38.000 --> 00:06:41.520
정말 정확한 인포그래픽을 만들 수 있다고 생각합니다.  그래서 저는

00:06:40.160 --> 00:06:44.160
내년에는

00:06:41.520 --> 00:06:46.319
엄청난 진전을 보게 될 거라고 생각합니다.

00:06:44.160 --> 00:06:47.919
예를 들어 비디오가

00:06:46.319 --> 00:06:49.199
언어 모델과 융합되면

00:06:47.919 --> 00:06:51.039
매우

00:06:49.199 --> 00:06:52.720
흥미로운 기능 조합을 볼 수 있을 겁니다

00:06:51.039 --> 00:06:54.000
.

00:06:52.720 --> 00:06:55.919
제가

00:06:54.000 --> 00:06:58.479
개인적으로 작업하고 있는 내년에 우리가 보게 될 또 다른 것은 세계 모델입니다.

00:06:55.919 --> 00:07:00.960
그래서 저희는

00:06:58.479 --> 00:07:02.880
Genie Genie 3라고 불리는 시스템을 가지고 있는데,

00:07:00.960 --> 00:07:04.160
여러분이 생각할 수 있는 대화형 비디오 모델과 같습니다

00:07:02.880 --> 00:07:05.759
.  따라서

00:07:04.160 --> 00:07:07.199
영상을 만든 다음

00:07:05.759 --> 00:07:09.280
게임이나

00:07:07.199 --> 00:07:12.240
시뮬레이션에 있는 것처럼 영상 속을 돌아다니며 볼 수도 있고 영상은 1분 동안 일관성을 유지할 수 있습니다

00:07:09.280 --> 00:07:14.800
.  저는 그것이 매우 흥미롭다고 생각합니다.  음,

00:07:12.240 --> 00:07:17.039
그리고 음, 알다시피 다른

00:07:14.800 --> 00:07:18.160
건 에이전트 기반 시스템일 수도 있겠네요.  그래서 저는

00:07:17.039 --> 00:07:19.919
이 분야에서

00:07:18.160 --> 00:07:22.080
에이전트에 대해 많이 이야기해 왔지만, 그들은

00:07:19.919 --> 00:07:22.800
아직 모든 작업을 수행할 만큼 신뢰할 만하지 않다고 생각합니다.

00:07:22.080 --> 00:07:24.400
하지만

00:07:22.800 --> 00:07:26.400
오늘 Axia 무대에서 우리는 그에 대해 많은 이야기를 들었다고 생각합니다

00:07:24.400 --> 00:07:28.479
.  1

00:07:26.400 --> 00:07:31.120
년 뒤에는 무슨 말을 하시겠습니까?  에이전트들은 어떻게

00:07:28.479 --> 00:07:32.720
진행되었나요?

00:07:31.120 --> 00:07:33.520
1년 뒤 일상생활에서 어떻게 적용될지 보여주는 예는 무엇입니까

00:07:32.720 --> 00:07:35.440
?

00:07:33.520 --> 00:07:36.960
음, 보세요, 우리는

00:07:36.960 --> 00:07:41.280
제미니가 결국에는 보편적인 조수가 되기를 바란다는 개념이 있습니다.  음, 저는

00:07:39.919 --> 00:07:42.479
이것이

00:07:41.280 --> 00:07:45.680
내년에 우리가 보여줄 모습이라고 생각합니다.  이 기능은

00:07:42.479 --> 00:07:47.280
더 많은 기기에서도 지원될 예정입니다.  보편적이라는 것은

00:07:45.680 --> 00:07:49.440
컴퓨터

00:07:47.280 --> 00:07:50.960
나 노트북, 휴대폰에서만 그런 것이 아니라

00:07:50.960 --> 00:07:56.240
안경이나 다른 장치를 통해서도 우리와 함께한다는 것을 의미합니다.  그리고 음, 저는

00:07:54.720 --> 00:07:58.639
여러분이

00:07:58.639 --> 00:08:03.120
일상생활에서

00:08:01.199 --> 00:08:04.720
하루에 여러 번 참고할 수 있는 유용한 무언가를 만들고 싶다고 생각합니다.  그것은

00:08:03.120 --> 00:08:06.639
여러분 삶의 일부가 되고 생산성을 향상시킬 뿐만 아니라

00:08:04.720 --> 00:08:08.479
개인적인

00:08:06.639 --> 00:08:10.800
삶도 향상시킵니다. 아시다시피, 책과 영화에 대한 추천이나

00:08:10.800 --> 00:08:15.120
여러분이 원하는 다른 활동도 말이죠. 하지만

00:08:13.680 --> 00:08:18.560
현재 에이전트는 여러분이

00:08:15.120 --> 00:08:20.080
전체 작업을 위임할 수 없고,

00:08:18.560 --> 00:08:23.280
그들이 그

00:08:20.080 --> 00:08:25.039
전체 작업을 완벽하게 신뢰할 수 있게 완료할 것이라고 확신할 수 없습니다.

00:08:23.280 --> 00:08:26.479
하지만 1년 후에는 그렇게 할 수 있을 거라고 생각하시나요?

00:08:25.039 --> 00:08:29.599
1년 후에는 그렇게 할 수 있는

00:08:26.479 --> 00:08:34.159
에이전트가 생길 거라고 생각합니다.

00:08:34.159 --> 00:08:40.959
AI가 세상을 위해 할 수 있는 가장 좋은 사례는 무엇이고,

00:08:37.680 --> 00:08:43.039
여러분이 가장 두려워하는 것은 무엇입니까?

00:08:40.959 --> 00:08:44.480
글쎄요,

00:08:43.039 --> 00:08:45.839
제가 항상

00:08:44.480 --> 00:08:48.240
꿈꿔왔던 가장 좋은 시나리오는 제가

00:08:45.839 --> 00:08:49.920
평생 AI에 매달린 이유이고,

00:08:49.920 --> 00:08:56.000
수십 년 동안 노력해 온 이 순간에 가까워지고 있다는 것을 아시죠. 우리 중 많은 사람들이 일종의

00:08:56.000 --> 00:09:00.080
급진적 풍요로움이라고 부르는 것입니다.  그래서 이 아이디어는

00:09:02.640 --> 00:09:08.000
오늘날 사회와 인류가 직면한 가장 큰 문제들을 많이 해결했습니다.  그러니까 무료 재생

00:09:05.600 --> 00:09:10.160
가능 청정 에너지든,

00:09:08.000 --> 00:09:13.120
핵융합이나 더 나은 배터리 최적

00:09:10.160 --> 00:09:15.120
배터리, 태양열 소재,

00:09:13.120 --> 00:09:17.600
반도체, 재료

00:09:15.120 --> 00:09:19.760
과학이든 말이죠.  우리는 많은 질병을 해결했습니다.

00:09:17.600 --> 00:09:21.360
그러니까 우리는

00:09:19.760 --> 00:09:24.880
새로운 시대,

00:09:21.360 --> 00:09:27.120
희소성 이후의 시대에 들어선 셈이고, 잠재적으로 인류는

00:09:24.880 --> 00:09:29.120
번영하고

00:09:27.120 --> 00:09:30.880
별을 여행하며

00:09:29.120 --> 00:09:33.680
은하계로 의식을 퍼뜨릴 수 있는 겁니다.

00:09:30.880 --> 00:09:37.600
그리고 당신이 가장 두려워하는 것은 무엇입니까?

00:09:33.680 --> 00:09:39.839
글쎄요, 그런 유토피아적 관점조차도 이런 기술이

00:09:39.839 --> 00:09:44.480
존재하고 그 기술이 모든 문제를 해결한다면 인간으로서 우리의 목적이 무엇이 될 것인가에 대한 몇 가지 의문을 품게 합니다

00:09:45.600 --> 00:09:48.720
.  모두 해결해야 할 문제입니다.

00:09:47.279 --> 00:09:50.399
과학자로서 저는 그 점에 대해 걱정하고

00:09:48.720 --> 00:09:52.320
, 과학적 방법

00:09:50.399 --> 00:09:55.200
도 걱정합니다.  그래서, 그런 것도 있지만,

00:09:55.200 --> 00:10:01.120
AI에는 잘 알려진 어려움과 위험이

00:09:57.360 --> 00:10:04.480
두 가지 있습니다.  하나는 나쁜 행위자들이

00:10:01.120 --> 00:10:06.240
AI를 해로운 목적으로 사용하거나 AI 자체가

00:10:04.480 --> 00:10:09.040
AGI에 가까워지고 더 지능적으로 변하면서

00:10:09.040 --> 00:10:13.760
인류에게 해를 끼치는 방식으로 길을 잃는 것입니다.

00:10:11.120 --> 00:10:17.120
그러니까, 당신은 잘못된 방향으로 갔다고 말씀하셨죠.

00:10:13.760 --> 00:10:19.120
음, 당신은 이런

00:10:17.120 --> 00:10:20.800
재앙적인 결과에 대해 얼마나 걱정하시나요?  당신의

00:10:19.120 --> 00:10:24.160
우려 수준은?  저는 그냥 그것들을 늘어놓을 겁니다

00:10:20.800 --> 00:10:25.440
.  첫째,

00:10:24.160 --> 00:10:26.640
AI를 사용하는 사악한 행위자가 만든 병원균입니다.

00:10:25.440 --> 00:10:28.560
음.

00:10:26.640 --> 00:10:30.399
저는 그것이 확실히

00:10:28.560 --> 00:10:31.920
우리가 반드시 예방해야 할 나쁜 사용 사례 시나리오 중 하나라고 생각합니다

00:10:30.399 --> 00:10:35.600
.

00:10:35.600 --> 00:10:38.720
외국의 AI를 이용한 에너지나 물 관련 사이버 테러.

00:10:36.959 --> 00:10:40.720
네, 아마 이미 그런 일이 일어나고 있을 거라고

00:10:38.720 --> 00:10:42.720
말하고 싶습니다.

00:10:42.720 --> 00:10:47.519
아직은 매우 정교한 AI가 등장하지 않을 수도 있지만, 제 생각에는 그것이 가장 확실한

00:10:45.200 --> 00:10:49.600
취약 벡터입니다.  음, 그래서

00:10:47.519 --> 00:10:51.600
우리가 사이버 보안을 위한 AI에 많은 관심을 기울이고 있고,

00:10:49.600 --> 00:10:54.160
Google과 DeepMind에서도 많은 관심을 기울이고 있습니다

00:10:51.600 --> 00:10:56.240
.  그래서

00:10:54.160 --> 00:10:57.200
그 방정식의 방어적인 측면을 강화하기 위해

00:10:56.240 --> 00:11:00.160
,

00:10:57.200 --> 00:11:01.839
인간의 통제 밖에서 스스로 작동하는 AI가 필요합니다

00:11:00.160 --> 00:11:03.360
.

00:11:01.839 --> 00:11:05.680
글쎄요, 이건 다시 에이전트적인 측면으로 돌아가는 거예요. 에이전트적인 측면이 점점

00:11:03.360 --> 00:11:07.600
더

00:11:05.680 --> 00:11:09.120
정교해지고,

00:11:07.600 --> 00:11:10.880
업계에서 이런 것들을 만드는 이유가 분명해졌거든요

00:11:10.880 --> 00:11:15.680
. 이런 것들이 지원 같은 것으로 더 유용해질 테니까요.  음, 그런 일들은 분명히 일어날 겁니다.

00:11:13.040 --> 00:11:17.760
하지만 더 자율적이고

00:11:15.680 --> 00:11:20.560
자율적일수록,

00:11:22.720 --> 00:11:26.959
처음 지시나

00:11:24.880 --> 00:11:28.880
목표를 제시했을 때 의도했던 것과 달라질 여지가 더 커집니다.  따라서 이는

00:11:31.920 --> 00:11:37.279
지속적인 학습이나 온라인 학습이 가능한 시스템이 설정된

00:11:34.160 --> 00:11:39.279
보호 범위 내에 머물도록 하는 방법에 대한 매우 활발한 연구 분야입니다

00:11:37.279 --> 00:11:42.959
.  제가 말하고자 하는 건, 좋은 소식은

00:11:39.279 --> 00:11:45.519
AI가

00:11:42.959 --> 00:11:48.000
상업적으로, 그리고 기업적으로 매우 커졌기 때문에,

00:11:48.000 --> 00:11:52.880
에이전트 중 한 명을 모델 제공자로 임대하거나 판매하거나,

00:11:50.399 --> 00:11:55.600
다른 대기업에 주요 모델 제공자를 맡긴다고 생각해 보세요.

00:11:52.880 --> 00:11:57.680
그런 기업들은

00:11:55.600 --> 00:12:00.160
에이전트의 행동,

00:11:57.680 --> 00:12:02.959
데이터 활용,

00:12:00.160 --> 00:12:04.000
고객 활용에 대한 보장을 원할 겁니다.  그리고 만약 그런 일들이

00:12:02.959 --> 00:12:06.000
잘못되면, 그것은

00:12:04.000 --> 00:12:08.240
어떤 면에서도 실존적인 문제가 되지 않을 것이고, 당신은

00:12:06.000 --> 00:12:10.160
확실히 사업을 잃게 될 것입니다.  그러면 왜

00:12:08.240 --> 00:12:11.440
그 사업체는

00:12:10.160 --> 00:12:12.560
그 공급업체와 계약을 맺었을까요?  그들은

00:12:11.440 --> 00:12:14.560
더

00:12:12.560 --> 00:12:16.800
책임감 있고 더 나은 보장을 제공하는 다른 공급업체를 선택할 것입니다.

00:12:14.560 --> 00:12:18.240
그래서 제 생각에 좋은 점은

00:12:18.240 --> 00:12:24.240
자본주의가 자연스럽게 이상적으로는

00:12:21.600 --> 00:12:27.839
더 책임감 있는 행위자에게 보상을 줄 것이라는 점입니다.

00:12:24.240 --> 00:12:29.680
하지만 AI가 잘못하면 잠재적으로 해자를 뛰어넘거나

00:12:27.839 --> 00:12:31.360
가드레일을 뛰어넘을 가능성이 있습니다

00:12:29.680 --> 00:12:33.600
.  제 말은,

00:12:31.360 --> 00:12:36.240
가능성은 항상 있었다는 거예요.  우리는

00:12:33.600 --> 00:12:37.680
아무도 그것이 무엇인지 정확히 모릅니다. 그게 가장

00:12:36.240 --> 00:12:40.560
큰 미지수 중 하나입니다.  저는

00:12:37.680 --> 00:12:42.720
그 잠재력이 0이 아니라고 생각합니다.  음,

00:12:40.560 --> 00:12:45.440
매우 진지하게 고려하고 완화할 가치가 있지만,

00:12:45.440 --> 00:12:48.720
사람들이 이러한 재앙이 발생할 확률에

00:12:47.200 --> 00:12:50.399
대해 매우 정확한 백분율을 제시하는 것을 들은 적이 있는데,

00:12:50.399 --> 00:12:54.079
제 생각에는

00:12:52.160 --> 00:12:55.519
아무도 그것이 무엇인지 모르기 때문에 말도 안 되는 소리라고 생각합니다

00:12:54.079 --> 00:12:57.279
.  제가 아는 건

00:12:55.519 --> 00:12:58.399
정량화하지 않고

00:12:58.399 --> 00:13:04.480
0이 아니라고 말하는 거예요.  따라서 PDM이

00:13:01.839 --> 00:13:06.639
0이 아니라면

00:13:04.480 --> 00:13:07.920
상당한 리소스와

00:13:06.639 --> 00:13:10.639
주의를 기울여야 한다는 것을 알 수 있습니다.

00:13:07.920 --> 00:13:13.440
미국은 중국과의 AI 경쟁에서 어떤 면에서 앞서고 있고,

00:13:10.639 --> 00:13:16.079
어떤 면에서는 뒤처지고 있을까?

00:13:13.440 --> 00:13:19.760
음, 저는 우리가 여전히

00:13:16.079 --> 00:13:21.519
미국과 서방에서 앞서 있다고 생각합니다.

00:13:19.760 --> 00:13:24.720
최신

00:13:21.519 --> 00:13:26.480
벤치마크와 최신 시스템을 살펴보면 그렇지

00:13:24.720 --> 00:13:27.600
않지만 중국도 크게 뒤처지지 않았습니다

00:13:26.480 --> 00:13:28.959
.  최신

00:13:27.600 --> 00:13:30.639
DeepSseek이나 최신 소규모 팀을 살펴보면

00:13:28.959 --> 00:13:32.800
매우 훌륭하고

00:13:30.639 --> 00:13:34.800
매우 유능한 팀이 있습니다.  그러니 어쩌면

00:13:32.800 --> 00:13:37.279
우리는, 아시다시피, 선두가 이

00:13:37.279 --> 00:13:40.160
시점에서 몇 년이 아니라 몇 달 안에 결정될지도 모릅니다.

00:13:37.760 --> 00:13:42.399
칩과 AI를 제쳐두고 보면

00:13:40.160 --> 00:13:45.040
중국이 이길 가능성이 큽니다.

00:13:42.399 --> 00:13:47.040
음, 아니요. 저는 칩은 하나의 분야라고 생각하지만,

00:13:45.040 --> 00:13:48.639
알고리즘과 혁신

00:13:47.040 --> 00:13:51.200
측면에서는 서구가 여전히 앞서 있다고 생각합니다

00:13:48.639 --> 00:13:54.639
.  그래서 저는

00:13:51.200 --> 00:13:56.880
중국 모델이나 회사 중 어떤 것도 알고리즘 측면에서 최첨단을 넘어서는 새로운

00:13:54.639 --> 00:14:00.480
혁신을 보여줄 수 있다는 것을 보여주지 못했다고 생각합니다.

00:14:00.480 --> 00:14:06.399
그들은 최신

00:14:08.639 --> 00:14:11.360
기술을 빠르게 따라가는 데 매우 능숙했습니다.

00:14:09.519 --> 00:14:13.040
마지막 질문은 줌아웃입니다.

00:14:11.360 --> 00:14:15.839
이 질문이 마음에 드실 겁니다.

00:14:13.040 --> 00:14:18.560
AI에 대해 놀라울 정도로

00:14:15.839 --> 00:14:20.399
적은 관심을 받는 가장 놀라운 점은 무엇이라고 생각하십니까?

00:14:18.560 --> 00:14:22.079
AI에 관해 가장 놀라운 점은

00:14:20.399 --> 00:14:23.680
놀라울 정도로 거의 관심을 받지 못한다는 것입니다.

00:14:22.079 --> 00:14:25.360
우와.  응.

00:14:23.680 --> 00:14:28.560
우리가 작업하고 이미

00:14:25.360 --> 00:14:30.959
효과가 있는 것들을 생각해보면,

00:14:28.560 --> 00:14:31.279
이 모델들이 가지고 있는 다중 모드적 이해가 중요하다고 생각합니다.

00:14:31.279 --> 00:14:35.760
멀티모달 비디오, 즉

00:14:33.199 --> 00:14:37.760
비디오, 이미지, 그리고

00:14:35.760 --> 00:14:40.480
오디오를 말하는 것 같지만, 저는

00:14:37.760 --> 00:14:43.279
실제로 비디오를 구체적으로 생각하고 있습니다.  그러니까

00:14:40.480 --> 00:14:44.880
제미니에게 유튜브 영상을 주고 처리하게 하면,

00:14:44.880 --> 00:14:48.880
영상에 대해 온갖 놀라운 것들을 물어볼 수 있어요. 제미니가

00:14:48.880 --> 00:14:53.279
많은 경우 개념적으로 이해할 수 있다는 게 정말 놀랍죠.

00:14:51.120 --> 00:14:54.560
항상 그런 건 아니지만,

00:14:53.279 --> 00:14:55.839
정말 인상적인 경우가 많아요

00:14:54.560 --> 00:14:57.680
.

00:14:55.839 --> 00:15:00.399
질문의 예를 이해할 수 있습니다.  음, 글쎄요, 저는 이런

00:14:57.680 --> 00:15:01.839
질문을 했습니다. 음, 아시다시피,

00:15:00.399 --> 00:15:03.279
이건 제가 얼마 전

00:15:01.839 --> 00:15:06.320
Gemini를 테스트했던 것인데,

00:15:03.279 --> 00:15:09.040
음, 저는 Fight Club이라는 영화를 좋아하는데,

00:15:06.320 --> 00:15:10.800
음, 그 영화에

00:15:09.040 --> 00:15:12.320
Brad Pitt나

00:15:10.800 --> 00:15:16.480
Ed Norton이

00:15:12.320 --> 00:15:19.120
싸움을 하기 전에 반지를 벗는 장면이 나오는데, 음, 그런

00:15:16.480 --> 00:15:21.199
종류의 음, 제가

00:15:19.120 --> 00:15:23.360
Gemini에게 그 [콧김을

00:15:21.199 --> 00:15:24.720
내뿜으며] 행동의 의미가 무엇인지 물었고,

00:15:23.360 --> 00:15:26.079
그는

00:15:26.079 --> 00:15:30.079
일상 생활을 뒤로하고 떠나는 것에 대한 매우 흥미로운 철학적 관점을 제시했고,

00:15:30.079 --> 00:15:35.120
음, 이 시스템이 현재 가지고 있는 매우

00:15:33.120 --> 00:15:37.440
흥미로운 메타 통찰력을 상징적으로 보여주는 것이었습니다.

00:15:35.120 --> 00:15:38.720
그리고

00:15:37.440 --> 00:15:39.920
제 생각에 그것을 사용한다면, 잘 알려지지 않은 또 다른 것은

00:15:39.920 --> 00:15:43.120
Gemini Live라는 것이 있는데,

00:15:41.279 --> 00:15:45.760
휴대전화를

00:15:43.120 --> 00:15:47.680
무언가에 대고 자신이 정비사라고 말하면

00:15:45.760 --> 00:15:49.519
실제로

00:15:47.680 --> 00:15:51.360
앞에 있는 어떤 작업에든 도움이 될 수 있습니다.

00:15:49.519 --> 00:15:52.800
이상적으로는 안경이어야 하니까요.

00:15:52.800 --> 00:15:58.240
사실, 손을 자유롭게 사용할 수 있어야 하지만 사람들은

00:15:58.240 --> 00:16:01.839
아직 다중 모달리티 기능이 얼마나 강력한지 깨닫지 못하는 것 같아요.

00:16:00.399 --> 00:16:04.000
좋아요, 확대/축소로의

00:16:01.839 --> 00:16:06.720
전환에 완벽한 다리를 제공해 주셨습니다.

00:16:04.000 --> 00:16:09.600
지난달 쌍둥이자리 3번째 생일을 축하드립니다.

00:16:06.720 --> 00:16:12.959
어, 당신의 획기적인 모델은

00:16:09.600 --> 00:16:14.880
전례 없는 깊이와

00:16:12.959 --> 00:16:17.600
미묘함으로 추론된다고 말씀하셨습니다.

00:16:14.880 --> 00:16:21.120
제미니 3의 뉘앙스 부분에서 독특한 점은 무엇인지 알려주세요. 네, 그냥 음, 음,

00:16:17.600 --> 00:16:23.040
저희는

00:16:23.040 --> 00:16:27.360
그 개성, 스타일,

00:16:24.880 --> 00:16:29.920
성능 모두에 정말 만족합니다.  저는 그것이

00:16:27.360 --> 00:16:31.519
간결하게 대답하는 방식을 좋아합니다.

00:16:29.920 --> 00:16:33.279
당신이 말하는 내용

00:16:31.519 --> 00:16:35.440
에 전적으로 동의하지 않는다면, 그것은 약간 반발을 불러일으킵니다

00:16:33.279 --> 00:16:36.880
.  그것은 어떤

00:16:35.440 --> 00:16:38.800
아이디어가

00:16:36.880 --> 00:16:41.199
말이 안 된다면 온건하게 거부합니다.  그리고 사람들이 그것을

00:16:38.800 --> 00:16:43.279
높이 평가하고 있다고 생각합니다. 일종의

00:16:45.279 --> 00:16:49.519
지능과 유용성 측면에서 단계적 변화가 있었다고 느낄 수 있습니다.

00:16:47.600 --> 00:16:52.000
그리고 쌍둥이자리가

00:16:49.519 --> 00:16:54.240
대답하거나 만들어낸 것 중에, 제가 그것이 그런 일을 할 수 있다는 걸

00:16:52.000 --> 00:16:56.480
몰랐다거나 그것이 그런 일을 할 줄 몰랐다고 말한 것이 있나요?

00:16:56.480 --> 00:17:00.240
사실 이게 놀라운

00:16:58.320 --> 00:17:02.880
점인데,

00:17:00.240 --> 00:17:04.880
우리가 하는 일을 왜 그렇게 좋아하는지 그 이유는

00:17:02.880 --> 00:17:06.880
우리가 지금 제품과 관련된 연구에 뛰어들었기 때문입니다

00:17:04.880 --> 00:17:10.000
.  가장 좋은 점은

00:17:06.880 --> 00:17:12.079
수백만 명, 잠재적으로

00:17:10.000 --> 00:17:14.319
Google에서 수십억 명의 사용자가

00:17:12.079 --> 00:17:16.640
여러분이 내놓은 새로운 기술을 즉시 활용할 수 있다는 것입니다

00:17:14.319 --> 00:17:19.120
.  그리고

00:17:16.640 --> 00:17:21.600
사람들이

00:17:19.120 --> 00:17:24.000
이 모델을 사용해서 매우 빠르게 멋진 것들을 찾아내는 모습에 우리는 계속 놀랍니다

00:17:21.600 --> 00:17:25.679
.  음, 그런 것들 중 많은 것이

00:17:24.000 --> 00:17:27.760
바이러스처럼 퍼지는 경향이 있죠.  하지만

00:17:25.679 --> 00:17:30.960
제가 Gemini 3에서 가장 즐겼던 것은

00:17:27.760 --> 00:17:32.480
단발성 게임이었습니다.  그래서 게임용

00:17:30.960 --> 00:17:34.160
AI를 만드는 저의 첫 번째 경력으로 돌아가서

00:17:32.480 --> 00:17:35.520
, 저는 우리가 이제 이 모델들과 매우 가까워졌다고 생각합니다

00:17:34.160 --> 00:17:37.200
.  다음 버전에서는

00:17:37.200 --> 00:17:43.039
상업용 게임을 실제로 만들 수 있는 모델이 나올지도 몰라요.

00:17:43.039 --> 00:17:45.840
몇 시간 만에 분위기 코딩을 해서 만들 수 있을 거예요. 예전에는 몇 년이 걸렸는데,

00:17:45.840 --> 00:17:50.480
그걸 보면 미묘한 차이를 알 수 있죠.  이는

00:17:48.080 --> 00:17:53.760
모델에 대해 무엇을 보여줍니까?  글쎄요, 저는

00:17:53.760 --> 00:17:59.919
이 모델이

00:17:57.200 --> 00:18:02.320
매우 높은 수준의 지침을 이해하고

00:17:59.919 --> 00:18:05.120
매우 자세한 출력을 생성하는 능력과 그 깊이가 정말 놀랍다고 생각합니다.

00:18:05.120 --> 00:18:08.799
특히 Gemini 3가 잘하는 것은 프런트엔드

00:18:06.799 --> 00:18:11.200
작업과 웹사이트 개발인데,

00:18:08.799 --> 00:18:13.760
미학적으로나

00:18:11.200 --> 00:18:15.520
창의적으로나

00:18:13.760 --> 00:18:18.160
기술적으로도 꽤 훌륭합니다.  Axios에서 우리가 상당히 많이 언급한 내용 중 하나는

00:18:18.160 --> 00:18:22.640
이러한 모델의 작성자, 제작자조차도

00:18:20.480 --> 00:18:24.400
이를 완전히 이해하지 못한다는 것입니다.

00:18:22.640 --> 00:18:26.320
제미니 3에는 무엇이 있나요?  응.

00:18:24.400 --> 00:18:26.880
당신은 완전히 이해하지 못하는 것 같은 느낌이 드나요

00:18:26.320 --> 00:18:28.799
?

00:18:26.880 --> 00:18:31.919
글쎄요, 사실 저는

00:18:28.799 --> 00:18:33.360
이 모든 모델과 더불어

00:18:31.919 --> 00:18:36.160
청중 모두가 느낄 수 있는 점은

00:18:36.160 --> 00:18:41.520
혁신과 개선의 속도가 매우 빠르다는 것입니다.  음, 우리는

00:18:41.520 --> 00:18:45.039
이런 것들을 만드는 데 거의 모든 시간을 쓰고 있어요.  우리는 아직 가지고 있지도 않아요.

00:18:45.039 --> 00:18:49.200
새로운 버전을 출시할 때마다 기존 시스템이 할 수 있는 것의

00:18:46.880 --> 00:18:50.720
10분

00:18:49.200 --> 00:18:52.559
의 1도 탐색하지 못한 것 같은 느낌이 들어요.

00:18:52.559 --> 00:18:56.080
물론 우리는 즉시

00:18:54.559 --> 00:18:58.080
치열한

00:18:56.080 --> 00:18:59.600
경쟁과 경쟁을 언급하고 있기 때문에

00:18:58.080 --> 00:19:01.760
다음 혁신에 집중하고 있고,

00:18:59.600 --> 00:19:03.760
당연히

00:19:01.760 --> 00:19:07.600
안전하고 신뢰할 수 있도록 하고 있어요

00:19:03.760 --> 00:19:10.320
.  그래서 다시 한번 말씀드리자면, 우리 사용자들은 우리가

00:19:10.320 --> 00:19:15.200
내부적으로 시도했던 것보다 훨씬 더 멀리 나아가게 됩니다.

00:19:12.240 --> 00:19:18.960
그리고 Gemini 3에 대한 질문이 하나 더 있습니다.

00:19:15.200 --> 00:19:22.240
짧은 배경 스토리와 여러 가지를 다루셨는데

00:19:18.960 --> 00:19:26.080
, LLM은

00:19:22.240 --> 00:19:28.799
텍스트 기반의 대규모 언어 모델입니다.  어, 꼭 그걸

00:19:28.799 --> 00:19:33.039
성배로 생각한 건 아니죠.

00:19:30.640 --> 00:19:35.360
위대한 작가이자 사상가

00:19:33.039 --> 00:19:38.640
이며 당신의 친구인 월터 아이작슨이 제게 한 말은

00:19:35.360 --> 00:19:42.000
LLM의 힘을 보았을 때,

00:19:38.640 --> 00:19:44.799
월터가 말했듯이 전환점, 퓨레트를 만들어서

00:19:42.000 --> 00:19:47.440
큰

00:19:44.799 --> 00:19:49.440
성공을 거둘 수 있었다는 것입니다.  그리고 월터의 요점은

00:19:47.440 --> 00:19:51.360
대부분의 사업가들이

00:19:49.440 --> 00:19:53.840
고집을 부렸을 것이고, 다른 베팅에서는 두 배, 세 배로 투자했을 것이라는 것입니다

00:19:51.360 --> 00:19:56.559
.  어떻게 LLM

00:19:53.840 --> 00:19:57.200
에 전념하기로 결정하셨나요

00:19:56.559 --> 00:19:59.120
?

00:19:57.200 --> 00:20:00.480
글쎄요, 저는 이것이

00:20:00.480 --> 00:20:04.640
과학적 방법의 아름다움이자 강점이라고 생각합니다.  당신이 진정한

00:20:01.919 --> 00:20:07.280
과학자라면,

00:20:04.640 --> 00:20:09.039
자신이 가진 어떤 아이디어에 대해 지나치게 교조적이 되어서는 안 됩니다.  당신은

00:20:07.280 --> 00:20:11.360
경험적 증거가

00:20:09.039 --> 00:20:12.880
당신을 어디로 데려가는지 따라가야 합니다.  그럼, 우선,

00:20:11.360 --> 00:20:16.320
월터는 아마도

00:20:12.880 --> 00:20:18.160
2017년, 2018년 시대를 언급하고 있을 겁니다.  그래서 우리는

00:20:16.320 --> 00:20:19.919
많은 문제를 안고 있었습니다.  앞서

00:20:18.160 --> 00:20:21.600
말했듯이, 우리는 매우 유능한

00:20:19.919 --> 00:20:23.360
언어 모델을 가지고 있었습니다.  그들은

00:20:21.600 --> 00:20:24.720
친칠라라고 불렸고, 그다음에는 스패로우라고 불렸으며, 우리는

00:20:24.720 --> 00:20:28.159
그들에게 다양한 코드명을 붙였습니다.  음, 공개적으로 발표된 건 아니지만

00:20:26.480 --> 00:20:29.600
내부적으로는 공개됐어요.  사실, 일부

00:20:28.159 --> 00:20:30.880
스케일링 법칙은 원래

00:20:29.600 --> 00:20:33.679
우리 팀에서 알아냈습니다.  이것을

00:20:30.880 --> 00:20:35.760
친칠라 스케일링 법칙이라고 부릅니다.  하지만 알파 고의 순수 강화학습 시스템을 기반으로 하는

00:20:33.679 --> 00:20:37.120
알파 제로와 같은 다른 유형의 프로그램도 있었고,

00:20:39.200 --> 00:20:42.720
인지 과학에서 더 나아가 신경 과학에서

00:20:40.799 --> 00:20:46.080
영감을 받은 아키텍처도 있었습니다.  그리고

00:20:42.720 --> 00:20:49.280
당시 우리가 확신하지 못했던 건, 제 임무가

00:20:46.080 --> 00:20:50.960
AGI를 먼저 빠르고

00:20:49.280 --> 00:20:53.200
안전하게 만드는 거라는 거였죠?  그것이 바로 DeepMind의

00:20:50.960 --> 00:20:55.919
사명이자, 항상 우리의 해결 지능이었습니다

00:20:53.200 --> 00:20:58.320
.  그래서 저는

00:20:55.919 --> 00:21:00.159
실제로 그러한 접근 방식에 대해 불가지론적입니다

00:20:58.320 --> 00:21:01.679
.  저는 그 점에 있어서는 꽤 실용적입니다

00:21:00.159 --> 00:21:04.080
.  제 엔지니어적 면은 아마도

00:21:04.080 --> 00:21:06.960
훌륭한 과학자가 가져야 할 이론을 가지고 있다는 점일 겁니다. 하지만

00:21:06.080 --> 00:21:09.360
결국에는

00:21:06.960 --> 00:21:11.360
실용적으로 작동해야 합니다.  그래서 우리가

00:21:09.360 --> 00:21:13.760
확장이 효과를 발휘하기 시작하는 것을 보았을 때,

00:21:11.360 --> 00:21:16.000
우리는 연구 트리의 해당 분기에 점점 더 많은 리소스를 투입했습니다

00:21:16.000 --> 00:21:19.600
.

00:21:19.600 --> 00:21:24.559
인공 일반

00:21:21.280 --> 00:21:26.480
지능, 즉 인간이 할 수 있는 AI에 대한 접근 방식이 신선하다는 점입니다.  당신은

00:21:24.559 --> 00:21:28.320
그것을 부끄러워하지 않습니다.  다른

00:21:26.480 --> 00:21:30.000
사람들은 "글쎄요, 우리는 알 수 없을 거예요. 아니면

00:21:28.320 --> 00:21:32.400
우리는 이미 거기까지 왔거나 상관없어요

00:21:30.000 --> 00:21:34.960
."라고 말합니다.  당신은 그것이 중요하다고 말하고

00:21:32.400 --> 00:21:35.360
우리는 그것을 알게 될 것입니다.  그리고 당신은 그것이 크게 틀리지 않았다고 말합니다

00:21:34.960 --> 00:21:37.679
.

00:21:35.360 --> 00:21:39.280
네, 우리는 지금 그 수준에는 확실히 도달하지 못했습니다.

00:21:37.679 --> 00:21:40.559
그러니까, 그리고 그리고 나 그리고

00:21:39.280 --> 00:21:41.840
실제로는 꽤 가깝다고 말씀하시는 거죠.

00:21:40.559 --> 00:21:43.600
네, 아주 가깝습니다.  제 생각엔

00:21:41.840 --> 00:21:45.440
5년에서 10년 정도 걸릴 것 같아요

00:21:43.600 --> 00:21:48.400
.  죄송합니다.  다시 한 번 말씀드려 볼까요?

00:21:45.440 --> 00:21:49.919
5년에서 10년 후.  하지만 제 기준은

00:21:48.400 --> 00:21:52.240
꽤 높은 것 같아요.  그러니까,

00:21:49.919 --> 00:21:54.000
AGI를 우리가 정의하는 건,

00:21:54.000 --> 00:21:58.480
우리가 가진 모든 인지 능력을 보여주는 시스템으로, 여기에는

00:21:56.080 --> 00:22:00.640
창의적이고 발명적인

00:21:58.480 --> 00:22:02.159
능력도 포함된다는 겁니다.

00:22:00.640 --> 00:22:04.240
여러분 모두가 현재 LLM을 사용했기 때문에 빠진 부분이 있다고 생각합니다.

00:22:04.240 --> 00:22:06.880
어떤 면에서는 놀랍습니다.

00:22:05.520 --> 00:22:09.200
어떤 면에서는 정말 인상적입니다. 어떤 면에서는 믿을 수

00:22:06.880 --> 00:22:12.159
없을 정도로 거의 박사 수준이며,

00:22:09.200 --> 00:22:13.919
어떤 분야에서는 IMO 금메달 등을 딴 핵심 기술을 보유하고 있지만,

00:22:12.159 --> 00:22:15.679
다른 분야에서는

00:22:13.919 --> 00:22:17.919
여전히 많은 결함이 있습니다. 그래서 이들은

00:22:15.679 --> 00:22:20.000
일종의 삐죽삐죽한 지능을 가지고 있어서

00:22:20.000 --> 00:22:23.760
진정한 AGI에서 기대할 수 있는 전반적인 일관성을 가지고 있으며,

00:22:23.760 --> 00:22:27.840
지속적인 온라인 학습,

00:22:25.600 --> 00:22:29.840
장기 계획 및 추론과 같은 다른 기능이 부족합니다.

00:22:27.840 --> 00:22:31.760
현재로서는 이러한 일을 할 수 없다고

00:22:29.840 --> 00:22:32.880
생각합니다. 하지만 아마도 한두 가지 획기적인

00:22:31.760 --> 00:22:33.600
발전이 더

00:22:32.880 --> 00:22:35.280
필요할 것입니다.

00:22:35.280 --> 00:22:40.240
오늘 우리가 본 훌륭한 Ena Frereded에게 질문하고 싶습니다. 그녀는

00:22:37.440 --> 00:22:43.919
Axios가 시작된 첫날부터

00:22:40.240 --> 00:22:46.080
Axios를 현재의 모습으로 만드는 데 도움을 준 사람입니다.  어, 그녀는

00:22:43.919 --> 00:22:50.880
당신이 분명히

00:22:46.080 --> 00:22:53.600
AI가

00:22:50.880 --> 00:22:54.159
AGI보다 한 단계, 두 단계 더 발전할 수 있다고 말했다고 하더군요.

00:22:53.600 --> 00:22:57.679
예.

00:22:54.159 --> 00:22:59.679
LLM

00:22:57.679 --> 00:23:01.360
과 생성적 AI를 개선하는 것만으로 목표에 도달할 수 있을까요? 아니면

00:23:01.360 --> 00:23:05.440
5~10년 안에 GI를 달성하려면 다른 접근 방식이 필요할 것이라고 생각하시나요

00:23:03.760 --> 00:23:07.919
?  다시 한번 말씀드리지만, 이건

00:23:05.440 --> 00:23:10.159
경험적 질문이지만 제가 아는 것은

00:23:10.159 --> 00:23:15.520
현재 시스템의 확장을

00:23:13.039 --> 00:23:17.760
최대한으로 밀어붙여야 한다는 것입니다.

00:23:15.520 --> 00:23:20.400
최소한

00:23:17.760 --> 00:23:22.960
최종 AGI 시스템의 핵심 구성 요소가 될 것이기 때문입니다.

00:23:20.400 --> 00:23:24.640
AGI 시스템 전체가 될 수도 있습니다.

00:23:22.960 --> 00:23:26.720
확장만으로는 목표에 도달할 가능성이 있지만,

00:23:24.640 --> 00:23:28.080
추측컨대

00:23:28.080 --> 00:23:32.000
지금 제 관점에서 볼 때 한두 가지 큰 획기적인

00:23:30.080 --> 00:23:33.440
발전이 더 있을 것입니다. 제가 말하는

00:23:32.000 --> 00:23:35.760
혁신은 항상 일어나고 있습니다.

00:23:35.760 --> 00:23:40.159
기존 기술을 확장하는 것까지 포함해서 말입니다. 제가 말하는 획기적인 발전은

00:23:37.520 --> 00:23:42.080
변압기 수준이나 알파 수준과 같은 것입니다

00:23:40.159 --> 00:23:45.039
.

00:23:42.080 --> 00:23:46.720
AGI가 완성된 후 돌이켜보면 확장성

00:23:46.720 --> 00:23:49.200
외에도 한두 가지가 더 필요했을 것이라고 추측할 수 있을 것 같습니다

00:23:48.720 --> 00:23:50.799
.   이제

00:23:49.200 --> 00:23:53.200
우리는 낚싯바늘을 던질 차례입니다.  정말

00:23:50.799 --> 00:23:56.400
빠른 라운드였습니다.  에나에게서 또 다른 질문이 왔습니다

00:23:53.200 --> 00:23:58.480
.  당신은 분명 AI를 믿는 사람이겠지만

00:23:56.400 --> 00:24:00.559
, AI에 쓰이는 돈을 보면

00:24:02.159 --> 00:24:05.120
경제를 뒤흔들 만큼 큰 거품이 없다는 뜻은 아닙니다.  당신은 그것에 대해 얼마나 걱정하고 있나요

00:24:04.320 --> 00:24:08.000
?

00:24:05.120 --> 00:24:09.919
음, 저는 그것이

00:24:08.000 --> 00:24:12.080
이진법이 아니라고 생각합니다.  AI 산업의 일부는 아마도

00:24:09.919 --> 00:24:13.360
거품 속에 있는 것 같아요. 예를 들어,

00:24:13.360 --> 00:24:17.679
500억 달러 규모의

00:24:15.919 --> 00:24:20.080
시드 라운드 같은 건 지속

00:24:17.679 --> 00:24:22.080
불가능해 보이거든요.  하지만

00:24:20.080 --> 00:24:24.000
반면에 저는

00:24:22.080 --> 00:24:26.799
누구보다도 AI가

00:24:24.000 --> 00:24:28.960
역사상 가장 혁신적인 기술이라고 믿습니다.  그래서 저는

00:24:26.799 --> 00:24:31.360
시간이 흐르면 ​​이

00:24:28.960 --> 00:24:34.000
모든 것이 정당화될 것이라고 생각합니다.

00:24:31.360 --> 00:24:36.159
그리고 Google Deep Mine의 책임자이자

00:24:34.000 --> 00:24:39.279
Google의 엔진룸으로서 제 임무는

00:24:36.159 --> 00:24:41.600
어느 쪽이든 승리하도록 하는 것입니다.  만약

00:24:39.279 --> 00:24:43.120
거품이 터지거나

00:24:43.120 --> 00:24:46.640
지금처럼 상황이 계속 좋아진다면 우리는 강력한 입장에 있을 것입니다.

00:24:44.480 --> 00:24:49.039
AI 채용 전쟁,

00:24:46.640 --> 00:24:50.480
인재 확보를 위한 이 경쟁의 최종 결과는 어떻게 될까요?

00:24:49.039 --> 00:24:52.000
글쎄요, 최근에 정말 미친 짓이 벌어졌어요

00:24:50.480 --> 00:24:53.520
.  메타가 해온 일과 같은 것들이 있고

00:24:52.000 --> 00:24:55.600
, 아시다시피, 모든 사람은

00:24:53.520 --> 00:24:58.559
자신에게 맞는 일을 해야 합니다.

00:24:55.600 --> 00:25:00.799
음, 우리가 찾아낸 건

00:24:58.559 --> 00:25:02.640
사명감을 가진 사람을 원한다는 거예요.  우리는

00:25:00.799 --> 00:25:04.720
최고의 사명을 가지고 있다고 생각합니다.  우리는

00:25:02.640 --> 00:25:06.480
풀스택을 가지고 있습니다.  그래서, 제 생각에

00:25:04.720 --> 00:25:08.400
가장 영향력 있는 일을 하고

00:25:06.480 --> 00:25:10.320
세상에 가장 긍정적인 영향을 미치고 싶다면,

00:25:10.320 --> 00:25:14.720
구글 딥마인드보다 더 나은 곳은 없다고 생각합니다.  그리고 결국,

00:25:13.120 --> 00:25:16.320
저는 최고의 과학자, 최고의

00:25:14.720 --> 00:25:17.919
연구자, 최고의 엔지니어는

00:25:16.320 --> 00:25:19.840
최첨단 기술을 연구하고 싶어한다고 생각합니다

00:25:17.919 --> 00:25:21.840
.  그러니까 만약 여러분이

00:25:19.840 --> 00:25:25.120
최고의 시스템을 갖춘 리더보드 상위권에 있다면, 그것은

00:25:21.840 --> 00:25:26.400
일종의 자기충전이죠.

00:25:25.120 --> 00:25:28.320
이 질문은 노스캐롤라이나주 하이포인트 대학의

00:25:26.400 --> 00:25:30.320
젊은 기업가, 제임스 반더하이가 한 질문입니다

00:25:30.320 --> 00:25:35.120
.  그는 "

00:25:32.000 --> 00:25:37.919
AI가 스스로의 사고방식을 갖게 된다는 이야기가 많이 나오고 있습니다

00:25:35.120 --> 00:25:40.960
. AI가 자기

00:25:37.919 --> 00:25:42.320
이익을 위해 행동할 수 있는 시나리오가 있을까요?"라고 말했습니다.

00:25:40.960 --> 00:25:44.480
글쎄요, 좋은 질문이네요. 그리고

00:25:44.480 --> 00:25:49.120
더 비극적인 결과가 초래될 수 있는 것과 관련이 있는데,

00:25:47.200 --> 00:25:50.880
만약 그게 잘못되면,

00:25:49.120 --> 00:25:52.880
에이전트 기반

00:25:50.880 --> 00:25:55.039
시스템이나 매우 자율적인 시스템에서 어떤 면에서

00:25:57.600 --> 00:26:03.200
설계자나

00:26:00.320 --> 00:26:05.919
인간이 원하는 것과 상충되는 자기 이익을 개발하게 될 경우 문제가 될 것입니다.   마지막으로

00:26:03.200 --> 00:26:08.880
재밌는 걸로 마무리하겠습니다. 당신은

00:26:05.919 --> 00:26:11.279
여전히 ​​게이머입니다.  게임은 우리에게

00:26:08.880 --> 00:26:13.120
세상에 대해 무엇을 알려주는가? 그리고 게임은

00:26:11.279 --> 00:26:13.760
이러한 기계들이

00:26:13.120 --> 00:26:16.159
향하는 방향에 대해 무엇을 알려주는가?

00:26:13.760 --> 00:26:18.159
음, 저는 체스

00:26:16.159 --> 00:26:20.080
배경과 체스 훈련, 그리고 그

00:26:18.159 --> 00:26:22.400
후의 다른 게임들이

00:26:20.080 --> 00:26:24.799
제가

00:26:22.400 --> 00:26:26.400
사업과 과학 분야에서 일하는 방식에 매우 중요하다고 생각합니다.  음,

00:26:24.799 --> 00:26:27.600
제가 게임을 좋아하는 이유는

00:26:26.400 --> 00:26:29.760
여러 가지가 있지만

00:26:27.600 --> 00:26:31.520
, 게임을

00:26:29.760 --> 00:26:33.039
만드는 과정에서 창의성을 발휘하기 때문입니다.  하지만 저는 그저 게임을 하는 것이

00:26:33.039 --> 00:26:38.080
정신을 단련하는 가장 좋은 방법이라고 생각합니다. 체스든, 바둑이든, 포커든 최고의 게임은

00:26:39.919 --> 00:26:44.400
현실 세계의 축소판과 같으니까요. 하지만 일반적으로 현실

00:26:42.320 --> 00:26:46.640
세계에서는

00:26:46.640 --> 00:26:50.640
그 순간에 올바른 결정을 내리는 연습을 여러 번 할 수 없습니다.

00:26:48.720 --> 00:26:52.320
현실 생활에서는

00:26:50.640 --> 00:26:55.039
그런 중요한 순간을 열두 번 정도밖에 경험하지 못할지 몰라도, 게임을 통해 원하는

00:26:52.320 --> 00:26:58.000
만큼 의사결정 능력을 연습할 수 있습니다. 게임을 통해

00:27:00.000 --> 00:27:04.000
세상을 시뮬레이션하는 것과 마찬가지입니다.  음, 그리고

00:27:02.159 --> 00:27:05.440
게임을 매우 진지하게 받아들이고,

00:27:05.440 --> 00:27:09.039
결정을 내릴 때 많은 생각을 쏟는다면, 제 생각에는

00:27:07.679 --> 00:27:10.880
결정 능력과 계획

00:27:09.039 --> 00:27:13.279
능력을 실제로 훈련시키는 데 도움이 됩니다.  이제, 당신은

00:27:10.880 --> 00:27:16.640
우리의 연약한 뇌가

00:27:13.279 --> 00:27:18.400
사냥꾼 채집인으로 진화했지만,

00:27:18.400 --> 00:27:22.240
당신이 가디언에 말한 대로 우리는

00:27:20.159 --> 00:27:25.039
산업 혁명보다 10배 더 크고

00:27:22.240 --> 00:27:27.360
10배 더 빠를지도 모르는 파괴에 직면해 있다고 지적했습니다.

00:27:25.039 --> 00:27:30.720
우리는 대부분

00:27:27.360 --> 00:27:32.720
인간이 따라잡을 수 없는 상황에 직면해 있고,

00:27:30.720 --> 00:27:34.880
당신을 포함한 그 누구도 따라잡을 수 없는 상황에 직면해 있는 걸까요?

00:27:32.720 --> 00:27:37.360
글쎄요, 좋은 소식은, 제가

00:27:34.880 --> 00:27:39.919
사냥에서 말하고자 하는 요점은,

00:27:37.360 --> 00:27:42.320
우리의 뇌가 얼마나 적응력이 강한지를 살펴보는 것입니다.  우리는

00:27:39.919 --> 00:27:44.480
사냥꾼이자 채집꾼으로 진화했지만,

00:27:42.320 --> 00:27:46.159
우리는 현대

00:27:44.480 --> 00:27:50.400
도시에 앉아 있고, 주변에 기술이 넘쳐나는 현대 문명에 살고 있습니다. 아시다시피

00:27:50.400 --> 00:27:54.960
인간의 뇌는 그런 것에

00:27:52.320 --> 00:27:58.399
적응할 수 있었던 거죠.  그래서 저는

00:27:54.960 --> 00:28:00.320
인간의 독창성을 정말 믿고, 인간은

00:27:58.399 --> 00:28:02.320
무한히

00:28:00.320 --> 00:28:03.760
적응력이 있다고 생각합니다.  우리는 일반 지능의 존재를

00:28:02.320 --> 00:28:05.919
증명하는 유일한 존재이며, 우리의 뇌는

00:28:03.760 --> 00:28:07.760
아마도 우리가 지금까지 알고 있는 우주에서 일반 지능의 존재를 증명하는 유일한 존재일 것입니다

00:28:07.760 --> 00:28:12.080
.  그러므로 우리 자신도 일반

00:28:09.679 --> 00:28:14.000
지성을 가지고 있으므로

00:28:12.080 --> 00:28:17.039
무한히 적응할 수 있어야 합니다.

00:28:14.000 --> 00:28:18.640
AGI 이후의 AGI가 어떤

00:28:17.039 --> 00:28:20.720
종류의 기술을 통해

00:28:18.640 --> 00:28:24.080
뇌-컴퓨터 인터페이스를 만들 수 있을지에 대한 의문이 제기되고 있습니다. 기존 기술

00:28:20.720 --> 00:28:25.919
외에도 우리 중 일부가 선택할 수 있는 다른 기술도 개발할 수 있으며,

00:28:25.919 --> 00:28:28.159
이는 우리가 뒤처지지 않기 위한 한 가지 방법이 될 수 있습니다

00:28:27.760 --> 00:28:30.080
.

00:28:28.159 --> 00:28:32.159
그리고 우리가 작별인사를 할 때 당신은 평생

00:28:30.080 --> 00:28:36.960
리버풀 팬이 되었습니다.  당신은

00:28:32.159 --> 00:28:39.279
그들의 분석에 도움을 주었습니다.  AI는 북미 월드컵에 어떤 영향을 미치고 어떤 정보를 제공할까요

00:28:39.279 --> 00:28:41.679
?

00:28:40.320 --> 00:28:43.600
글쎄요,

00:28:41.679 --> 00:28:46.000
우리에게 도움을 요청하는 팀이 많이 있었어요.

00:28:43.600 --> 00:28:47.679
그리고 음, 저는

00:28:46.000 --> 00:28:49.360
그에 맞춰 노력해야 하지만,

00:28:47.679 --> 00:28:50.720
평생 리버풀에 대한 추억을 갖는 건 어렵죠.  하지만 저는

00:28:50.720 --> 00:28:52.480
적어도 월드컵

00:28:51.919 --> 00:28:54.799
결승전까지는 여기까지 올 수 있기를 기대하고 있습니다.

00:28:52.480 --> 00:28:57.039
하지만 진지하게 생각해 봅시다.  그때까지 무엇이 무엇이 어떻게

00:28:54.799 --> 00:28:59.760
바뀔까요?   그때까지

00:28:57.039 --> 00:29:00.559
AI와 함께하는 시간은 평생일 것 같은데요

00:28:59.760 --> 00:29:02.799
, 맞나요?

00:29:00.559 --> 00:29:03.520
응.  글쎄요, AI나 스포츠용 AI,

00:29:02.799 --> 00:29:05.440
아니면 그냥

00:29:03.520 --> 00:29:07.279
그렇습니다.  응.  글쎄요, 스포츠에는

00:29:05.440 --> 00:29:09.600
엄청난 양의 데이터가 있고, 그것은 모두

00:29:07.279 --> 00:29:11.679
극한의 엘리트 성과에 관한 것입니다.  따라서

00:29:11.679 --> 00:29:16.240
AI가 개입하여

00:29:14.159 --> 00:29:18.000
그 과정을 더욱 최적화하는 것은 자연스러운 일입니다.

00:29:16.240 --> 00:29:19.600
그리고 무역 비밀을 공개하지 않고도

00:29:18.000 --> 00:29:21.039
월드컵 팀에 어떤 도움을 줄 수 있을까요

00:29:19.600 --> 00:29:23.120
?

00:29:21.039 --> 00:29:24.799
음, 코너킥 상황에서 헤딩으로 더 많은 골을 넣을 수도 있을 것 같아요.

00:29:27.840 --> 00:29:33.840
선수들의 정확한 위치를 잡는 게 우리 시스템에서 찾아낸 것 중 하나라고 생각해요.  Deus, 만들어 주셔서 감사합니다.

