{
  "wireMagic": "pb3",
  "pens": [
    {}
  ],
  "wsWinStyles": [
    {}
  ],
  "wpWinPositions": [
    {}
  ],
  "events": [
    {
      "tStartMs": 240,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "You know what's crazy? That all of this is real.\nMeaning what? "
        }
      ]
    },
    {
      "tStartMs": 5200,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "Don't you think so? All this AI stuff and \nall this Bay Area… that it's happening. "
        }
      ]
    },
    {
      "tStartMs": 11440,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "Isn't it straight out of science fiction?\nAnother thing that's crazy is how  "
        }
      ]
    },
    {
      "tStartMs": 16240,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "normal the slow takeoff feels.\nThe idea that we'd be investing 1%  "
        }
      ]
    },
    {
      "tStartMs": 21520,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "of GDP in AI, I feel like it would have felt like \na bigger deal, whereas right now it just feels... "
        }
      ]
    },
    {
      "tStartMs": 26880,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "We get used to things pretty fast, it turns out.\nBut also it's kind of abstract. What does it  "
        }
      ]
    },
    {
      "tStartMs": 32640,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "mean? It means that you see it in the news, \nthat such and such company announced such  "
        }
      ]
    },
    {
      "tStartMs": 37920,
      "dDurationMs": 7920,
      "segs": [
        {
          "utf8": "and such dollar amount. That's all you see. \nIt's not really felt in any other way so far. "
        }
      ]
    },
    {
      "tStartMs": 45840,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "Should we actually begin here? I think \nthis is an interesting discussion. "
        }
      ]
    },
    {
      "tStartMs": 48000,
      "dDurationMs": 1920,
      "segs": [
        {
          "utf8": "Sure.\nI think your point,  "
        }
      ]
    },
    {
      "tStartMs": 49920,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "about how from the average person's point of view \nnothing is that different, will continue being  "
        }
      ]
    },
    {
      "tStartMs": 55440,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "true even into the singularity.\nNo, I don't think so. "
        }
      ]
    },
    {
      "tStartMs": 58880,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "Okay, interesting.\nThe thing which I was referring  "
        }
      ]
    },
    {
      "tStartMs": 61920,
      "dDurationMs": 8960,
      "segs": [
        {
          "utf8": "to not feeling different is, okay, such and such \ncompany announced some difficult-to-comprehend  "
        }
      ]
    },
    {
      "tStartMs": 70880,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "dollar amount of investment.\nI don't think anyone knows what to do with that. "
        }
      ]
    },
    {
      "tStartMs": 75920,
      "dDurationMs": 8240,
      "segs": [
        {
          "utf8": "But I think the impact of AI is going to be felt.\nAI is going to be diffused through the economy. "
        }
      ]
    },
    {
      "tStartMs": 84160,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "There'll be very strong economic forces \nfor this, and I think the impact is  "
        }
      ]
    },
    {
      "tStartMs": 88560,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "going to be felt very strongly.\nWhen do you expect that impact? "
        }
      ]
    },
    {
      "tStartMs": 92640,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "I think the models seem smarter than \ntheir economic impact would imply. "
        }
      ]
    },
    {
      "tStartMs": 98480,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "Yeah. This is one of the very confusing \nthings about the models right now. "
        }
      ]
    },
    {
      "tStartMs": 104720,
      "dDurationMs": 7680,
      "segs": [
        {
          "utf8": "How to reconcile the fact that \nthey are doing so well on evals? "
        }
      ]
    },
    {
      "tStartMs": 113040,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "You look at the evals and you go, \"Those \nare pretty hard evals.\" They are doing  "
        }
      ]
    },
    {
      "tStartMs": 117680,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "so well. But the economic impact \nseems to be dramatically behind. "
        }
      ]
    },
    {
      "tStartMs": 127920,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "It's very difficult to make sense of, \nhow can the model, on the one hand,  "
        }
      ]
    },
    {
      "tStartMs": 132240,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "do these amazing things, and then on the other \nhand, repeat itself twice in some situation? "
        }
      ]
    },
    {
      "tStartMs": 140560,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "An example would be, let's say you \nuse vibe coding to do something. "
        }
      ]
    },
    {
      "tStartMs": 144560,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "You go to some place and then you get a bug.\nThen you tell the model,  "
        }
      ]
    },
    {
      "tStartMs": 148240,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "\"Can you please fix the bug?\"\nAnd the model says, \"Oh my God,  "
        }
      ]
    },
    {
      "tStartMs": 152000,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "you're so right. I have a bug. Let me go \nfix that.\" And it introduces a second bug. "
        }
      ]
    },
    {
      "tStartMs": 156960,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "Then you tell it, \"You have this \nnew second bug,\" and it tells you,  "
        }
      ]
    },
    {
      "tStartMs": 160080,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "\"Oh my God, how could I have done it?\nYou're so right again,\" and brings back  "
        }
      ]
    },
    {
      "tStartMs": 163520,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "the first bug, and you can alternate between \nthose. How is that possible? I'm not sure, but it  "
        }
      ]
    },
    {
      "tStartMs": 172000,
      "dDurationMs": 10080,
      "segs": [
        {
          "utf8": "does suggest that something strange is going on. I \nhave two possible explanations. The more whimsical  "
        }
      ]
    },
    {
      "tStartMs": 182080,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "explanation is that maybe RL training makes the \nmodels a little too single-minded and narrowly  "
        }
      ]
    },
    {
      "tStartMs": 187760,
      "dDurationMs": 9920,
      "segs": [
        {
          "utf8": "focused, a little bit too unaware, even though \nit also makes them aware in some other ways. "
        }
      ]
    },
    {
      "tStartMs": 197680,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "Because of this, they can't do basic things. \nBut there is another explanation. Back when  "
        }
      ]
    },
    {
      "tStartMs": 205040,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "people were doing pre-training, the \nquestion of what data to train on was  "
        }
      ]
    },
    {
      "tStartMs": 211440,
      "dDurationMs": 9600,
      "segs": [
        {
          "utf8": "answered, because that answer was everything.\nWhen you do pre-training, you need all the data. "
        }
      ]
    },
    {
      "tStartMs": 221040,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "So you don't have to think if it's \ngoing to be this data or that data. "
        }
      ]
    },
    {
      "tStartMs": 224960,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "But when people do RL training, \nthey do need to think. "
        }
      ]
    },
    {
      "tStartMs": 228560,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "They say, \"Okay, we want to have this \nkind of RL training for this thing  "
        }
      ]
    },
    {
      "tStartMs": 232080,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "and that kind of RL training for that thing.\"\nFrom what I hear, all the companies have teams  "
        }
      ]
    },
    {
      "tStartMs": 238160,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "that just produce new RL environments \nand just add it to the training mix. "
        }
      ]
    },
    {
      "tStartMs": 242800,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "The question is, well, what are those?\nThere are so many degrees of freedom. "
        }
      ]
    },
    {
      "tStartMs": 246080,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "There is such a huge variety of \nRL environments you could produce. "
        }
      ]
    },
    {
      "tStartMs": 252720,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "One thing you could do, and I think this \nis something that is done inadvertently,  "
        }
      ]
    },
    {
      "tStartMs": 257360,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "is that people take inspiration from the evals.\nYou say, \"Hey, I would love our model to do  "
        }
      ]
    },
    {
      "tStartMs": 264880,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "really well when we release it.\nI want the evals to look great. "
        }
      ]
    },
    {
      "tStartMs": 268720,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "What would be RL training \nthat could help on this task?\" "
        }
      ]
    },
    {
      "tStartMs": 273840,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "I think that is something that happens, and \nit could explain a lot of what's going on. "
        }
      ]
    },
    {
      "tStartMs": 279040,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "If you combine this with generalization \nof the models actually being inadequate,  "
        }
      ]
    },
    {
      "tStartMs": 284720,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "that has the potential to explain a lot \nof what we are seeing, this disconnect  "
        }
      ]
    },
    {
      "tStartMs": 288640,
      "dDurationMs": 7680,
      "segs": [
        {
          "utf8": "between eval performance and actual real-world \nperformance, which is something that we don't  "
        }
      ]
    },
    {
      "tStartMs": 296320,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "today even understand, what we mean by that.\nI like this idea that the real reward hacking  "
        }
      ]
    },
    {
      "tStartMs": 303680,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "is the human researchers who \nare too focused on the evals. "
        }
      ]
    },
    {
      "tStartMs": 309040,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "I think there are two ways to \nunderstand, or to try to think about,  "
        }
      ]
    },
    {
      "tStartMs": 312960,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "what you have just pointed out.\nOne is that if it's the case that  "
        }
      ]
    },
    {
      "tStartMs": 318880,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "simply by becoming superhuman at a coding \ncompetition, a model will not automatically  "
        }
      ]
    },
    {
      "tStartMs": 323600,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "become more tasteful and exercise better judgment \nabout how to improve your codebase, well then you  "
        }
      ]
    },
    {
      "tStartMs": 330080,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "should expand the suite of environments such \nthat you're not just testing it on having  "
        }
      ]
    },
    {
      "tStartMs": 335120,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "the best performance in coding competition.\nIt should also be able to make the best kind  "
        }
      ]
    },
    {
      "tStartMs": 338560,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "of application for X thing or Y thing or Z thing.\nAnother, maybe this is what you're hinting at,  "
        }
      ]
    },
    {
      "tStartMs": 344400,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "is to say, \"Why should it be the case in \nthe first place that becoming superhuman  "
        }
      ]
    },
    {
      "tStartMs": 350320,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "at coding competitions doesn't make you a \nmore tasteful programmer more generally?\" "
        }
      ]
    },
    {
      "tStartMs": 354400,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "Maybe the thing to do is not to keep \nstacking up the amount and diversity  "
        }
      ]
    },
    {
      "tStartMs": 359280,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "of environments, but to figure out an approach \nwhich lets you learn from one environment and  "
        }
      ]
    },
    {
      "tStartMs": 365200,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "improve your performance on something else.\nI have a human analogy which might be helpful. "
        }
      ]
    },
    {
      "tStartMs": 374160,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "Let's take the case of competitive programming, \nsince you mentioned that. Suppose you have two  "
        }
      ]
    },
    {
      "tStartMs": 378160,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "students. One of them decided they want \nto be the best competitive programmer, so  "
        }
      ]
    },
    {
      "tStartMs": 384240,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "they will practice 10,000 hours for that domain.\nThey will solve all the problems, memorize all the  "
        }
      ]
    },
    {
      "tStartMs": 391200,
      "dDurationMs": 8720,
      "segs": [
        {
          "utf8": "proof techniques, and be very skilled at quickly \nand correctly implementing all the algorithms. "
        }
      ]
    },
    {
      "tStartMs": 400640,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "By doing so, they became one of the best.\nStudent number two thought, \"Oh,  "
        }
      ]
    },
    {
      "tStartMs": 406240,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "competitive programming is cool.\"\nMaybe they practiced for 100 hours,  "
        }
      ]
    },
    {
      "tStartMs": 410400,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "much less, and they also did really well.\nWhich one do you think is going to do better  "
        }
      ]
    },
    {
      "tStartMs": 414640,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "in their career later on?\nThe second. "
        }
      ]
    },
    {
      "tStartMs": 416800,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "Right. I think that's basically what's going on.\nThe models are much more like the  "
        }
      ]
    },
    {
      "tStartMs": 420560,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "first student, but even more.\nBecause then we say, the model should  "
        }
      ]
    },
    {
      "tStartMs": 424400,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "be good at competitive programming so let's get \nevery single competitive programming problem ever. "
        }
      ]
    },
    {
      "tStartMs": 430080,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "And then let's do some data augmentation \nso we have even more competitive  "
        }
      ]
    },
    {
      "tStartMs": 433120,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "programming problems, and we train on that.\nNow you've got this great competitive programmer. "
        }
      ]
    },
    {
      "tStartMs": 438480,
      "dDurationMs": 8640,
      "segs": [
        {
          "utf8": "With this analogy, I think it's more intuitive.\nYeah, okay, if it's so well trained, all the  "
        }
      ]
    },
    {
      "tStartMs": 447120,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "different algorithms and all the different \nproof techniques are right at its fingertips. "
        }
      ]
    },
    {
      "tStartMs": 452480,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "And it's more intuitive that with this \nlevel of preparation, it would not  "
        }
      ]
    },
    {
      "tStartMs": 456320,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "necessarily generalize to other things.\nBut then what is the analogy for what  "
        }
      ]
    },
    {
      "tStartMs": 462160,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "the second student is doing before \nthey do the 100 hours of fine-tuning? "
        }
      ]
    },
    {
      "tStartMs": 468160,
      "dDurationMs": 8320,
      "segs": [
        {
          "utf8": "I think they have \"it.\" The \"it\" \nfactor. When I was an undergrad,  "
        }
      ]
    },
    {
      "tStartMs": 476480,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "I remember there was a student like this \nthat studied with me, so I know it exists. "
        }
      ]
    },
    {
      "tStartMs": 481840,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "I think it's interesting to distinguish \n\"it\" from whatever pre-training does. "
        }
      ]
    },
    {
      "tStartMs": 486160,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "One way to understand what you just said \nabout not having to choose the data in  "
        }
      ]
    },
    {
      "tStartMs": 490240,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "pre-training is to say it's actually not \ndissimilar to the 10,000 hours of practice. "
        }
      ]
    },
    {
      "tStartMs": 495040,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "It's just that you get that 10,000 hours \nof practice for free because it's already  "
        }
      ]
    },
    {
      "tStartMs": 500320,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "somewhere in the pre-training distribution.\nBut maybe you're suggesting there's actually  "
        }
      ]
    },
    {
      "tStartMs": 505120,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "not that much generalization from pre-training.\nThere's just so much data in pre-training, but  "
        }
      ]
    },
    {
      "tStartMs": 508800,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "it's not necessarily generalizing better than RL.\nThe main strength of pre-training is  "
        }
      ]
    },
    {
      "tStartMs": 513360,
      "dDurationMs": 6800,
      "segs": [
        {
          "utf8": "that: A, there is so much of it, and B, \nyou don't have to think hard about what  "
        }
      ]
    },
    {
      "tStartMs": 520160,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "data to put into pre-training.\nIt's very natural data, and it  "
        }
      ]
    },
    {
      "tStartMs": 525680,
      "dDurationMs": 9120,
      "segs": [
        {
          "utf8": "does include in it a lot of what people do: \npeople's thoughts and a lot of the features. "
        }
      ]
    },
    {
      "tStartMs": 534800,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "It's like the whole world as projected by \npeople onto text, and pre-training tries  "
        }
      ]
    },
    {
      "tStartMs": 541120,
      "dDurationMs": 7840,
      "segs": [
        {
          "utf8": "to capture that using a huge amount of data.\nPre-training is very difficult to reason about  "
        }
      ]
    },
    {
      "tStartMs": 548960,
      "dDurationMs": 8800,
      "segs": [
        {
          "utf8": "because it's so hard to understand the manner \nin which the model relies on pre-training data. "
        }
      ]
    },
    {
      "tStartMs": 557760,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "Whenever the model makes a mistake, could it be \nbecause something by chance is not as supported  "
        }
      ]
    },
    {
      "tStartMs": 563600,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "by the pre-training data? \"Support by \npre-training\" is maybe a loose term. "
        }
      ]
    },
    {
      "tStartMs": 570720,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "I don't know if I can add \nanything more useful on this. "
        }
      ]
    },
    {
      "tStartMs": 576000,
      "dDurationMs": 2400,
      "segs": [
        {
          "utf8": "I don't think there is a \nhuman analog to pre-training. "
        }
      ]
    },
    {
      "tStartMs": 579440,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "Here are analogies that people have proposed \nfor what the human analogy to pre-training is. "
        }
      ]
    },
    {
      "tStartMs": 583600,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "I'm curious to get your thoughts \non why they're potentially wrong. "
        }
      ]
    },
    {
      "tStartMs": 587920,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "One is to think about the first 18, or 15, \nor 13 years of a person's life when they  "
        }
      ]
    },
    {
      "tStartMs": 594160,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "aren't necessarily economically productive, \nbut they are doing something that is making  "
        }
      ]
    },
    {
      "tStartMs": 600320,
      "dDurationMs": 6880,
      "segs": [
        {
          "utf8": "them understand the world better and so forth.\nThe other is to think about evolution as doing  "
        }
      ]
    },
    {
      "tStartMs": 607200,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "some kind of search for 3 billion years, which \nthen results in a human lifetime instance. "
        }
      ]
    },
    {
      "tStartMs": 614480,
      "dDurationMs": 2880,
      "segs": [
        {
          "utf8": "I'm curious if you think either of \nthese are analogous to pre-training. "
        }
      ]
    },
    {
      "tStartMs": 618000,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "How would you think about what lifetime \nhuman learning is like, if not pre-training? "
        }
      ]
    },
    {
      "tStartMs": 622800,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "I think there are some similarities between both \nof these and pre-training, and pre-training tries  "
        }
      ]
    },
    {
      "tStartMs": 628800,
      "dDurationMs": 3200,
      "segs": [
        {
          "utf8": "to play the role of both of these.\nBut I think there are some  "
        }
      ]
    },
    {
      "tStartMs": 632000,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "big differences as well.\nThe amount of pre-training data is very,  "
        }
      ]
    },
    {
      "tStartMs": 638240,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "very staggering.\nYes. "
        }
      ]
    },
    {
      "tStartMs": 641520,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "Somehow a human being, after even 15 years \nwith a tiny fraction of the pre-training  "
        }
      ]
    },
    {
      "tStartMs": 647600,
      "dDurationMs": 3200,
      "segs": [
        {
          "utf8": "data, they know much less.\nBut whatever they do know,  "
        }
      ]
    },
    {
      "tStartMs": 650800,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "they know much more deeply somehow.\nAlready at that age, you would not  "
        }
      ]
    },
    {
      "tStartMs": 657120,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "make mistakes that our AIs make. There is another \nthing. You might say, could it be something like  "
        }
      ]
    },
    {
      "tStartMs": 662240,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "evolution? The answer is maybe. But in this case, \nI think evolution might actually have an edge. "
        }
      ]
    },
    {
      "tStartMs": 668880,
      "dDurationMs": 10800,
      "segs": [
        {
          "utf8": "I remember reading about this case.\nOne way in which neuroscientists can  "
        }
      ]
    },
    {
      "tStartMs": 679680,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "learn about the brain is by studying people with \nbrain damage to different parts of the brain. "
        }
      ]
    },
    {
      "tStartMs": 686960,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "Some people have the most strange symptoms \nyou could imagine. It's actually really,  "
        }
      ]
    },
    {
      "tStartMs": 690640,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "really interesting. One case that \ncomes to mind that's relevant. "
        }
      ]
    },
    {
      "tStartMs": 695680,
      "dDurationMs": 8320,
      "segs": [
        {
          "utf8": "I read about this person who had some kind \nof brain damage, a stroke or an accident,  "
        }
      ]
    },
    {
      "tStartMs": 704000,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "that took out his emotional processing.\nSo he stopped feeling any emotion. "
        }
      ]
    },
    {
      "tStartMs": 714800,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "He still remained very articulate \nand he could solve little puzzles,  "
        }
      ]
    },
    {
      "tStartMs": 718240,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "and on tests he seemed to be just fine. \nBut he felt no emotion. He didn't feel sad,  "
        }
      ]
    },
    {
      "tStartMs": 723520,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "he didn't feel anger, he didn't feel animated.\nHe became somehow extremely bad at making any  "
        }
      ]
    },
    {
      "tStartMs": 730000,
      "dDurationMs": 2240,
      "segs": [
        {
          "utf8": "decisions at all.\nIt would take him  "
        }
      ]
    },
    {
      "tStartMs": 732240,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "hours to decide on which socks to wear.\nHe would make very bad financial decisions. "
        }
      ]
    },
    {
      "tStartMs": 743120,
      "dDurationMs": 11120,
      "segs": [
        {
          "utf8": "What does it say about the role of our built-in \nemotions in making us a viable agent, essentially? "
        }
      ]
    },
    {
      "tStartMs": 754240,
      "dDurationMs": 7440,
      "segs": [
        {
          "utf8": "To connect to your question about pre-training, \nmaybe if you are good enough at getting everything  "
        }
      ]
    },
    {
      "tStartMs": 761680,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "out of pre-training, you could get that as well.\nBut that's the kind of thing which seems... "
        }
      ]
    },
    {
      "tStartMs": 770960,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "Well, it may or may not be possible \nto get that from pre-training. "
        }
      ]
    },
    {
      "tStartMs": 776160,
      "dDurationMs": 8640,
      "segs": [
        {
          "utf8": "What is \"that\"? Clearly not just directly \nemotion. It seems like some almost value  "
        }
      ]
    },
    {
      "tStartMs": 784800,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "function-like thing which is telling you what \nthe end reward for any decision should be. "
        }
      ]
    },
    {
      "tStartMs": 792080,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "You think that doesn't sort of \nimplicitly come from pre-training? "
        }
      ]
    },
    {
      "tStartMs": 795200,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "I think it could. I'm just \nsaying it's not 100% obvious. "
        }
      ]
    },
    {
      "tStartMs": 800400,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "But what is that? How do you think about emotions?\nWhat is the ML analogy for emotions? "
        }
      ]
    },
    {
      "tStartMs": 806320,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "It should be some kind of a value function thing.\nBut I don’t think there is a great ML analogy  "
        }
      ]
    },
    {
      "tStartMs": 811440,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "because right now, value functions don't play \na very prominent role in the things people do. "
        }
      ]
    },
    {
      "tStartMs": 816480,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "It might be worth defining for the audience what \na value function is, if you want to do that. "
        }
      ]
    },
    {
      "tStartMs": 820560,
      "dDurationMs": 10320,
      "segs": [
        {
          "utf8": "Certainly, I'll be very happy to do that.\nWhen people do reinforcement learning,  "
        }
      ]
    },
    {
      "tStartMs": 830880,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "the way reinforcement learning is done \nright now, how do people train those agents? "
        }
      ]
    },
    {
      "tStartMs": 836560,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "You have your neural net and you \ngive it a problem, and then you  "
        }
      ]
    },
    {
      "tStartMs": 840160,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "tell the model, \"Go solve it.\"\nThe model takes maybe thousands,  "
        }
      ]
    },
    {
      "tStartMs": 843440,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "hundreds of thousands of actions or thoughts or \nsomething, and then it produces a solution. The  "
        }
      ]
    },
    {
      "tStartMs": 849040,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "solution is graded. And then the score \nis used to provide a training signal  "
        }
      ]
    },
    {
      "tStartMs": 854960,
      "dDurationMs": 8240,
      "segs": [
        {
          "utf8": "for every single action in your trajectory.\nThat means that if you are doing something  "
        }
      ]
    },
    {
      "tStartMs": 863200,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "that goes for a long time—if you're training \na task that takes a long time to solve—it  "
        }
      ]
    },
    {
      "tStartMs": 869120,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "will do no learning at all until you \ncome up with the proposed solution. "
        }
      ]
    },
    {
      "tStartMs": 873440,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "That's how reinforcement learning is done naively.\nThat's how o1, R1 ostensibly are done. "
        }
      ]
    },
    {
      "tStartMs": 880800,
      "dDurationMs": 7440,
      "segs": [
        {
          "utf8": "The value function says something like, \n\"Maybe I could sometimes, not always,  "
        }
      ]
    },
    {
      "tStartMs": 888240,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "tell you if you are doing well or badly.\"\nThe notion of a value function is more  "
        }
      ]
    },
    {
      "tStartMs": 892400,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "useful in some domains than others.\nFor example, when you play chess and  "
        }
      ]
    },
    {
      "tStartMs": 896960,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "you lose a piece, I messed up.\nYou don't need to play the whole  "
        }
      ]
    },
    {
      "tStartMs": 901440,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "game to know that what I just did was bad, and \ntherefore whatever preceded it was also bad. "
        }
      ]
    },
    {
      "tStartMs": 908960,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "The value function lets you short-circuit \nthe wait until the very end. "
        }
      ]
    },
    {
      "tStartMs": 919040,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "Let's suppose that you are doing some kind \nof a math thing or a programming thing,  "
        }
      ]
    },
    {
      "tStartMs": 923040,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "and you're trying to explore a \nparticular solution or direction. "
        }
      ]
    },
    {
      "tStartMs": 926800,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "After, let's say, a thousand steps of thinking, \nyou concluded that this direction is unpromising. "
        }
      ]
    },
    {
      "tStartMs": 934320,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "As soon as you conclude this, you \ncould already get a reward signal  "
        }
      ]
    },
    {
      "tStartMs": 939760,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "a thousand timesteps previously, when \nyou decided to pursue down this path. "
        }
      ]
    },
    {
      "tStartMs": 943520,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "You say, \"Next time I shouldn't pursue this \npath in a similar situation,\" long before you  "
        }
      ]
    },
    {
      "tStartMs": 949600,
      "dDurationMs": 6880,
      "segs": [
        {
          "utf8": "actually came up with the proposed solution.\nThis was in the DeepSeek R1 paper— that the  "
        }
      ]
    },
    {
      "tStartMs": 956480,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "space of trajectories is so wide that \nmaybe it's hard to learn a mapping  "
        }
      ]
    },
    {
      "tStartMs": 962640,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "from an intermediate trajectory and value.\nAnd also given that, in coding for example  "
        }
      ]
    },
    {
      "tStartMs": 968800,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "you'll have the wrong idea, then you'll \ngo back, then you'll change something. "
        }
      ]
    },
    {
      "tStartMs": 972720,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "This sounds like such lack \nof faith in deep learning. "
        }
      ]
    },
    {
      "tStartMs": 976800,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "Sure it might be difficult, but \nnothing deep learning can't do. "
        }
      ]
    },
    {
      "tStartMs": 983520,
      "dDurationMs": 9120,
      "segs": [
        {
          "utf8": "My expectation is that a value function should \nbe useful, and I fully expect that they will  "
        }
      ]
    },
    {
      "tStartMs": 992640,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "be used in the future, if not already.\nWhat I was alluding to with the person  "
        }
      ]
    },
    {
      "tStartMs": 997040,
      "dDurationMs": 10640,
      "segs": [
        {
          "utf8": "whose emotional center got damaged, it’s more \nthat maybe what it suggests is that the value  "
        }
      ]
    },
    {
      "tStartMs": 1007680,
      "dDurationMs": 7680,
      "segs": [
        {
          "utf8": "function of humans is modulated by emotions in \nsome important way that's hardcoded by evolution. "
        }
      ]
    },
    {
      "tStartMs": 1015360,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "And maybe that is important for \npeople to be effective in the world. "
        }
      ]
    },
    {
      "tStartMs": 1020400,
      "dDurationMs": 2240,
      "segs": [
        {
          "utf8": "That's the thing I was planning on asking you. "
        }
      ]
    },
    {
      "tStartMs": 1022640,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "There's something really interesting about \nemotions of the value function, which is that  "
        }
      ]
    },
    {
      "tStartMs": 1026160,
      "dDurationMs": 10000,
      "segs": [
        {
          "utf8": "it's impressive that they have this much utility \nwhile still being rather simple to understand. "
        }
      ]
    },
    {
      "tStartMs": 1036160,
      "dDurationMs": 9440,
      "segs": [
        {
          "utf8": "I have two responses. I do agree that compared to \nthe kind of things that we learn and the things  "
        }
      ]
    },
    {
      "tStartMs": 1045600,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "we are talking about, the kind of AI we are \ntalking about, emotions are relatively simple. "
        }
      ]
    },
    {
      "tStartMs": 1051120,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "They might even be so simple that maybe you \ncould map them out in a human-understandable way. "
        }
      ]
    },
    {
      "tStartMs": 1055680,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "I think it would be cool to do.\nIn terms of utility though,  "
        }
      ]
    },
    {
      "tStartMs": 1060960,
      "dDurationMs": 8400,
      "segs": [
        {
          "utf8": "I think there is a thing where there is this \ncomplexity-robustness tradeoff, where complex  "
        }
      ]
    },
    {
      "tStartMs": 1069360,
      "dDurationMs": 9600,
      "segs": [
        {
          "utf8": "things can be very useful, but simple things are \nvery useful in a very broad range of situations. "
        }
      ]
    },
    {
      "tStartMs": 1080240,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "One way to interpret what we are seeing is that \nwe've got these emotions that evolved mostly  "
        }
      ]
    },
    {
      "tStartMs": 1086720,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "from our mammal ancestors and then fine-tuned a \nlittle bit while we were hominids, just a bit. "
        }
      ]
    },
    {
      "tStartMs": 1093200,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "We do have a decent amount of social emotions \nthough which mammals may lack. But they're  "
        }
      ]
    },
    {
      "tStartMs": 1099760,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "not very sophisticated. And because they're \nnot sophisticated, they serve us so well in  "
        }
      ]
    },
    {
      "tStartMs": 1104720,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "this very different world compared to the \none that we've been living in. Actually,  "
        }
      ]
    },
    {
      "tStartMs": 1108480,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "they also make mistakes. For example, our \nemotions… Well actually, I don’t know. "
        }
      ]
    },
    {
      "tStartMs": 1112800,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "Does hunger count as an emotion? It's debatable. \nBut I think, for example, our intuitive feeling  "
        }
      ]
    },
    {
      "tStartMs": 1119760,
      "dDurationMs": 9360,
      "segs": [
        {
          "utf8": "of hunger is not succeeding in guiding us \ncorrectly in this world with an abundance of food. "
        }
      ]
    },
    {
      "tStartMs": 1130000,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "People have been talking about scaling \ndata, scaling parameters, scaling compute. "
        }
      ]
    },
    {
      "tStartMs": 1136240,
      "dDurationMs": 1760,
      "segs": [
        {
          "utf8": "Is there a more general \nway to think about scaling? "
        }
      ]
    },
    {
      "tStartMs": 1138000,
      "dDurationMs": 12480,
      "segs": [
        {
          "utf8": "What are the other scaling axes?\nHere's a perspective that I think might be true. "
        }
      ]
    },
    {
      "tStartMs": 1152160,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "The way ML used to work is that \npeople would just tinker with  "
        }
      ]
    },
    {
      "tStartMs": 1156320,
      "dDurationMs": 11920,
      "segs": [
        {
          "utf8": "stuff and try to get interesting results.\nThat's what's been going on in the past. Then  "
        }
      ]
    },
    {
      "tStartMs": 1168240,
      "dDurationMs": 10880,
      "segs": [
        {
          "utf8": "the scaling insight arrived. Scaling laws, GPT-3, \nand suddenly everyone realized we should scale. "
        }
      ]
    },
    {
      "tStartMs": 1180400,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "This is an example of how language \naffects thought. \"Scaling\" is just  "
        }
      ]
    },
    {
      "tStartMs": 1187120,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "one word, but it's such a powerful word \nbecause it informs people what to do. "
        }
      ]
    },
    {
      "tStartMs": 1191120,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "They say, \"Let's try to scale things.\"\nSo you say, what are we scaling? "
        }
      ]
    },
    {
      "tStartMs": 1197280,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "Pre-training was the thing to scale.\nIt was a particular scaling recipe. "
        }
      ]
    },
    {
      "tStartMs": 1202240,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "The big breakthrough of pre-training is \nthe realization that this recipe is good. "
        }
      ]
    },
    {
      "tStartMs": 1208000,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "You say, \"Hey, if you mix some compute \nwith some data into a neural net of  "
        }
      ]
    },
    {
      "tStartMs": 1214400,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "a certain size, you will get results.\nYou will know that you'll be better if you  "
        }
      ]
    },
    {
      "tStartMs": 1219600,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "just scale the recipe up.\" This is also great. \nCompanies love this because it gives you a very  "
        }
      ]
    },
    {
      "tStartMs": 1226560,
      "dDurationMs": 8240,
      "segs": [
        {
          "utf8": "low-risk way of investing your resources.\nIt's much harder to invest your resources  "
        }
      ]
    },
    {
      "tStartMs": 1234800,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "in research. Compare that. If you research, \nyou need to be like, \"Go forth researchers  "
        }
      ]
    },
    {
      "tStartMs": 1239760,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "and research and come up with something\", \nversus get more data, get more compute. "
        }
      ]
    },
    {
      "tStartMs": 1245440,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "You know you'll get something from pre-training.\nIndeed, it looks like, based on various  "
        }
      ]
    },
    {
      "tStartMs": 1254640,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "things some people say on Twitter, maybe it \nappears that Gemini have found a way to get  "
        }
      ]
    },
    {
      "tStartMs": 1260160,
      "dDurationMs": 2400,
      "segs": [
        {
          "utf8": "more out of pre-training.\nAt some point though,  "
        }
      ]
    },
    {
      "tStartMs": 1262560,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "pre-training will run out of data.\nThe data is very clearly finite. What  "
        }
      ]
    },
    {
      "tStartMs": 1266480,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "do you do next? Either you do some kind \nof souped-up pre-training, a different  "
        }
      ]
    },
    {
      "tStartMs": 1271280,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "recipe from the one you've done before, or \nyou're doing RL, or maybe something else. "
        }
      ]
    },
    {
      "tStartMs": 1275920,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "But now that compute is big, compute \nis now very big, in some sense we  "
        }
      ]
    },
    {
      "tStartMs": 1280320,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "are back to the age of research.\nMaybe here's another way to put it. "
        }
      ]
    },
    {
      "tStartMs": 1284160,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "Up until 2020, from 2012 to \n2020, it was the age of research. "
        }
      ]
    },
    {
      "tStartMs": 1291280,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "Now, from 2020 to 2025, it was the \nage of scaling—maybe plus or minus,  "
        }
      ]
    },
    {
      "tStartMs": 1295920,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "let's add error bars to those years—because \npeople say, \"This is amazing. You've got to  "
        }
      ]
    },
    {
      "tStartMs": 1299440,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "scale more. Keep scaling.\" The one word: \nscaling. But now the scale is so big. "
        }
      ]
    },
    {
      "tStartMs": 1306320,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "Is the belief really, \"Oh, it's so big, but if you \nhad 100x more, everything would be so different?\" "
        }
      ]
    },
    {
      "tStartMs": 1313440,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "It would be different, for sure.\nBut is the belief that if you just  "
        }
      ]
    },
    {
      "tStartMs": 1318000,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "100x the scale, everything would be transformed? \nI don't think that's true. So it's back to the age  "
        }
      ]
    },
    {
      "tStartMs": 1324560,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "of research again, just with big computers.\nThat's a very interesting way to put it. "
        }
      ]
    },
    {
      "tStartMs": 1330400,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "But let me ask you the \nquestion you just posed then. "
        }
      ]
    },
    {
      "tStartMs": 1332560,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "What are we scaling, and what \nwould it mean to have a recipe? "
        }
      ]
    },
    {
      "tStartMs": 1337440,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "I guess I'm not aware of a very clean \nrelationship that almost looks like a law  "
        }
      ]
    },
    {
      "tStartMs": 1343680,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "of physics which existed in pre-training.\nThere was a power law between data or  "
        }
      ]
    },
    {
      "tStartMs": 1347920,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "compute or parameters and loss.\nWhat is the kind of relationship  "
        }
      ]
    },
    {
      "tStartMs": 1353920,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "we should be seeking, and how should we think \nabout what this new recipe might look like? "
        }
      ]
    },
    {
      "tStartMs": 1360240,
      "dDurationMs": 7760,
      "segs": [
        {
          "utf8": "We've already witnessed a transition from one \ntype of scaling to a different type of scaling,  "
        }
      ]
    },
    {
      "tStartMs": 1368000,
      "dDurationMs": 8800,
      "segs": [
        {
          "utf8": "from pre-training to RL. Now people are scaling \nRL. Now based on what people say on Twitter,  "
        }
      ]
    },
    {
      "tStartMs": 1376800,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "they spend more compute on RL than on \npre-training at this point, because RL  "
        }
      ]
    },
    {
      "tStartMs": 1381280,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "can actually consume quite a bit of compute.\nYou do very long rollouts, so it takes a lot  "
        }
      ]
    },
    {
      "tStartMs": 1387840,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "of compute to produce those rollouts.\nThen you get a relatively small amount  "
        }
      ]
    },
    {
      "tStartMs": 1391360,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "of learning per rollout, so you \nreally can spend a lot of compute. "
        }
      ]
    },
    {
      "tStartMs": 1401680,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "I wouldn't even call it scaling.\nI would say, \"Hey, what are you doing? "
        }
      ]
    },
    {
      "tStartMs": 1407280,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "Is the thing you are doing the most \nproductive thing you could be doing? "
        }
      ]
    },
    {
      "tStartMs": 1411840,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "Can you find a more productive \nway of using your compute?\" "
        }
      ]
    },
    {
      "tStartMs": 1416000,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "We've discussed the value \nfunction business earlier. "
        }
      ]
    },
    {
      "tStartMs": 1419520,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "Maybe once people get good at value \nfunctions, they will be using their  "
        }
      ]
    },
    {
      "tStartMs": 1424720,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "resources more productively.\nIf you find a whole other way  "
        }
      ]
    },
    {
      "tStartMs": 1430320,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "of training models, you could say, \"Is this \nscaling or is it just using your resources?\" "
        }
      ]
    },
    {
      "tStartMs": 1436240,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "I think it becomes a little bit ambiguous.\nIn the sense that, when people were in the  "
        }
      ]
    },
    {
      "tStartMs": 1439680,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "age of research back then, it was, \n\"Let's try this and this and this. "
        }
      ]
    },
    {
      "tStartMs": 1443600,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "Let's try that and that and that.\nOh, look, something interesting is happening.\" "
        }
      ]
    },
    {
      "tStartMs": 1447680,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "I think there will be a return to that.\nIf we're back in the era of research,  "
        }
      ]
    },
    {
      "tStartMs": 1452480,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "stepping back, what is the part of the \nrecipe that we need to think most about? "
        }
      ]
    },
    {
      "tStartMs": 1457040,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "When you say value function, people \nare already trying the current recipe,  "
        }
      ]
    },
    {
      "tStartMs": 1461200,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "but then having LLM-as-a-Judge and so forth.\nYou could say that's a value function,  "
        }
      ]
    },
    {
      "tStartMs": 1464720,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "but it sounds like you have something \nmuch more fundamental in mind. "
        }
      ]
    },
    {
      "tStartMs": 1469920,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "Should we even rethink pre-training at all and not \njust add more steps to the end of that process? "
        }
      ]
    },
    {
      "tStartMs": 1478240,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "The discussion about value function, \nI think it was interesting. "
        }
      ]
    },
    {
      "tStartMs": 1481600,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "I want to emphasize that I think the value \nfunction is something that's going to make RL more  "
        }
      ]
    },
    {
      "tStartMs": 1488320,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "efficient, and I think that makes a difference.\nBut I think anything you can do with a value  "
        }
      ]
    },
    {
      "tStartMs": 1495440,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "function, you can do without, just more slowly.\nThe thing which I think is the most fundamental  "
        }
      ]
    },
    {
      "tStartMs": 1502080,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "is that these models somehow just generalize \ndramatically worse than people. It's super  "
        }
      ]
    },
    {
      "tStartMs": 1508560,
      "dDurationMs": 9920,
      "segs": [
        {
          "utf8": "obvious. That seems like a very fundamental thing.\nSo this is the crux: generalization. There are two  "
        }
      ]
    },
    {
      "tStartMs": 1518480,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "sub-questions. There's one which is about sample \nefficiency: why should it take so much more data  "
        }
      ]
    },
    {
      "tStartMs": 1524160,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "for these models to learn than humans? There's \na second question. Even separate from the amount  "
        }
      ]
    },
    {
      "tStartMs": 1529520,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "of data it takes, why is it so hard to teach \nthe thing we want to a model than to a human? "
        }
      ]
    },
    {
      "tStartMs": 1537360,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "For a human, we don't necessarily need a \nverifiable reward to be able to… You're probably  "
        }
      ]
    },
    {
      "tStartMs": 1543920,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "mentoring a bunch of researchers right now, and \nyou're talking with them, you're showing them your  "
        }
      ]
    },
    {
      "tStartMs": 1548560,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "code, and you're showing them how you think.\nFrom that, they're picking up your way of  "
        }
      ]
    },
    {
      "tStartMs": 1552560,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "thinking and how they should do research.\nYou don’t have to set a verifiable reward for  "
        }
      ]
    },
    {
      "tStartMs": 1556800,
      "dDurationMs": 2880,
      "segs": [
        {
          "utf8": "them that's like, \"Okay, this is the next part of \nthe curriculum, and now this is the next part of  "
        }
      ]
    },
    {
      "tStartMs": 1559680,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "your curriculum. Oh, this training was unstable.\" \nThere's not this schleppy, bespoke process. "
        }
      ]
    },
    {
      "tStartMs": 1566640,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "Perhaps these two issues are actually \nrelated in some way, but I'd be curious  "
        }
      ]
    },
    {
      "tStartMs": 1570160,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "to explore this second thing, which is more \nlike continual learning, and this first thing,  "
        }
      ]
    },
    {
      "tStartMs": 1575520,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "which feels just like sample efficiency.\nYou could actually wonder that one possible  "
        }
      ]
    },
    {
      "tStartMs": 1582720,
      "dDurationMs": 8000,
      "segs": [
        {
          "utf8": "explanation for the human sample efficiency \nthat needs to be considered is evolution. "
        }
      ]
    },
    {
      "tStartMs": 1591760,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "Evolution has given us a small amount \nof the most useful information possible. "
        }
      ]
    },
    {
      "tStartMs": 1598880,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "For things like vision, hearing, and \nlocomotion, I think there's a pretty  "
        }
      ]
    },
    {
      "tStartMs": 1605040,
      "dDurationMs": 10080,
      "segs": [
        {
          "utf8": "strong case that evolution has given us a lot.\nFor example, human dexterity far exceeds… I mean  "
        }
      ]
    },
    {
      "tStartMs": 1615120,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "robots can become dexterous too if you subject \nthem to a huge amount of training in simulation. "
        }
      ]
    },
    {
      "tStartMs": 1620480,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "But to train a robot in the real world \nto quickly pick up a new skill like  "
        }
      ]
    },
    {
      "tStartMs": 1624560,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "a person does seems very out of reach.\nHere you could say, \"Oh yeah, locomotion. "
        }
      ]
    },
    {
      "tStartMs": 1630560,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "All our ancestors needed \ngreat locomotion, squirrels. "
        }
      ]
    },
    {
      "tStartMs": 1635920,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "So with locomotion, maybe we've \ngot some unbelievable prior.\" "
        }
      ]
    },
    {
      "tStartMs": 1639760,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "You could make the same case for vision.\nI believe Yann LeCun made the point that  "
        }
      ]
    },
    {
      "tStartMs": 1645440,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "children learn to drive after 10 \nhours of practice, which is true. "
        }
      ]
    },
    {
      "tStartMs": 1650800,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "But our vision is so good.\nAt least for me,  "
        }
      ]
    },
    {
      "tStartMs": 1655280,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "I remember myself being a five-year-old.\nI was very excited about cars back then. "
        }
      ]
    },
    {
      "tStartMs": 1661120,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "I'm pretty sure my car recognition was more than \nadequate for driving already as a five-year-old. "
        }
      ]
    },
    {
      "tStartMs": 1667200,
      "dDurationMs": 1840,
      "segs": [
        {
          "utf8": "You don't get to see that \nmuch data as a five-year-old. "
        }
      ]
    },
    {
      "tStartMs": 1669040,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "You spend most of your time in your parents' \nhouse, so you have very low data diversity. "
        }
      ]
    },
    {
      "tStartMs": 1673200,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "But you could say maybe that's evolution too.\nBut in language and math and coding, probably not. "
        }
      ]
    },
    {
      "tStartMs": 1680400,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "It still seems better than models.\nObviously, models are better than the average  "
        }
      ]
    },
    {
      "tStartMs": 1684720,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "human at language, math, and coding.\nBut are they better than  "
        }
      ]
    },
    {
      "tStartMs": 1687760,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "the average human at learning?\nOh yeah. Oh yeah, absolutely. What I meant  "
        }
      ]
    },
    {
      "tStartMs": 1692160,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "to say is that language, math, and coding—and \nespecially math and coding—suggests that whatever  "
        }
      ]
    },
    {
      "tStartMs": 1698320,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "it is that makes people good at learning is \nprobably not so much a complicated prior,  "
        }
      ]
    },
    {
      "tStartMs": 1705920,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "but something more, some fundamental thing.\nI'm not sure I understood. Why  "
        }
      ]
    },
    {
      "tStartMs": 1710880,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "should that be the case?\nSo consider a skill in which  "
        }
      ]
    },
    {
      "tStartMs": 1715120,
      "dDurationMs": 10000,
      "segs": [
        {
          "utf8": "people exhibit some kind of great reliability.\nIf the skill is one that was very useful to our  "
        }
      ]
    },
    {
      "tStartMs": 1725120,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "ancestors for many millions of years, hundreds \nof millions of years, you could argue that maybe  "
        }
      ]
    },
    {
      "tStartMs": 1732240,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "humans are good at it because of evolution, \nbecause we have a prior, an evolutionary prior  "
        }
      ]
    },
    {
      "tStartMs": 1740720,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "that's encoded in some very non-obvious \nway that somehow makes us so good at it. "
        }
      ]
    },
    {
      "tStartMs": 1747200,
      "dDurationMs": 9360,
      "segs": [
        {
          "utf8": "But if people exhibit great ability, reliability, \nrobustness, and ability to learn in a domain that  "
        }
      ]
    },
    {
      "tStartMs": 1756560,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "really did not exist until recently, then \nthis is more an indication that people  "
        }
      ]
    },
    {
      "tStartMs": 1763760,
      "dDurationMs": 11040,
      "segs": [
        {
          "utf8": "might have just better machine learning, period.\nHow should we think about what that is? What is  "
        }
      ]
    },
    {
      "tStartMs": 1774800,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "the ML analogy? There are a couple of interesting \nthings about it. It takes fewer samples. It's  "
        }
      ]
    },
    {
      "tStartMs": 1781200,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "more unsupervised. A child learning to drive a \ncar… Children are not learning to drive a car. "
        }
      ]
    },
    {
      "tStartMs": 1787200,
      "dDurationMs": 8400,
      "segs": [
        {
          "utf8": "A teenager learning how to drive a car is not \nexactly getting some prebuilt, verifiable reward. "
        }
      ]
    },
    {
      "tStartMs": 1796400,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "It comes from their interaction with \nthe machine and with the environment.  "
        }
      ]
    },
    {
      "tStartMs": 1802720,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "It takes much fewer samples. It seems \nmore unsupervised. It seems more robust? "
        }
      ]
    },
    {
      "tStartMs": 1807440,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "Much more robust. The robustness \nof people is really staggering. "
        }
      ]
    },
    {
      "tStartMs": 1814160,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "Do you have a unified way of thinking about \nwhy all these things are happening at once? "
        }
      ]
    },
    {
      "tStartMs": 1818320,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "What is the ML analogy that could \nrealize something like this? "
        }
      ]
    },
    {
      "tStartMs": 1826320,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "One of the things that you've been asking about is \nhow can the teenage driver self-correct and learn  "
        }
      ]
    },
    {
      "tStartMs": 1833680,
      "dDurationMs": 7920,
      "segs": [
        {
          "utf8": "from their experience without an external teacher?\nThe answer is that they have their value function. "
        }
      ]
    },
    {
      "tStartMs": 1841600,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "They have a general sense which is also, \nby the way, extremely robust in people. "
        }
      ]
    },
    {
      "tStartMs": 1850480,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "Whatever the human value function is, \nwith a few exceptions around addiction,  "
        }
      ]
    },
    {
      "tStartMs": 1856080,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "it's actually very, very robust.\nSo for something like a teenager  "
        }
      ]
    },
    {
      "tStartMs": 1860800,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "that's learning to drive, they start to drive, and \nthey already have a sense of how they're driving  "
        }
      ]
    },
    {
      "tStartMs": 1867520,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "immediately, how badly they are, how unconfident. \nAnd then they see, \"Okay.\" And then, of course,  "
        }
      ]
    },
    {
      "tStartMs": 1873200,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "the learning speed of any teenager is so fast.\nAfter 10 hours, you're good to go. "
        }
      ]
    },
    {
      "tStartMs": 1877520,
      "dDurationMs": 2240,
      "segs": [
        {
          "utf8": "It seems like humans have some \nsolution, but I'm curious about  "
        }
      ]
    },
    {
      "tStartMs": 1880800,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "how they are doing it and why is it so hard?\nHow do we need to reconceptualize the way  "
        }
      ]
    },
    {
      "tStartMs": 1884960,
      "dDurationMs": 2560,
      "segs": [
        {
          "utf8": "we're training models to make \nsomething like this possible? "
        }
      ]
    },
    {
      "tStartMs": 1888720,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "That is a great question to ask, and it's \na question I have a lot of opinions about. "
        }
      ]
    },
    {
      "tStartMs": 1897200,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "But unfortunately, we live in a world where \nnot all machine learning ideas are discussed  "
        }
      ]
    },
    {
      "tStartMs": 1903200,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "freely, and this is one of them.\nThere's probably a way to do it. "
        }
      ]
    },
    {
      "tStartMs": 1909360,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "I think it can be done.\nThe fact that people are like that,  "
        }
      ]
    },
    {
      "tStartMs": 1914240,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "I think it's a proof that it can be done.\nThere may be another blocker though,  "
        }
      ]
    },
    {
      "tStartMs": 1917840,
      "dDurationMs": 9920,
      "segs": [
        {
          "utf8": "which is that there is a possibility that the \nhuman neurons do more compute than we think. "
        }
      ]
    },
    {
      "tStartMs": 1927760,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "If that is true, and if that plays an important \nrole, then things might be more difficult. "
        }
      ]
    },
    {
      "tStartMs": 1933760,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "But regardless, I do think it points to \nthe existence of some machine learning  "
        }
      ]
    },
    {
      "tStartMs": 1940080,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "principle that I have opinions on.\nBut unfortunately, circumstances  "
        }
      ]
    },
    {
      "tStartMs": 1945840,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "make it hard to discuss in detail.\nNobody listens to this podcast, Ilya. "
        }
      ]
    },
    {
      "tStartMs": 1952560,
      "dDurationMs": 200800,
      "segs": [
        {
          "utf8": "I'm curious. If you say we are back in an era \nof research, you were there from 2012 to 2020. "
        }
      ]
    },
    {
      "tStartMs": 2155840,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "What is the vibe now going to be if \nwe go back to the era of research? "
        }
      ]
    },
    {
      "tStartMs": 2160640,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "For example, even after AlexNet, the \namount of compute that was used to  "
        }
      ]
    },
    {
      "tStartMs": 2165920,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "run experiments kept increasing, and the \nsize of frontier systems kept increasing. "
        }
      ]
    },
    {
      "tStartMs": 2173440,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "Do you think now that this era of research will \nstill require tremendous amounts of compute? "
        }
      ]
    },
    {
      "tStartMs": 2179760,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "Do you think it will require going back \ninto the archives and reading old papers? "
        }
      ]
    },
    {
      "tStartMs": 2188000,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "You were at Google and OpenAI and Stanford, these \nplaces, when there was more of a vibe of research? "
        }
      ]
    },
    {
      "tStartMs": 2194960,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "What kind of things should we \nbe expecting in the community? "
        }
      ]
    },
    {
      "tStartMs": 2200160,
      "dDurationMs": 9440,
      "segs": [
        {
          "utf8": "One consequence of the age of scaling is that \nscaling sucked out all the air in the room. "
        }
      ]
    },
    {
      "tStartMs": 2213120,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "Because scaling sucked out all the air in the \nroom, everyone started to do the same thing. "
        }
      ]
    },
    {
      "tStartMs": 2219680,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "We got to the point where we are \nin a world where there are more  "
        }
      ]
    },
    {
      "tStartMs": 2225840,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "companies than ideas by quite a bit.\nActually on that, there is this Silicon  "
        }
      ]
    },
    {
      "tStartMs": 2231440,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "Valley saying that says that ideas \nare cheap, execution is everything. "
        }
      ]
    },
    {
      "tStartMs": 2238480,
      "dDurationMs": 6800,
      "segs": [
        {
          "utf8": "People say that a lot, and there is truth to that.\nBut then I saw someone say on Twitter  "
        }
      ]
    },
    {
      "tStartMs": 2245280,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "something like, \"If ideas are so cheap, \nhow come no one's having any ideas?\" "
        }
      ]
    },
    {
      "tStartMs": 2250880,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "And I think it's true too.\nIf you think about research progress in terms  "
        }
      ]
    },
    {
      "tStartMs": 2257840,
      "dDurationMs": 9360,
      "segs": [
        {
          "utf8": "of bottlenecks, there are several bottlenecks.\nOne of them is ideas, and one of them is your  "
        }
      ]
    },
    {
      "tStartMs": 2267200,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "ability to bring them to life, which \nmight be compute but also engineering. "
        }
      ]
    },
    {
      "tStartMs": 2272240,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "If you go back to the '90s, let's say, \nyou had people who had pretty good ideas,  "
        }
      ]
    },
    {
      "tStartMs": 2276880,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "and if they had much larger computers, maybe they \ncould demonstrate that their ideas were viable. "
        }
      ]
    },
    {
      "tStartMs": 2281120,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "But they could not, so they could only \nhave a very, very small demonstration  "
        }
      ]
    },
    {
      "tStartMs": 2285200,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "that did not convince anyone. So the \nbottleneck was compute. Then in the  "
        }
      ]
    },
    {
      "tStartMs": 2290480,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "age of scaling, compute has increased a lot.\nOf course, there is a question of how much  "
        }
      ]
    },
    {
      "tStartMs": 2297120,
      "dDurationMs": 9520,
      "segs": [
        {
          "utf8": "compute is needed, but compute is large.\nCompute is large enough such that it's not  "
        }
      ]
    },
    {
      "tStartMs": 2306640,
      "dDurationMs": 6880,
      "segs": [
        {
          "utf8": "obvious that you need that much more \ncompute to prove some idea. I'll give  "
        }
      ]
    },
    {
      "tStartMs": 2313520,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "you an analogy. AlexNet was built on two GPUs.\nThat was the total amount of compute used for it. "
        }
      ]
    },
    {
      "tStartMs": 2320640,
      "dDurationMs": 8080,
      "segs": [
        {
          "utf8": "The transformer was built on 8 to 64 GPUs.\nNo single transformer paper experiment used  "
        }
      ]
    },
    {
      "tStartMs": 2328720,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "more than 64 GPUs of 2017, which would be \nlike, what, two GPUs of today? The ResNet,  "
        }
      ]
    },
    {
      "tStartMs": 2337200,
      "dDurationMs": 10800,
      "segs": [
        {
          "utf8": "right? You could argue that the o1 reasoning was \nnot the most compute-heavy thing in the world. "
        }
      ]
    },
    {
      "tStartMs": 2348000,
      "dDurationMs": 9200,
      "segs": [
        {
          "utf8": "So for research, you definitely need \nsome amount of compute, but it's far  "
        }
      ]
    },
    {
      "tStartMs": 2357200,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "from obvious that you need the absolutely \nlargest amount of compute ever for research. "
        }
      ]
    },
    {
      "tStartMs": 2362160,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "You might argue, and I think it is true, that \nif you want to build the absolutely best system  "
        }
      ]
    },
    {
      "tStartMs": 2371360,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "then it helps to have much more compute.\nEspecially if everyone is within the same  "
        }
      ]
    },
    {
      "tStartMs": 2375360,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "paradigm, then compute becomes \none of the big differentiators. "
        }
      ]
    },
    {
      "tStartMs": 2386400,
      "dDurationMs": 2000,
      "segs": [
        {
          "utf8": "I'm asking you for the history, \nbecause you were actually there. "
        }
      ]
    },
    {
      "tStartMs": 2388400,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "I'm not sure what actually happened.\nIt sounds like it was possible to develop  "
        }
      ]
    },
    {
      "tStartMs": 2391680,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "these ideas using minimal amounts of compute.\nBut the transformer didn't  "
        }
      ]
    },
    {
      "tStartMs": 2396480,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "immediately become famous.\nIt became the thing everybody started  "
        }
      ]
    },
    {
      "tStartMs": 2399840,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "doing and then started experimenting on top of \nand building on top of because it was validated  "
        }
      ]
    },
    {
      "tStartMs": 2404320,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "at higher and higher levels of compute.\nCorrect. "
        }
      ]
    },
    {
      "tStartMs": 2407360,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "And if you at SSI have 50 different ideas, how \nwill you know which one is the next transformer  "
        }
      ]
    },
    {
      "tStartMs": 2413840,
      "dDurationMs": 8080,
      "segs": [
        {
          "utf8": "and which one is brittle, without having the \nkinds of compute that other frontier labs have? "
        }
      ]
    },
    {
      "tStartMs": 2422960,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "I can comment on that. The short \ncomment is that you mentioned SSI. "
        }
      ]
    },
    {
      "tStartMs": 2430320,
      "dDurationMs": 10320,
      "segs": [
        {
          "utf8": "Specifically for us, the amount of compute \nthat SSI has for research is really not that  "
        }
      ]
    },
    {
      "tStartMs": 2440640,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "small. I want to explain why. Simple math \ncan explain why the amount of compute that  "
        }
      ]
    },
    {
      "tStartMs": 2445200,
      "dDurationMs": 13680,
      "segs": [
        {
          "utf8": "we have is comparable for research than one might \nthink. I'll explain. SSI has raised $3 billion,  "
        }
      ]
    },
    {
      "tStartMs": 2458880,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "which is a lot by any absolute sense.\nBut you could say, \"Look at the  "
        }
      ]
    },
    {
      "tStartMs": 2465440,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "other companies raising much more.\"\nBut a lot of their compute goes for inference. "
        }
      ]
    },
    {
      "tStartMs": 2473920,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "These big numbers, these big loans, it's \nearmarked for inference. That's number one.  "
        }
      ]
    },
    {
      "tStartMs": 2480160,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "Number two, if you want to have a product \non which you do inference, you need to  "
        }
      ]
    },
    {
      "tStartMs": 2485120,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "have a big staff of engineers, salespeople.\nA lot of the research needs to be dedicated to  "
        }
      ]
    },
    {
      "tStartMs": 2491120,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "producing all kinds of product-related features.\nSo then when you look at what's actually left for  "
        }
      ]
    },
    {
      "tStartMs": 2497440,
      "dDurationMs": 8320,
      "segs": [
        {
          "utf8": "research, the difference becomes a lot smaller.\nThe other thing is, if you are doing something  "
        }
      ]
    },
    {
      "tStartMs": 2505760,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "different, do you really need the \nabsolute maximal scale to prove it? "
        }
      ]
    },
    {
      "tStartMs": 2511040,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "I don't think that's true at all.\nI think that in our case, we have sufficient  "
        }
      ]
    },
    {
      "tStartMs": 2518080,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "compute to prove, to convince ourselves and \nanyone else, that what we are doing is correct. "
        }
      ]
    },
    {
      "tStartMs": 2522880,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "There have been public estimates that companies \nlike OpenAI spend on the order of $5-6 billion  "
        }
      ]
    },
    {
      "tStartMs": 2528320,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "a year just so far, on experiments.\nThis is separate from the amount of  "
        }
      ]
    },
    {
      "tStartMs": 2533680,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "money they're spending on inference and so forth.\nSo it seems like they're spending more a year  "
        }
      ]
    },
    {
      "tStartMs": 2538720,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "running research experiments than \nyou guys have in total funding. "
        }
      ]
    },
    {
      "tStartMs": 2542960,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "I think it's a question of what you do with it.\nIt's a question of what you do with it. "
        }
      ]
    },
    {
      "tStartMs": 2549840,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "In their case, in the case of others, there \nis a lot more demand on the training compute. "
        }
      ]
    },
    {
      "tStartMs": 2555120,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "There’s a lot more different work streams, there \nare different modalities, there is just more  "
        }
      ]
    },
    {
      "tStartMs": 2561120,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "stuff. So it becomes fragmented.\nHow will SSI make money? "
        }
      ]
    },
    {
      "tStartMs": 2568480,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "My answer to this question is something like this.\nRight now, we just focus on the research, and then  "
        }
      ]
    },
    {
      "tStartMs": 2575680,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "the answer to that question will reveal itself.\nI think there will be lots of possible answers. "
        }
      ]
    },
    {
      "tStartMs": 2581360,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "Is SSI's plan still to straight \nshot superintelligence? "
        }
      ]
    },
    {
      "tStartMs": 2585040,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "Maybe. I think that there is merit to it.\nI think there's a lot of merit because  "
        }
      ]
    },
    {
      "tStartMs": 2591280,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "it's very nice to not be affected by \nthe day-to-day market competition. "
        }
      ]
    },
    {
      "tStartMs": 2597840,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "But I think there are two reasons \nthat may cause us to change the plan. "
        }
      ]
    },
    {
      "tStartMs": 2605200,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "One is pragmatic, if timelines turned \nout to be long, which they might. "
        }
      ]
    },
    {
      "tStartMs": 2611360,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "Second, I think there is a lot \nof value in the best and most  "
        }
      ]
    },
    {
      "tStartMs": 2618400,
      "dDurationMs": 8080,
      "segs": [
        {
          "utf8": "powerful AI being out there impacting the world.\nI think this is a meaningfully valuable thing. "
        }
      ]
    },
    {
      "tStartMs": 2626480,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "So then why is your default plan \nto straight shot superintelligence? "
        }
      ]
    },
    {
      "tStartMs": 2629440,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "Because it sounds like OpenAI, Anthropic, all \nthese other companies, their explicit thinking  "
        }
      ]
    },
    {
      "tStartMs": 2634880,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "is, \"Look, we have weaker and weaker intelligences \nthat the public can get used to and prepare for.\" "
        }
      ]
    },
    {
      "tStartMs": 2641040,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "Why is it potentially better to \nbuild a superintelligence directly? "
        }
      ]
    },
    {
      "tStartMs": 2646400,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "I'll make the case for and against.\nThe case for is that one of the challenges  "
        }
      ]
    },
    {
      "tStartMs": 2654000,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "that people face when they're in the market is \nthat they have to participate in the rat race. "
        }
      ]
    },
    {
      "tStartMs": 2660320,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "The rat race is quite difficult in \nthat it exposes you to difficult  "
        }
      ]
    },
    {
      "tStartMs": 2664720,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "trade-offs which you need to make.\nIt is nice to say, \"We'll insulate ourselves  "
        }
      ]
    },
    {
      "tStartMs": 2672000,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "from all this and just focus on the research and \ncome out only when we are ready, and not before.\" "
        }
      ]
    },
    {
      "tStartMs": 2678160,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "But the counterpoint is valid too, \nand those are opposing forces. "
        }
      ]
    },
    {
      "tStartMs": 2683600,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "The counterpoint is, \"Hey, it is useful \nfor the world to see powerful AI. "
        }
      ]
    },
    {
      "tStartMs": 2690880,
      "dDurationMs": 2720,
      "segs": [
        {
          "utf8": "It is useful for the world to \nsee powerful AI because that's  "
        }
      ]
    },
    {
      "tStartMs": 2693600,
      "dDurationMs": 3200,
      "segs": [
        {
          "utf8": "the only way you can communicate it.\"\nWell, I guess not even just that you can  "
        }
      ]
    },
    {
      "tStartMs": 2696800,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "communicate the idea—\nCommunicate the AI,  "
        }
      ]
    },
    {
      "tStartMs": 2699760,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "not the idea. Communicate the AI.\nWhat do you mean, \"communicate the AI\"? "
        }
      ]
    },
    {
      "tStartMs": 2704640,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "Let's suppose you write an essay about AI, and \nthe essay says, \"AI is going to be this, and AI is  "
        }
      ]
    },
    {
      "tStartMs": 2710080,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "going to be that, and it's going to be this.\"\nYou read it and you say, \"Okay,  "
        }
      ]
    },
    {
      "tStartMs": 2713760,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "this is an interesting essay.\"\nNow suppose you see an AI doing this,  "
        }
      ]
    },
    {
      "tStartMs": 2718320,
      "dDurationMs": 9600,
      "segs": [
        {
          "utf8": "an AI doing that. It is incomparable. Basically \nI think that there is a big benefit from AI  "
        }
      ]
    },
    {
      "tStartMs": 2727920,
      "dDurationMs": 7680,
      "segs": [
        {
          "utf8": "being in the public, and that would be a \nreason for us to not be quite straight shot. "
        }
      ]
    },
    {
      "tStartMs": 2736320,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "I guess it's not even that, but I do \nthink that is an important part of it. "
        }
      ]
    },
    {
      "tStartMs": 2740320,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "The other big thing is that I can't think of \nanother discipline in human engineering and  "
        }
      ]
    },
    {
      "tStartMs": 2745360,
      "dDurationMs": 7920,
      "segs": [
        {
          "utf8": "research where the end artifact was made \nsafer mostly through just thinking about  "
        }
      ]
    },
    {
      "tStartMs": 2753280,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "how to make it safe, as opposed to, \nwhy airplane crashes per mile are so  "
        }
      ]
    },
    {
      "tStartMs": 2758000,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "much lower today than they were decades ago.\nWhy is it so much harder to find a bug in Linux  "
        }
      ]
    },
    {
      "tStartMs": 2762560,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "than it would have been decades ago?\nI think it's mostly because these  "
        }
      ]
    },
    {
      "tStartMs": 2766320,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "systems were deployed to the world.\nYou noticed failures, those failures  "
        }
      ]
    },
    {
      "tStartMs": 2771040,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "were corrected and the systems became more robust.\nI'm not sure why AGI and superhuman intelligence  "
        }
      ]
    },
    {
      "tStartMs": 2777280,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "would be any different, especially given—and I \nhope we're going to get to this—it seems like  "
        }
      ]
    },
    {
      "tStartMs": 2783600,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "the harms of superintelligence are not just about \nhaving some malevolent paper clipper out there. "
        }
      ]
    },
    {
      "tStartMs": 2789760,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "But this is a really powerful thing and we don't \neven know how to conceptualize how people interact  "
        }
      ]
    },
    {
      "tStartMs": 2794640,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "with it, what people will do with it.\nHaving gradual access to it seems like a  "
        }
      ]
    },
    {
      "tStartMs": 2800880,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "better way to maybe spread out the impact \nof it and to help people prepare for it. "
        }
      ]
    },
    {
      "tStartMs": 2805440,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "Well I think on this point, even in the straight \nshot scenario, you would still do a gradual  "
        }
      ]
    },
    {
      "tStartMs": 2812720,
      "dDurationMs": 9040,
      "segs": [
        {
          "utf8": "release of it, that’s how I would imagine it.\nGradualism would be an inherent  "
        }
      ]
    },
    {
      "tStartMs": 2821760,
      "dDurationMs": 2800,
      "segs": [
        {
          "utf8": "component of any plan.\nIt's just a question of what is the first  "
        }
      ]
    },
    {
      "tStartMs": 2824560,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "thing that you get out of the door. That's number \none. Number two, I believe you have advocated  "
        }
      ]
    },
    {
      "tStartMs": 2831680,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "for continual learning more than other people, \nand I actually think that this is an important  "
        }
      ]
    },
    {
      "tStartMs": 2837680,
      "dDurationMs": 11920,
      "segs": [
        {
          "utf8": "and correct thing. Here is why. I'll give you \nanother example of how language affects thinking. "
        }
      ]
    },
    {
      "tStartMs": 2849600,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "In this case, it will be two words that \nhave shaped everyone's thinking, I maintain.  "
        }
      ]
    },
    {
      "tStartMs": 2857520,
      "dDurationMs": 10880,
      "segs": [
        {
          "utf8": "First word: AGI. Second word: pre-training. \nLet me explain. The term AGI, why does this  "
        }
      ]
    },
    {
      "tStartMs": 2868400,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "term exist? It's a very particular term. Why \ndoes it exist? There's a reason. The reason  "
        }
      ]
    },
    {
      "tStartMs": 2875120,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "that the term AGI exists is, in my opinion, not \nso much because it's a very important, essential  "
        }
      ]
    },
    {
      "tStartMs": 2882640,
      "dDurationMs": 11520,
      "segs": [
        {
          "utf8": "descriptor of some end state of intelligence, but \nbecause it is a reaction to a different term that  "
        }
      ]
    },
    {
      "tStartMs": 2894160,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "existed, and the term is narrow AI.\nIf you go back to ancient history  "
        }
      ]
    },
    {
      "tStartMs": 2899360,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "of gameplay and AI, of checkers AI, chess \nAI, computer games AI, everyone would say,  "
        }
      ]
    },
    {
      "tStartMs": 2905600,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "look at this narrow intelligence.\nSure, the chess AI can beat Kasparov,  "
        }
      ]
    },
    {
      "tStartMs": 2909600,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "but it can't do anything else.\nIt is so narrow, artificial narrow intelligence. "
        }
      ]
    },
    {
      "tStartMs": 2914720,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "So in response, as a reaction to this, \nsome people said, this is not good. It  "
        }
      ]
    },
    {
      "tStartMs": 2921280,
      "dDurationMs": 9520,
      "segs": [
        {
          "utf8": "is so narrow. What we need is general AI, \nan AI that can just do all the things. "
        }
      ]
    },
    {
      "tStartMs": 2933040,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "That term just got a lot of traction.\nThe second thing that got a lot of traction  "
        }
      ]
    },
    {
      "tStartMs": 2939360,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "is pre-training, specifically \nthe recipe of pre-training. "
        }
      ]
    },
    {
      "tStartMs": 2943120,
      "dDurationMs": 9840,
      "segs": [
        {
          "utf8": "I think the way people do RL now is maybe \nundoing the conceptual imprint of pre-training.  "
        }
      ]
    },
    {
      "tStartMs": 2952960,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "But pre-training had this property. You \ndo more pre-training and the model gets  "
        }
      ]
    },
    {
      "tStartMs": 2957520,
      "dDurationMs": 12240,
      "segs": [
        {
          "utf8": "better at everything, more or less uniformly. \nGeneral AI. Pre-training gives AGI. But the  "
        }
      ]
    },
    {
      "tStartMs": 2969760,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "thing that happened with AGI and pre-training \nis that in some sense they overshot the target. "
        }
      ]
    },
    {
      "tStartMs": 2978160,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "If you think about the term \"AGI\", \nespecially in the context of pre-training,  "
        }
      ]
    },
    {
      "tStartMs": 2983360,
      "dDurationMs": 10400,
      "segs": [
        {
          "utf8": "you will realize that a human being is not an AGI.\nYes, there is definitely a foundation of skills,  "
        }
      ]
    },
    {
      "tStartMs": 2993760,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "but a human being lacks a \nhuge amount of knowledge. "
        }
      ]
    },
    {
      "tStartMs": 3000080,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "Instead, we rely on continual learning.\nSo when you think about, \"Okay,  "
        }
      ]
    },
    {
      "tStartMs": 3006720,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "so let's suppose that we achieve success and we \nproduce some kind of safe superintelligence.\" "
        }
      ]
    },
    {
      "tStartMs": 3012240,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "The question is, how do you define it?\nWhere on the curve of continual  "
        }
      ]
    },
    {
      "tStartMs": 3016320,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "learning is it going to be?\nI produce a superintelligent  "
        }
      ]
    },
    {
      "tStartMs": 3020640,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "15-year-old that's very eager to go.\nThey don't know very much at all,  "
        }
      ]
    },
    {
      "tStartMs": 3025200,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "a great student, very eager.\nYou go and be a programmer,  "
        }
      ]
    },
    {
      "tStartMs": 3029280,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "you go and be a doctor, go and learn.\nSo you could imagine that the deployment  "
        }
      ]
    },
    {
      "tStartMs": 3034720,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "itself will involve some kind of \na learning trial-and-error period. "
        }
      ]
    },
    {
      "tStartMs": 3038880,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "It's a process, as opposed to \nyou dropping the finished thing. "
        }
      ]
    },
    {
      "tStartMs": 3044240,
      "dDurationMs": 6880,
      "segs": [
        {
          "utf8": "I see. You're suggesting that the thing \nyou're pointing out with superintelligence  "
        }
      ]
    },
    {
      "tStartMs": 3051120,
      "dDurationMs": 7440,
      "segs": [
        {
          "utf8": "is not some finished mind which knows how \nto do every single job in the economy. "
        }
      ]
    },
    {
      "tStartMs": 3058560,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "Because the way, say, the original OpenAI charter \nor whatever defines AGI is like, it can do every  "
        }
      ]
    },
    {
      "tStartMs": 3065120,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "single job, every single thing a human can do.\nYou're proposing instead a mind which can  "
        }
      ]
    },
    {
      "tStartMs": 3071680,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "learn to do every single job, \nand that is superintelligence. "
        }
      ]
    },
    {
      "tStartMs": 3075520,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "Yes.\nBut once you have the learning algorithm,  "
        }
      ]
    },
    {
      "tStartMs": 3079840,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "it gets deployed into the world the same way \na human laborer might join an organization. "
        }
      ]
    },
    {
      "tStartMs": 3085200,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "Exactly.\nIt seems like one of these two things  "
        }
      ]
    },
    {
      "tStartMs": 3087360,
      "dDurationMs": 8080,
      "segs": [
        {
          "utf8": "might happen, maybe neither of these happens.\nOne, this super-efficient learning algorithm  "
        }
      ]
    },
    {
      "tStartMs": 3095440,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "becomes superhuman, becomes as good \nas you and potentially even better,  "
        }
      ]
    },
    {
      "tStartMs": 3100400,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "at the task of ML research.\nAs a result the algorithm  "
        }
      ]
    },
    {
      "tStartMs": 3105920,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "itself becomes more and more superhuman.\nThe other is, even if that doesn't happen,  "
        }
      ]
    },
    {
      "tStartMs": 3110560,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "if you have a single model—this is explicitly \nyour vision—where instances of a model  "
        }
      ]
    },
    {
      "tStartMs": 3116800,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "which are deployed through the economy doing \ndifferent jobs, learning how to do those jobs,  "
        }
      ]
    },
    {
      "tStartMs": 3120800,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "continually learning on the job, picking up \nall the skills that any human could pick up,  "
        }
      ]
    },
    {
      "tStartMs": 3125520,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "but picking them all up at the same time, \nand then amalgamating their learnings,  "
        }
      ]
    },
    {
      "tStartMs": 3130480,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "you basically have a model which functionally \nbecomes superintelligent even without any sort  "
        }
      ]
    },
    {
      "tStartMs": 3135440,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "of recursive self-improvement in software.\nBecause you now have one model that can do  "
        }
      ]
    },
    {
      "tStartMs": 3140560,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "every single job in the economy and humans \ncan't merge our minds in the same way. "
        }
      ]
    },
    {
      "tStartMs": 3145040,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "So do you expect some sort of intelligence \nexplosion from broad deployment? "
        }
      ]
    },
    {
      "tStartMs": 3148640,
      "dDurationMs": 8640,
      "segs": [
        {
          "utf8": "I think that it is likely that we \nwill have rapid economic growth. "
        }
      ]
    },
    {
      "tStartMs": 3157280,
      "dDurationMs": 9360,
      "segs": [
        {
          "utf8": "I think with broad deployment, there are two \narguments you could make which are conflicting. "
        }
      ]
    },
    {
      "tStartMs": 3166640,
      "dDurationMs": 12480,
      "segs": [
        {
          "utf8": "One is that once indeed you get to a point where \nyou have an AI that can learn to do things quickly  "
        }
      ]
    },
    {
      "tStartMs": 3179120,
      "dDurationMs": 8160,
      "segs": [
        {
          "utf8": "and you have many of them, then there will be \na strong force to deploy them in the economy  "
        }
      ]
    },
    {
      "tStartMs": 3187280,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "unless there will be some kind of a regulation \nthat stops it, which by the way there might be. "
        }
      ]
    },
    {
      "tStartMs": 3193360,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "But the idea of very rapid \neconomic growth for some time,  "
        }
      ]
    },
    {
      "tStartMs": 3199120,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "I think it’s very possible from broad deployment.\nThe question is how rapid it's going to be. "
        }
      ]
    },
    {
      "tStartMs": 3205440,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "I think this is hard to know because on the \none hand you have this very efficient worker. "
        }
      ]
    },
    {
      "tStartMs": 3210720,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "On the other hand, the world is just \nreally big and there's a lot of stuff,  "
        }
      ]
    },
    {
      "tStartMs": 3216160,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "and that stuff moves at a different speed.\nBut then on the other hand, now the AI could…  "
        }
      ]
    },
    {
      "tStartMs": 3221600,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "So I think very rapid economic growth is possible.\nWe will see all kinds of things like different  "
        }
      ]
    },
    {
      "tStartMs": 3227920,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "countries with different rules and the \nones which have the friendlier rules, the  "
        }
      ]
    },
    {
      "tStartMs": 3231280,
      "dDurationMs": 79040,
      "segs": [
        {
          "utf8": "economic growth will be faster. Hard to predict.\nIt seems to me that this is a very precarious  "
        }
      ]
    },
    {
      "tStartMs": 3310320,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "situation to be in.\nIn the limit,  "
        }
      ]
    },
    {
      "tStartMs": 3314080,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "we know that this should be possible.\nIf you have something that is as good  "
        }
      ]
    },
    {
      "tStartMs": 3317760,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "as a human at learning, but which can merge its \nbrains—merge different instances in a way that  "
        }
      ]
    },
    {
      "tStartMs": 3324400,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "humans can't merge—already, this seems like \na thing that should physically be possible. "
        }
      ]
    },
    {
      "tStartMs": 3328320,
      "dDurationMs": 2480,
      "segs": [
        {
          "utf8": "Humans are possible, digital \ncomputers are possible. "
        }
      ]
    },
    {
      "tStartMs": 3330800,
      "dDurationMs": 2640,
      "segs": [
        {
          "utf8": "You just need both of those \ncombined to produce this thing. "
        }
      ]
    },
    {
      "tStartMs": 3333440,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "It also seems this kind of \nthing is extremely powerful. "
        }
      ]
    },
    {
      "tStartMs": 3341440,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "Economic growth is one way to put it.\nA Dyson sphere is a lot of economic growth. "
        }
      ]
    },
    {
      "tStartMs": 3345680,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "But another way to put it is that you will have, \nin potentially a very short period of time... "
        }
      ]
    },
    {
      "tStartMs": 3352080,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "You hire people at SSI, and in six \nmonths, they're net productive, probably. "
        }
      ]
    },
    {
      "tStartMs": 3356000,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "A human learns really fast, and this thing \nis becoming smarter and smarter very fast. "
        }
      ]
    },
    {
      "tStartMs": 3361200,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "How do you think about making that go well?\nWhy is SSI positioned to do that well? "
        }
      ]
    },
    {
      "tStartMs": 3365840,
      "dDurationMs": 2320,
      "segs": [
        {
          "utf8": "What is SSI's plan there, is \nbasically what I'm trying to ask. "
        }
      ]
    },
    {
      "tStartMs": 3372160,
      "dDurationMs": 10720,
      "segs": [
        {
          "utf8": "One of the ways in which my thinking has been \nchanging is that I now place more importance on  "
        }
      ]
    },
    {
      "tStartMs": 3382880,
      "dDurationMs": 12080,
      "segs": [
        {
          "utf8": "AI being deployed incrementally and in advance.\nOne very difficult thing about AI is that we are  "
        }
      ]
    },
    {
      "tStartMs": 3394960,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "talking about systems that don't yet \nexist and it's hard to imagine them. "
        }
      ]
    },
    {
      "tStartMs": 3403600,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "I think that one of the things that's happening is \nthat in practice, it's very hard to feel the AGI. "
        }
      ]
    },
    {
      "tStartMs": 3412080,
      "dDurationMs": 9360,
      "segs": [
        {
          "utf8": "It's very hard to feel the AGI.\nWe can talk about it, but imagine  "
        }
      ]
    },
    {
      "tStartMs": 3421440,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "having a conversation about how it is \nlike to be old when you're old and frail. "
        }
      ]
    },
    {
      "tStartMs": 3427840,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "You can have a conversation, you can try to \nimagine it, but it's just hard, and you come  "
        }
      ]
    },
    {
      "tStartMs": 3432640,
      "dDurationMs": 9760,
      "segs": [
        {
          "utf8": "back to reality where that's not the case.\nI think that a lot of the issues around AGI  "
        }
      ]
    },
    {
      "tStartMs": 3442400,
      "dDurationMs": 8560,
      "segs": [
        {
          "utf8": "and its future power stem from the fact \nthat it's very difficult to imagine. "
        }
      ]
    },
    {
      "tStartMs": 3450960,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "Future AI is going to be different. It's going \nto be powerful. Indeed, the whole problem,  "
        }
      ]
    },
    {
      "tStartMs": 3457680,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "what is the problem of AI and AGI?\nThe whole problem is the power. "
        }
      ]
    },
    {
      "tStartMs": 3463360,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "The whole problem is the power.\nWhen the power is really big,  "
        }
      ]
    },
    {
      "tStartMs": 3468320,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "what's going to happen?\nOne of the ways in which I've  "
        }
      ]
    },
    {
      "tStartMs": 3473120,
      "dDurationMs": 9200,
      "segs": [
        {
          "utf8": "changed my mind over the past year—and that \nchange of mind, I'll hedge a little bit, may  "
        }
      ]
    },
    {
      "tStartMs": 3482320,
      "dDurationMs": 10640,
      "segs": [
        {
          "utf8": "back-propagate into the plans of our company—is \nthat if it's hard to imagine, what do you do? "
        }
      ]
    },
    {
      "tStartMs": 3492960,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "You’ve got to be showing the thing.\nYou’ve got to be showing the thing. "
        }
      ]
    },
    {
      "tStartMs": 3496560,
      "dDurationMs": 7840,
      "segs": [
        {
          "utf8": "I maintain that most people who work on AI also \ncan't imagine it because it's too different from  "
        }
      ]
    },
    {
      "tStartMs": 3504400,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "what people see on a day-to-day basis.\nI do maintain, here's something which  "
        }
      ]
    },
    {
      "tStartMs": 3511440,
      "dDurationMs": 9520,
      "segs": [
        {
          "utf8": "I predict will happen. This is a prediction. \nI maintain that as AI becomes more powerful,  "
        }
      ]
    },
    {
      "tStartMs": 3520960,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "people will change their behaviors.\nWe will see all kinds of unprecedented  "
        }
      ]
    },
    {
      "tStartMs": 3528320,
      "dDurationMs": 8400,
      "segs": [
        {
          "utf8": "things which are not happening right now. I’ll \ngive some examples. I think for better or worse,  "
        }
      ]
    },
    {
      "tStartMs": 3537440,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "the frontier companies will play a very important \nrole in what happens, as will the government. "
        }
      ]
    },
    {
      "tStartMs": 3543120,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "The kind of things that I think \nyou'll see, which you see the  "
        }
      ]
    },
    {
      "tStartMs": 3546800,
      "dDurationMs": 9120,
      "segs": [
        {
          "utf8": "beginnings of, are companies that are fierce \ncompetitors starting to collaborate on AI safety. "
        }
      ]
    },
    {
      "tStartMs": 3555920,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "You may have seen OpenAI and Anthropic doing \na first small step, but that did not exist. "
        }
      ]
    },
    {
      "tStartMs": 3562880,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "That's something which I predicted in \none of my talks about three years ago,  "
        }
      ]
    },
    {
      "tStartMs": 3567360,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "that such a thing will happen.\nI also maintain that as AI continues  "
        }
      ]
    },
    {
      "tStartMs": 3570960,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "to become more powerful, more visibly \npowerful, there will also be a desire from  "
        }
      ]
    },
    {
      "tStartMs": 3578320,
      "dDurationMs": 7840,
      "segs": [
        {
          "utf8": "governments and the public to do something.\nI think this is a very important force,  "
        }
      ]
    },
    {
      "tStartMs": 3586160,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "of showing the AI. That's number one. \nNumber two, okay, so the AI is being  "
        }
      ]
    },
    {
      "tStartMs": 3591600,
      "dDurationMs": 8000,
      "segs": [
        {
          "utf8": "built. What needs to be done? One thing that \nI maintain that will happen is that right now,  "
        }
      ]
    },
    {
      "tStartMs": 3599600,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "people who are working on AI, I maintain that the \nAI doesn't feel powerful because of its mistakes. "
        }
      ]
    },
    {
      "tStartMs": 3606560,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "I do think that at some point the AI \nwill start to feel powerful actually. "
        }
      ]
    },
    {
      "tStartMs": 3610880,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "I think when that happens, we will see a big \nchange in the way all AI companies approach  "
        }
      ]
    },
    {
      "tStartMs": 3618160,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "safety. They'll become much more paranoid. \nI say this as a prediction that we will  "
        }
      ]
    },
    {
      "tStartMs": 3625680,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "see happen. We'll see if I'm right. But I think \nthis is something that will happen because they  "
        }
      ]
    },
    {
      "tStartMs": 3630320,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "will see the AI becoming more powerful.\nEverything that's happening right now,  "
        }
      ]
    },
    {
      "tStartMs": 3634160,
      "dDurationMs": 7920,
      "segs": [
        {
          "utf8": "I maintain, is because people look at today's \nAI and it's hard to imagine the future AI. "
        }
      ]
    },
    {
      "tStartMs": 3642640,
      "dDurationMs": 6880,
      "segs": [
        {
          "utf8": "There is a third thing which needs to happen.\nI'm talking about it in broader terms,  "
        }
      ]
    },
    {
      "tStartMs": 3649520,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "not just from the perspective of SSI \nbecause you asked me about our company. "
        }
      ]
    },
    {
      "tStartMs": 3654640,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "The question is, what should \nthe companies aspire to build? "
        }
      ]
    },
    {
      "tStartMs": 3658400,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "What should they aspire to build?\nThere has been one big idea that  "
        }
      ]
    },
    {
      "tStartMs": 3664000,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "everyone has been locked into, which is \nthe self-improving AI. Why did it happen?  "
        }
      ]
    },
    {
      "tStartMs": 3671120,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "Because there are fewer ideas than companies.\nBut I maintain that there is something that's  "
        }
      ]
    },
    {
      "tStartMs": 3677200,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "better to build, and I think \nthat everyone will want that. "
        }
      ]
    },
    {
      "tStartMs": 3681360,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "It's the AI that's robustly aligned to \ncare about sentient life specifically. "
        }
      ]
    },
    {
      "tStartMs": 3689680,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "I think in particular, there's a case to \nbe made that it will be easier to build  "
        }
      ]
    },
    {
      "tStartMs": 3695280,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "an AI that cares about sentient life than \nan AI that cares about human life alone,  "
        }
      ]
    },
    {
      "tStartMs": 3700640,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "because the AI itself will be sentient.\nAnd if you think about things like mirror  "
        }
      ]
    },
    {
      "tStartMs": 3706080,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "neurons and human empathy for animals, which you \nmight argue it's not big enough, but it exists. "
        }
      ]
    },
    {
      "tStartMs": 3713200,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "I think it's an emergent property from \nthe fact that we model others with the  "
        }
      ]
    },
    {
      "tStartMs": 3718240,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "same circuit that we use to model ourselves, \nbecause that's the most efficient thing to do. "
        }
      ]
    },
    {
      "tStartMs": 3723680,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "So even if you got an AI to care about \nsentient beings—and it's not actually  "
        }
      ]
    },
    {
      "tStartMs": 3728880,
      "dDurationMs": 1840,
      "segs": [
        {
          "utf8": "clear to me that that's what you \nshould try to do if you solved  "
        }
      ]
    },
    {
      "tStartMs": 3730720,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "alignment—it would still be the case \nthat most sentient beings will be AIs. "
        }
      ]
    },
    {
      "tStartMs": 3736480,
      "dDurationMs": 2880,
      "segs": [
        {
          "utf8": "There will be trillions, \neventually quadrillions, of AIs. "
        }
      ]
    },
    {
      "tStartMs": 3739360,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "Humans will be a very small \nfraction of sentient beings. "
        }
      ]
    },
    {
      "tStartMs": 3743280,
      "dDurationMs": 9200,
      "segs": [
        {
          "utf8": "So it's not clear to me if the goal is some kind \nof human control over this future civilization,  "
        }
      ]
    },
    {
      "tStartMs": 3752480,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "that this is the best criterion.\nIt's true. It's possible it's not  "
        }
      ]
    },
    {
      "tStartMs": 3759680,
      "dDurationMs": 13440,
      "segs": [
        {
          "utf8": "the best criterion. I'll say two things. Number \none, care for sentient life, I think there is  "
        }
      ]
    },
    {
      "tStartMs": 3773120,
      "dDurationMs": 8000,
      "segs": [
        {
          "utf8": "merit to it. It should be considered. I think it \nwould be helpful if there was some kind of short  "
        }
      ]
    },
    {
      "tStartMs": 3781120,
      "dDurationMs": 9120,
      "segs": [
        {
          "utf8": "list of ideas that the companies, when they are \nin this situation, could use. That’s number two.  "
        }
      ]
    },
    {
      "tStartMs": 3790240,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "Number three, I think it would be really \nmaterially helpful if the power of the  "
        }
      ]
    },
    {
      "tStartMs": 3796480,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "most powerful superintelligence was somehow capped \nbecause it would address a lot of these concerns. "
        }
      ]
    },
    {
      "tStartMs": 3803680,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "The question of how to do it, I'm not sure, but I \nthink that would be materially helpful when you're  "
        }
      ]
    },
    {
      "tStartMs": 3809680,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "talking about really, really powerful systems.\nBefore we continue the alignment discussion,  "
        }
      ]
    },
    {
      "tStartMs": 3815360,
      "dDurationMs": 3200,
      "segs": [
        {
          "utf8": "I want to double-click on that.\nHow much room is there at the top? "
        }
      ]
    },
    {
      "tStartMs": 3818560,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "How do you think about superintelligence?\nDo you think, using this learning efficiency idea,  "
        }
      ]
    },
    {
      "tStartMs": 3824400,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "maybe it is just extremely fast at \nlearning new skills or new knowledge? "
        }
      ]
    },
    {
      "tStartMs": 3828880,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "Does it just have a bigger pool of strategies?\nIs there a single cohesive \"it\" in the  "
        }
      ]
    },
    {
      "tStartMs": 3834320,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "center that's more powerful or bigger?\nIf so, do you imagine that this will be  "
        }
      ]
    },
    {
      "tStartMs": 3841600,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "sort of godlike in comparison to the rest of human \ncivilization, or does it just feel like another  "
        }
      ]
    },
    {
      "tStartMs": 3845120,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "agent, or another cluster of agents?\nThis is an area where different  "
        }
      ]
    },
    {
      "tStartMs": 3850000,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "people have different intuitions.\nI think it will be very powerful, for sure. "
        }
      ]
    },
    {
      "tStartMs": 3856240,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "What I think is most likely to happen \nis that there will be multiple such  "
        }
      ]
    },
    {
      "tStartMs": 3863200,
      "dDurationMs": 10080,
      "segs": [
        {
          "utf8": "AIs being created roughly at the same time.\nI think that if the cluster is big enough—like  "
        }
      ]
    },
    {
      "tStartMs": 3873280,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "if the cluster is literally continent-sized—that \nthing could be really powerful, indeed. "
        }
      ]
    },
    {
      "tStartMs": 3879040,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "If you literally have a continent-sized \ncluster, those AIs can be very powerful. "
        }
      ]
    },
    {
      "tStartMs": 3886640,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "All I can tell you is that if you're \ntalking about extremely powerful AIs,  "
        }
      ]
    },
    {
      "tStartMs": 3891680,
      "dDurationMs": 8080,
      "segs": [
        {
          "utf8": "truly dramatically powerful, it would be nice if \nthey could be restrained in some ways or if there  "
        }
      ]
    },
    {
      "tStartMs": 3899760,
      "dDurationMs": 11440,
      "segs": [
        {
          "utf8": "were some kind of agreement or something.\nWhat is the concern of superintelligence? "
        }
      ]
    },
    {
      "tStartMs": 3911200,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "What is one way to explain the concern?\nIf you imagine a system that is sufficiently  "
        }
      ]
    },
    {
      "tStartMs": 3916800,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "powerful, really sufficiently powerful—and you \ncould say you need to do something sensible like  "
        }
      ]
    },
    {
      "tStartMs": 3923440,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "care for sentient life in a very single-minded \nway—we might not like the results. That's really  "
        }
      ]
    },
    {
      "tStartMs": 3929440,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "what it is. Maybe, by the way, the answer is \nthat you do not build an RL agent in the usual  "
        }
      ]
    },
    {
      "tStartMs": 3935840,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "sense. I'll point several things out. I \nthink human beings are semi-RL agents. "
        }
      ]
    },
    {
      "tStartMs": 3943600,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "We pursue a reward, and then the emotions \nor whatever make us tire out of the  "
        }
      ]
    },
    {
      "tStartMs": 3948160,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "reward and we pursue a different reward.\nThe market is a very short-sighted kind of  "
        }
      ]
    },
    {
      "tStartMs": 3955760,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "agent. Evolution is the same. Evolution \nis very intelligent in some ways,  "
        }
      ]
    },
    {
      "tStartMs": 3959600,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "but very dumb in other ways.\nThe government has been designed  "
        }
      ]
    },
    {
      "tStartMs": 3963040,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "to be a never-ending fight between \nthree parts, which has an effect. "
        }
      ]
    },
    {
      "tStartMs": 3968320,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "So I think things like this.\nAnother thing that makes this discussion  "
        }
      ]
    },
    {
      "tStartMs": 3973120,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "difficult is that we are talking about systems \nthat don't exist, that we don't know how to build. "
        }
      ]
    },
    {
      "tStartMs": 3979600,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "That’s the other thing and \nthat’s actually my belief. "
        }
      ]
    },
    {
      "tStartMs": 3981760,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "I think what people are doing right now \nwill go some distance and then peter out. "
        }
      ]
    },
    {
      "tStartMs": 3986560,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "It will continue to improve, \nbut it will also not be \"it\". "
        }
      ]
    },
    {
      "tStartMs": 3990080,
      "dDurationMs": 8880,
      "segs": [
        {
          "utf8": "The \"It\" we don't know how to build, and \na lot hinges on understanding reliable  "
        }
      ]
    },
    {
      "tStartMs": 3998960,
      "dDurationMs": 8160,
      "segs": [
        {
          "utf8": "generalization. I’ll say another thing. \nOne of the things that you could say about  "
        }
      ]
    },
    {
      "tStartMs": 4007120,
      "dDurationMs": 8640,
      "segs": [
        {
          "utf8": "what causes alignment to be difficult is that \nyour ability to learn human values is fragile. "
        }
      ]
    },
    {
      "tStartMs": 4015760,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "Then your ability to optimize them is fragile.\nYou actually learn to optimize them. "
        }
      ]
    },
    {
      "tStartMs": 4020080,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "And can't you say, \"Are these not all \ninstances of unreliable generalization?\" "
        }
      ]
    },
    {
      "tStartMs": 4026640,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "Why is it that human beings appear \nto generalize so much better? "
        }
      ]
    },
    {
      "tStartMs": 4030080,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "What if generalization was much better?\nWhat would happen in this case? What would  "
        }
      ]
    },
    {
      "tStartMs": 4033360,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "be the effect? But those questions \nare right now still unanswerable. "
        }
      ]
    },
    {
      "tStartMs": 4039200,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "How does one think about what \nAI going well looks like? "
        }
      ]
    },
    {
      "tStartMs": 4044640,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "You've scoped out how AI might evolve.\nWe'll have these sort of continual  "
        }
      ]
    },
    {
      "tStartMs": 4048480,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "learning agents. AI will be very powerful. \nMaybe there will be many different AIs. "
        }
      ]
    },
    {
      "tStartMs": 4053600,
      "dDurationMs": 7200,
      "segs": [
        {
          "utf8": "How do you think about lots of continent-sized \ncompute intelligences going around? How dangerous  "
        }
      ]
    },
    {
      "tStartMs": 4060800,
      "dDurationMs": 9040,
      "segs": [
        {
          "utf8": "is that? How do we make that less dangerous?\nAnd how do we do that in a way that protects an  "
        }
      ]
    },
    {
      "tStartMs": 4069840,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "equilibrium where there might be misaligned \nAIs out there and bad actors out there? "
        }
      ]
    },
    {
      "tStartMs": 4076240,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "Here’s one reason why I liked \"AI \nthat cares for sentient life\". "
        }
      ]
    },
    {
      "tStartMs": 4080960,
      "dDurationMs": 8560,
      "segs": [
        {
          "utf8": "We can debate on whether it's good or bad.\nBut if the first N of these dramatic  "
        }
      ]
    },
    {
      "tStartMs": 4089520,
      "dDurationMs": 8000,
      "segs": [
        {
          "utf8": "systems do care for, love, humanity \nor something, care for sentient life,  "
        }
      ]
    },
    {
      "tStartMs": 4097520,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "obviously this also needs to be achieved. This \nneeds to be achieved. So if this is achieved  "
        }
      ]
    },
    {
      "tStartMs": 4103840,
      "dDurationMs": 9120,
      "segs": [
        {
          "utf8": "by the first N of those systems, then I can \nsee it go well, at least for quite some time. "
        }
      ]
    },
    {
      "tStartMs": 4112960,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "Then there is the question of \nwhat happens in the long run. "
        }
      ]
    },
    {
      "tStartMs": 4116560,
      "dDurationMs": 8400,
      "segs": [
        {
          "utf8": "How do you achieve a long-run equilibrium?\nI think that there, there is an answer as well. "
        }
      ]
    },
    {
      "tStartMs": 4124960,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "I don't like this answer, but \nit needs to be considered. "
        }
      ]
    },
    {
      "tStartMs": 4131760,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "In the long run, you might say, \"Okay, if \nyou have a world where powerful AIs exist,  "
        }
      ]
    },
    {
      "tStartMs": 4137120,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "in the short term, you could say \nyou have universal high income. "
        }
      ]
    },
    {
      "tStartMs": 4141200,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "You have universal high income \nand we're all doing well.\" "
        }
      ]
    },
    {
      "tStartMs": 4144880,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "But what do the Buddhists say? \"Change is the \nonly constant.\" Things change. There is some  "
        }
      ]
    },
    {
      "tStartMs": 4151440,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "kind of government, political structure thing, and \nit changes because these things have a shelf life. "
        }
      ]
    },
    {
      "tStartMs": 4158720,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "Some new government thing comes up and \nit functions, and then after some time  "
        }
      ]
    },
    {
      "tStartMs": 4162560,
      "dDurationMs": 3040,
      "segs": [
        {
          "utf8": "it stops functioning.\nThat's something that  "
        }
      ]
    },
    {
      "tStartMs": 4165600,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "we see happening all the time.\nSo I think for the long-run equilibrium,  "
        }
      ]
    },
    {
      "tStartMs": 4172240,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "one approach is that you could say maybe every \nperson will have an AI that will do their bidding,  "
        }
      ]
    },
    {
      "tStartMs": 4178800,
      "dDurationMs": 2240,
      "segs": [
        {
          "utf8": "and that's good.\nIf that could be  "
        }
      ]
    },
    {
      "tStartMs": 4181040,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "maintained indefinitely, that's true.\nBut the downside with that is then the AI  "
        }
      ]
    },
    {
      "tStartMs": 4187040,
      "dDurationMs": 8800,
      "segs": [
        {
          "utf8": "goes and earns money for the person and advocates \nfor their needs in the political sphere, and maybe  "
        }
      ]
    },
    {
      "tStartMs": 4195840,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "then writes a little report saying, \"Okay, \nhere's what I've done, here's the situation,\"  "
        }
      ]
    },
    {
      "tStartMs": 4199520,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "and the person says, \"Great, keep it up.\"\nBut the person is no longer a participant. "
        }
      ]
    },
    {
      "tStartMs": 4205760,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "Then you can say that's a \nprecarious place to be in. "
        }
      ]
    },
    {
      "tStartMs": 4210480,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "I'm going to preface by saying I don't \nlike this solution, but it is a solution. "
        }
      ]
    },
    {
      "tStartMs": 4219040,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "The solution is if people become \npart-AI with some kind of Neuralink++. "
        }
      ]
    },
    {
      "tStartMs": 4223680,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "Because what will happen as a result is \nthat now the AI understands something,  "
        }
      ]
    },
    {
      "tStartMs": 4227920,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "and we understand it too, because now the \nunderstanding is transmitted wholesale. "
        }
      ]
    },
    {
      "tStartMs": 4234160,
      "dDurationMs": 7760,
      "segs": [
        {
          "utf8": "So now if the AI is in some situation, you \nare involved in that situation yourself fully. "
        }
      ]
    },
    {
      "tStartMs": 4241920,
      "dDurationMs": 7920,
      "segs": [
        {
          "utf8": "I think this is the answer to the equilibrium.\nI wonder if the fact that emotions which were  "
        }
      ]
    },
    {
      "tStartMs": 4249840,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "developed millions—or in many cases, billions—of \nyears ago in a totally different environment are  "
        }
      ]
    },
    {
      "tStartMs": 4256160,
      "dDurationMs": 7360,
      "segs": [
        {
          "utf8": "still guiding our actions so strongly \nis an example of alignment success. "
        }
      ]
    },
    {
      "tStartMs": 4263520,
      "dDurationMs": 8000,
      "segs": [
        {
          "utf8": "To spell out what I mean—I don’t know \nwhether it’s more accurate to call it  "
        }
      ]
    },
    {
      "tStartMs": 4271520,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "a value function or reward function—but the \nbrainstem has a directive where it's saying,  "
        }
      ]
    },
    {
      "tStartMs": 4275760,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "\"Mate with somebody who's more successful.\"\nThe cortex is the part that understands  "
        }
      ]
    },
    {
      "tStartMs": 4279600,
      "dDurationMs": 5600,
      "segs": [
        {
          "utf8": "what success means in the modern context.\nBut the brainstem is able to align the cortex  "
        }
      ]
    },
    {
      "tStartMs": 4285200,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "and say, \"However you recognize success to be—and \nI’m not smart enough to understand what that is—  "
        }
      ]
    },
    {
      "tStartMs": 4289840,
      "dDurationMs": 6720,
      "segs": [
        {
          "utf8": "you're still going to pursue this directive.\"\nI think there's a more general point. "
        }
      ]
    },
    {
      "tStartMs": 4296560,
      "dDurationMs": 10400,
      "segs": [
        {
          "utf8": "I think it's actually really mysterious \nhow evolution encodes high-level desires. "
        }
      ]
    },
    {
      "tStartMs": 4306960,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "It's pretty easy to understand how \nevolution would endow us with the  "
        }
      ]
    },
    {
      "tStartMs": 4311920,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "desire for food that smells good because smell \nis a chemical, so just pursue that chemical. "
        }
      ]
    },
    {
      "tStartMs": 4318400,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "It's very easy to imagine \nevolution doing that thing. "
        }
      ]
    },
    {
      "tStartMs": 4322800,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "But evolution also has endowed \nus with all these social desires. "
        }
      ]
    },
    {
      "tStartMs": 4328880,
      "dDurationMs": 3840,
      "segs": [
        {
          "utf8": "We really care about being \nseen positively by society. "
        }
      ]
    },
    {
      "tStartMs": 4332720,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "We care about being in good standing.\nAll these social intuitions that we have,  "
        }
      ]
    },
    {
      "tStartMs": 4339760,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "I feel strongly that they're baked in.\nI don't know how evolution did it  "
        }
      ]
    },
    {
      "tStartMs": 4346240,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "because it's a high-level concept \nthat's represented in the brain. "
        }
      ]
    },
    {
      "tStartMs": 4351360,
      "dDurationMs": 9520,
      "segs": [
        {
          "utf8": "Let’s say you care about some social thing, \nit's not a low-level signal like smell. "
        }
      ]
    },
    {
      "tStartMs": 4360880,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "It's not something for which there is a sensor.\nThe brain needs to do a lot of processing to  "
        }
      ]
    },
    {
      "tStartMs": 4366320,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "piece together lots of bits of information \nto understand what's going on socially. "
        }
      ]
    },
    {
      "tStartMs": 4371440,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "Somehow evolution said, \"That's what you should \ncare about.\" How did it do it? It did it quickly,  "
        }
      ]
    },
    {
      "tStartMs": 4376640,
      "dDurationMs": 7760,
      "segs": [
        {
          "utf8": "too. All these sophisticated social things that we \ncare about, I think they evolved pretty recently. "
        }
      ]
    },
    {
      "tStartMs": 4384400,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "Evolution had an easy time \nhard-coding this high-level desire. "
        }
      ]
    },
    {
      "tStartMs": 4392000,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "I'm unaware of a good \nhypothesis for how it's done. "
        }
      ]
    },
    {
      "tStartMs": 4396000,
      "dDurationMs": 7920,
      "segs": [
        {
          "utf8": "I had some ideas I was kicking around, \nbut none of them are satisfying. "
        }
      ]
    },
    {
      "tStartMs": 4404560,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "What's especially impressive is it was desire \nthat you learned in your lifetime, it makes sense  "
        }
      ]
    },
    {
      "tStartMs": 4409680,
      "dDurationMs": 2880,
      "segs": [
        {
          "utf8": "because your brain is intelligent.\nIt makes sense why you would  "
        }
      ]
    },
    {
      "tStartMs": 4412560,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "be able to learn intelligent desires.\nMaybe this is not your point, but one way  "
        }
      ]
    },
    {
      "tStartMs": 4418880,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "to understand it is that the desire is built into \nthe genome, and the genome is not intelligent. "
        }
      ]
    },
    {
      "tStartMs": 4424240,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "But you're somehow able to describe this feature.\nIt's not even clear how you define that feature,  "
        }
      ]
    },
    {
      "tStartMs": 4430160,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "and you can build it into the genes.\nEssentially, or maybe I'll put it differently. "
        }
      ]
    },
    {
      "tStartMs": 4435520,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "If you think about the tools that \nare available to the genome, it says,  "
        }
      ]
    },
    {
      "tStartMs": 4441280,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "\"Okay, here's a recipe for building a brain.\"\nYou could say, \"Here is a recipe for connecting  "
        }
      ]
    },
    {
      "tStartMs": 4445760,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "the dopamine neurons to the smell sensor.\"\nAnd if the smell is a certain kind  "
        }
      ]
    },
    {
      "tStartMs": 4450560,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "of good smell, you want to eat that.\nI could imagine the genome doing that. "
        }
      ]
    },
    {
      "tStartMs": 4455680,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "I'm claiming that it is harder to imagine.\nIt's harder to imagine the genome saying  "
        }
      ]
    },
    {
      "tStartMs": 4461120,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "you should care about some complicated computation \nthat your entire brain, a big chunk of your brain,  "
        }
      ]
    },
    {
      "tStartMs": 4468080,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "does. That's all I'm claiming. I can tell \nyou a speculation of how it could be done. "
        }
      ]
    },
    {
      "tStartMs": 4473760,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "Let me offer a speculation, and I'll explain \nwhy the speculation is probably false. "
        }
      ]
    },
    {
      "tStartMs": 4477760,
      "dDurationMs": 14320,
      "segs": [
        {
          "utf8": "So the brain has brain regions. We have \nour cortex. It has all those brain regions. "
        }
      ]
    },
    {
      "tStartMs": 4492080,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "The cortex is uniform, but the brain \nregions and the neurons in the cortex  "
        }
      ]
    },
    {
      "tStartMs": 4497040,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "kind of speak to their neighbors mostly.\nThat explains why you get brain regions. "
        }
      ]
    },
    {
      "tStartMs": 4501120,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "Because if you want to do some kind of \nspeech processing, all the neurons that  "
        }
      ]
    },
    {
      "tStartMs": 4504640,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "do speech need to talk to each other.\nAnd because neurons can only speak to  "
        }
      ]
    },
    {
      "tStartMs": 4508000,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "their nearby neighbors, for the \nmost part, it has to be a region. "
        }
      ]
    },
    {
      "tStartMs": 4511520,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "All the regions are mostly located in \nthe same place from person to person. "
        }
      ]
    },
    {
      "tStartMs": 4515280,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "So maybe evolution hard-coded \nliterally a location on the brain. "
        }
      ]
    },
    {
      "tStartMs": 4521360,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "So it says, \"Oh, when the GPS coordinates \nof the brain such and such, when that fires,  "
        }
      ]
    },
    {
      "tStartMs": 4527920,
      "dDurationMs": 2800,
      "segs": [
        {
          "utf8": "that's what you should care about.\"\nMaybe that's what evolution did because  "
        }
      ]
    },
    {
      "tStartMs": 4530720,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "that would be within the toolkit of evolution.\nYeah, although there are examples where,  "
        }
      ]
    },
    {
      "tStartMs": 4536000,
      "dDurationMs": 8160,
      "segs": [
        {
          "utf8": "for example, people who are born blind have that \narea of their cortex adopted by another sense. "
        }
      ]
    },
    {
      "tStartMs": 4544960,
      "dDurationMs": 8240,
      "segs": [
        {
          "utf8": "I have no idea, but I'd be surprised if the \ndesires or the reward functions which require a  "
        }
      ]
    },
    {
      "tStartMs": 4553200,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "visual signal no longer worked for people who have \ntheir different areas of their cortex co-opted. "
        }
      ]
    },
    {
      "tStartMs": 4558720,
      "dDurationMs": 6960,
      "segs": [
        {
          "utf8": "For example, if you no longer have vision, can \nyou still feel the sense that I want people  "
        }
      ]
    },
    {
      "tStartMs": 4565680,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "around me to like me and so forth, which \nusually there are also visual cues for. "
        }
      ]
    },
    {
      "tStartMs": 4570000,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "I fully agree with that. I think there's an \neven stronger counterargument to this theory. "
        }
      ]
    },
    {
      "tStartMs": 4576880,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "There are people who get half of \ntheir brains removed in childhood,  "
        }
      ]
    },
    {
      "tStartMs": 4583360,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "and they still have all their brain regions.\nBut they all somehow move to just one hemisphere,  "
        }
      ]
    },
    {
      "tStartMs": 4587760,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "which suggests that the brain regions, \ntheir location is not fixed and so  "
        }
      ]
    },
    {
      "tStartMs": 4592240,
      "dDurationMs": 2000,
      "segs": [
        {
          "utf8": "that theory is not true.\nIt would have been cool  "
        }
      ]
    },
    {
      "tStartMs": 4594240,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "if it was true, but it's not.\nSo I think that's a mystery.  "
        }
      ]
    },
    {
      "tStartMs": 4597680,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "But it's an interesting mystery. The fact is \nthat somehow evolution was able to endow us  "
        }
      ]
    },
    {
      "tStartMs": 4603600,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "to care about social stuff very, very reliably.\nEven people who have all kinds of strange mental  "
        }
      ]
    },
    {
      "tStartMs": 4609840,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "conditions and deficiencies and emotional \nproblems tend to care about this also. "
        }
      ]
    },
    {
      "tStartMs": 4693360,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "What is SSI planning on doing differently?\nPresumably your plan is to be one of the  "
        }
      ]
    },
    {
      "tStartMs": 4698080,
      "dDurationMs": 9040,
      "segs": [
        {
          "utf8": "frontier companies when this time arrives.\nPresumably you started SSI because you're like,  "
        }
      ]
    },
    {
      "tStartMs": 4707120,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "\"I think I have a way of approaching how \nto do this safely in a way that the other  "
        }
      ]
    },
    {
      "tStartMs": 4710720,
      "dDurationMs": 7040,
      "segs": [
        {
          "utf8": "companies don't.\" What is that difference?\nThe way I would describe it is that there  "
        }
      ]
    },
    {
      "tStartMs": 4717760,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "are some ideas that I think are promising and \nI want to investigate them and see if they  "
        }
      ]
    },
    {
      "tStartMs": 4723040,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "are indeed promising or not. It's really that \nsimple. It's an attempt. If the ideas turn out  "
        }
      ]
    },
    {
      "tStartMs": 4728960,
      "dDurationMs": 12080,
      "segs": [
        {
          "utf8": "to be correct—these ideas that we discussed \naround understanding generalization—then I  "
        }
      ]
    },
    {
      "tStartMs": 4741040,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "think we will have something worthy.\nWill they turn out to be correct? We  "
        }
      ]
    },
    {
      "tStartMs": 4745200,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "are doing research. We are squarely an \"age of \nresearch\" company. We are making progress. We've  "
        }
      ]
    },
    {
      "tStartMs": 4750720,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "actually made quite good progress over the past \nyear, but we need to keep making more progress,  "
        }
      ]
    },
    {
      "tStartMs": 4754720,
      "dDurationMs": 11200,
      "segs": [
        {
          "utf8": "more research. That's how I see it. I see it \nas an attempt to be a voice and a participant. "
        }
      ]
    },
    {
      "tStartMs": 4769840,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "Your cofounder and previous CEO left to go to \nMeta recently, and people have asked, \"Well,  "
        }
      ]
    },
    {
      "tStartMs": 4777440,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "if there were a lot of breakthroughs being \nmade, that seems like a thing that should  "
        }
      ]
    },
    {
      "tStartMs": 4780960,
      "dDurationMs": 8240,
      "segs": [
        {
          "utf8": "have been unlikely.\" I wonder how you respond.\nFor this, I will simply remind a few facts that  "
        }
      ]
    },
    {
      "tStartMs": 4789200,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "may have been forgotten.\nI think these facts which  "
        }
      ]
    },
    {
      "tStartMs": 4792640,
      "dDurationMs": 6560,
      "segs": [
        {
          "utf8": "provide the context explain the situation.\nThe context was that we were fundraising at  "
        }
      ]
    },
    {
      "tStartMs": 4799200,
      "dDurationMs": 11520,
      "segs": [
        {
          "utf8": "a $32 billion valuation, and then Meta came \nin and offered to acquire us, and I said no. "
        }
      ]
    },
    {
      "tStartMs": 4810720,
      "dDurationMs": 9200,
      "segs": [
        {
          "utf8": "But my former cofounder in some sense said yes.\nAs a result, he also was able to enjoy a lot of  "
        }
      ]
    },
    {
      "tStartMs": 4819920,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "near-term liquidity, and he was the \nonly person from SSI to join Meta. "
        }
      ]
    },
    {
      "tStartMs": 4825440,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "It sounds like SSI's plan is to be a company \nthat is at the frontier when you get to this  "
        }
      ]
    },
    {
      "tStartMs": 4831200,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "very important period in human history \nwhere you have superhuman intelligence. "
        }
      ]
    },
    {
      "tStartMs": 4835600,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "You have these ideas about how to \nmake superhuman intelligence go well. "
        }
      ]
    },
    {
      "tStartMs": 4839360,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "But other companies will \nbe trying their own ideas. "
        }
      ]
    },
    {
      "tStartMs": 4842480,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "What distinguishes SSI's approach \nto making superintelligence go well? "
        }
      ]
    },
    {
      "tStartMs": 4848400,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "The main thing that distinguishes \nSSI is its technical approach. "
        }
      ]
    },
    {
      "tStartMs": 4854880,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "We have a different technical approach that \nI think is worthy and we are pursuing it. "
        }
      ]
    },
    {
      "tStartMs": 4861520,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "I maintain that in the end there \nwill be a convergence of strategies. "
        }
      ]
    },
    {
      "tStartMs": 4866160,
      "dDurationMs": 8800,
      "segs": [
        {
          "utf8": "I think there will be a convergence of strategies \nwhere at some point, as AI becomes more powerful,  "
        }
      ]
    },
    {
      "tStartMs": 4874960,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "it's going to become more or less clearer \nto everyone what the strategy should be. "
        }
      ]
    },
    {
      "tStartMs": 4879520,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "It should be something like, you need to find \nsome way to talk to each other and you want  "
        }
      ]
    },
    {
      "tStartMs": 4884800,
      "dDurationMs": 13120,
      "segs": [
        {
          "utf8": "your first actual real superintelligent AI to \nbe aligned and somehow care for sentient life,  "
        }
      ]
    },
    {
      "tStartMs": 4897920,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "care for people, democratic, one \nof those, some combination thereof. "
        }
      ]
    },
    {
      "tStartMs": 4902560,
      "dDurationMs": 7840,
      "segs": [
        {
          "utf8": "I think this is the condition \nthat everyone should strive for. "
        }
      ]
    },
    {
      "tStartMs": 4910400,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "That's what SSI is striving for.\nI think that this time, if not already,  "
        }
      ]
    },
    {
      "tStartMs": 4917040,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "all the other companies will realize that \nthey're striving towards the same thing.  "
        }
      ]
    },
    {
      "tStartMs": 4920320,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "We'll see. I think that the world will \ntruly change as AI becomes more powerful. "
        }
      ]
    },
    {
      "tStartMs": 4927920,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "I think things will be really different and \npeople will be acting really differently. "
        }
      ]
    },
    {
      "tStartMs": 4932480,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "Speaking of forecasts, what are your \nforecasts to this system you're describing,  "
        }
      ]
    },
    {
      "tStartMs": 4936560,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "which can learn as well as a human and \nsubsequently, as a result, become superhuman? "
        }
      ]
    },
    {
      "tStartMs": 4943840,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "I think like 5 to 20.\n5 to 20 years? "
        }
      ]
    },
    {
      "tStartMs": 4947600,
      "dDurationMs": 2160,
      "segs": [
        {
          "utf8": "Mhm.\nI just want  "
        }
      ]
    },
    {
      "tStartMs": 4949760,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "to unroll how you might see the world coming.\nIt's like, we have a couple more years where  "
        }
      ]
    },
    {
      "tStartMs": 4955680,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "these other companies are continuing \nthe current approach and it stalls out.  "
        }
      ]
    },
    {
      "tStartMs": 4960000,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "\"Stalls out\" here meaning they earn no more \nthan low hundreds of billions in revenue? "
        }
      ]
    },
    {
      "tStartMs": 4964720,
      "dDurationMs": 12480,
      "segs": [
        {
          "utf8": "How do you think about what stalling out means?\nI think stalling out will look like…it will  "
        }
      ]
    },
    {
      "tStartMs": 4977200,
      "dDurationMs": 3588,
      "segs": [
        {
          "utf8": "all look very similar among \nall the different companies. "
        }
      ]
    },
    {
      "tStartMs": 4980788,
      "dDurationMs": 1852,
      "segs": [
        {
          "utf8": "It could be something like this.\nI'm not sure because I think  "
        }
      ]
    },
    {
      "tStartMs": 4985360,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "even with stalling out, I think these \ncompanies could make a stupendous revenue. "
        }
      ]
    },
    {
      "tStartMs": 4990320,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "Maybe not profits because they will need \nto work hard to differentiate each other  "
        }
      ]
    },
    {
      "tStartMs": 4995280,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "from themselves, but revenue definitely.\nBut something in your model implies that  "
        }
      ]
    },
    {
      "tStartMs": 5003760,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "when the correct solution does emerge, there \nwill be convergence between all the companies. "
        }
      ]
    },
    {
      "tStartMs": 5007920,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "I'm curious why you think that's the case.\nI was talking more about convergence  "
        }
      ]
    },
    {
      "tStartMs": 5011440,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "on their alignment strategies.\nI think eventual convergence on  "
        }
      ]
    },
    {
      "tStartMs": 5014720,
      "dDurationMs": 4080,
      "segs": [
        {
          "utf8": "the technical approach is probably going \nto happen as well, but I was alluding  "
        }
      ]
    },
    {
      "tStartMs": 5018800,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "to convergence to the alignment strategies.\nWhat exactly is the thing that should be done? "
        }
      ]
    },
    {
      "tStartMs": 5023680,
      "dDurationMs": 3200,
      "segs": [
        {
          "utf8": "I just want to better understand \nhow you see the future unrolling. "
        }
      ]
    },
    {
      "tStartMs": 5026880,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "Currently, we have these different companies, and \nyou expect their approach to continue generating  "
        }
      ]
    },
    {
      "tStartMs": 5030320,
      "dDurationMs": 6240,
      "segs": [
        {
          "utf8": "revenue but not get to this human-like learner.\nSo now we have these different forks of companies. "
        }
      ]
    },
    {
      "tStartMs": 5036560,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "We have you, we have Thinking Machines, \nthere's a bunch of other labs. "
        }
      ]
    },
    {
      "tStartMs": 5040160,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "Maybe one of them figures \nout the correct approach. "
        }
      ]
    },
    {
      "tStartMs": 5043120,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "But then the release of their product makes \nit clear to other people how to do this thing. "
        }
      ]
    },
    {
      "tStartMs": 5047600,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "I think it won't be clear how to do it, but \nit will be clear that something different is  "
        }
      ]
    },
    {
      "tStartMs": 5051920,
      "dDurationMs": 5680,
      "segs": [
        {
          "utf8": "possible, and that is information.\nPeople will then be trying  "
        }
      ]
    },
    {
      "tStartMs": 5057600,
      "dDurationMs": 8960,
      "segs": [
        {
          "utf8": "to figure out how that works.\nI do think though that one of the things not  "
        }
      ]
    },
    {
      "tStartMs": 5066560,
      "dDurationMs": 7760,
      "segs": [
        {
          "utf8": "addressed here, not discussed, is that with each \nincrease in the AI's capabilities, I think there  "
        }
      ]
    },
    {
      "tStartMs": 5074320,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "will be some kind of changes, but I don't know \nexactly which ones, in how things are being done. "
        }
      ]
    },
    {
      "tStartMs": 5082960,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "I think it's going to be important, yet \nI can't spell out what that is exactly. "
        }
      ]
    },
    {
      "tStartMs": 5090320,
      "dDurationMs": 5040,
      "segs": [
        {
          "utf8": "By default, you would expect the company that \nhas that model to be getting all these gains  "
        }
      ]
    },
    {
      "tStartMs": 5095360,
      "dDurationMs": 7120,
      "segs": [
        {
          "utf8": "because they have the model that has the skills \nand knowledge that it's building up in the world. "
        }
      ]
    },
    {
      "tStartMs": 5102480,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "What is the reason to think that the benefits \nof that would be widely distributed and not  "
        }
      ]
    },
    {
      "tStartMs": 5105760,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "just end up at whatever model company gets \nthis continuous learning loop going first? "
        }
      ]
    },
    {
      "tStartMs": 5114880,
      "dDurationMs": 10800,
      "segs": [
        {
          "utf8": "Here is what I think is going to happen.\nNumber one, let's look at how things have  "
        }
      ]
    },
    {
      "tStartMs": 5125680,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "gone so far with the AIs of the past.\nOne company produced an advance and the  "
        }
      ]
    },
    {
      "tStartMs": 5132000,
      "dDurationMs": 8400,
      "segs": [
        {
          "utf8": "other company scrambled and produced some similar \nthings after some amount of time and they started  "
        }
      ]
    },
    {
      "tStartMs": 5140400,
      "dDurationMs": 8240,
      "segs": [
        {
          "utf8": "to compete in the market and push the prices down.\nSo I think from the market perspective,  "
        }
      ]
    },
    {
      "tStartMs": 5148640,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "something similar will happen there as well.\nWe are talking about the good world, by the way.  "
        }
      ]
    },
    {
      "tStartMs": 5156640,
      "dDurationMs": 11360,
      "segs": [
        {
          "utf8": "What's the good world? It’s where we have these \npowerful human-like learners that are also… By  "
        }
      ]
    },
    {
      "tStartMs": 5168000,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "the way, maybe there's another thing we haven't \ndiscussed on the spec of the superintelligent  "
        }
      ]
    },
    {
      "tStartMs": 5173760,
      "dDurationMs": 6400,
      "segs": [
        {
          "utf8": "AI that I think is worth considering.\nIt’s that you make it narrow, it can  "
        }
      ]
    },
    {
      "tStartMs": 5180160,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "be useful and narrow at the same time.\nYou can have lots of narrow superintelligent AIs. "
        }
      ]
    },
    {
      "tStartMs": 5184320,
      "dDurationMs": 7280,
      "segs": [
        {
          "utf8": "But suppose you have many of them and you \nhave some company that's producing a lot of  "
        }
      ]
    },
    {
      "tStartMs": 5192720,
      "dDurationMs": 2720,
      "segs": [
        {
          "utf8": "profits from it.\nThen you have another  "
        }
      ]
    },
    {
      "tStartMs": 5195440,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "company that comes in and starts to compete.\nThe way the competition is going to work is  "
        }
      ]
    },
    {
      "tStartMs": 5200240,
      "dDurationMs": 12560,
      "segs": [
        {
          "utf8": "through specialization. Competition loves \nspecialization. You see it in the market,  "
        }
      ]
    },
    {
      "tStartMs": 5212800,
      "dDurationMs": 2480,
      "segs": [
        {
          "utf8": "you see it in evolution as well.\nYou're going to have lots of different  "
        }
      ]
    },
    {
      "tStartMs": 5215280,
      "dDurationMs": 4240,
      "segs": [
        {
          "utf8": "niches and you're going to have lots of different \ncompanies who are occupying different niches. "
        }
      ]
    },
    {
      "tStartMs": 5219520,
      "dDurationMs": 9120,
      "segs": [
        {
          "utf8": "In this world we might say one AI company \nis really quite a bit better at some area  "
        }
      ]
    },
    {
      "tStartMs": 5228640,
      "dDurationMs": 4720,
      "segs": [
        {
          "utf8": "of really complicated economic activity and a \ndifferent company is better at another area. "
        }
      ]
    },
    {
      "tStartMs": 5233360,
      "dDurationMs": 1840,
      "segs": [
        {
          "utf8": "And the third company is \nreally good at litigation. "
        }
      ]
    },
    {
      "tStartMs": 5235200,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "Isn't this contradicted by what human-like \nlearning implies? It’s that it can learn… "
        }
      ]
    },
    {
      "tStartMs": 5239840,
      "dDurationMs": 5280,
      "segs": [
        {
          "utf8": "It can, but you have accumulated \nlearning. You have a big investment.  "
        }
      ]
    },
    {
      "tStartMs": 5245120,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "You spent a lot of compute to become really, \nreally good, really phenomenal at this thing. "
        }
      ]
    },
    {
      "tStartMs": 5250960,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "Someone else spent a huge amount \nof compute and a huge amount of  "
        }
      ]
    },
    {
      "tStartMs": 5254080,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "experience to get really good at some other thing.\nYou apply a lot of human learning to get there,  "
        }
      ]
    },
    {
      "tStartMs": 5258000,
      "dDurationMs": 6160,
      "segs": [
        {
          "utf8": "but now you are at this high point where \nsomeone else would say, \"Look, I don't want  "
        }
      ]
    },
    {
      "tStartMs": 5264160,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "to start learning what you've learned.\"\nI guess that would require many different  "
        }
      ]
    },
    {
      "tStartMs": 5267600,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "companies to begin at the human-like continual \nlearning agent at the same time so that they  "
        }
      ]
    },
    {
      "tStartMs": 5273360,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "can start their different tree \nsearch in different branches. "
        }
      ]
    },
    {
      "tStartMs": 5278000,
      "dDurationMs": 9280,
      "segs": [
        {
          "utf8": "But if one company gets that agent first, or gets \nthat learner first, it does then seem like… Well,  "
        }
      ]
    },
    {
      "tStartMs": 5289120,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "if you just think about every single job in \nthe economy, having an instance learning each  "
        }
      ]
    },
    {
      "tStartMs": 5295760,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "one seems tractable for a company.\nThat's a valid argument. My strong  "
        }
      ]
    },
    {
      "tStartMs": 5301200,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "intuition is that it's not how it's going to go.\nThe argument says it will go this way, but my  "
        }
      ]
    },
    {
      "tStartMs": 5308720,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "strong intuition is that it will not go this way.\nIn theory, there is no difference between theory  "
        }
      ]
    },
    {
      "tStartMs": 5316240,
      "dDurationMs": 3200,
      "segs": [
        {
          "utf8": "and practice. In practice, there is. I \nthink that's going to be one of those. "
        }
      ]
    },
    {
      "tStartMs": 5319440,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "A lot of people's models of recursive \nself-improvement literally, explicitly state  "
        }
      ]
    },
    {
      "tStartMs": 5324000,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "we will have a million Ilyas in a server that are \ncoming up with different ideas, and this will lead  "
        }
      ]
    },
    {
      "tStartMs": 5329360,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "to a superintelligence emerging very fast.\nDo you have some intuition about how  "
        }
      ]
    },
    {
      "tStartMs": 5332800,
      "dDurationMs": 8000,
      "segs": [
        {
          "utf8": "parallelizable the thing you are doing is?\nWhat are the gains from making copies of Ilya? "
        }
      ]
    },
    {
      "tStartMs": 5340800,
      "dDurationMs": 8640,
      "segs": [
        {
          "utf8": "I don’t know. I think there'll definitely be \ndiminishing returns because you want people  "
        }
      ]
    },
    {
      "tStartMs": 5349440,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "who think differently rather than the same.\nIf there were literal copies of me, I'm not sure  "
        }
      ]
    },
    {
      "tStartMs": 5354320,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "how much more incremental value you'd get.\nPeople who think differently,  "
        }
      ]
    },
    {
      "tStartMs": 5361920,
      "dDurationMs": 3440,
      "segs": [
        {
          "utf8": "that's what you want.\nWhy is it that if you look  "
        }
      ]
    },
    {
      "tStartMs": 5365360,
      "dDurationMs": 5440,
      "segs": [
        {
          "utf8": "at different models, even released by totally \ndifferent companies trained on potentially  "
        }
      ]
    },
    {
      "tStartMs": 5370800,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "non-overlapping datasets, it's actually \ncrazy how similar LLMs are to each other? "
        }
      ]
    },
    {
      "tStartMs": 5375920,
      "dDurationMs": 3600,
      "segs": [
        {
          "utf8": "Maybe the datasets are not as \nnon-overlapping as it seems. "
        }
      ]
    },
    {
      "tStartMs": 5379520,
      "dDurationMs": 4560,
      "segs": [
        {
          "utf8": "But there’s some sense in which even \nif an individual human might be less  "
        }
      ]
    },
    {
      "tStartMs": 5384080,
      "dDurationMs": 2880,
      "segs": [
        {
          "utf8": "productive than the future AI, maybe there’s \nsomething to the fact that human teams have  "
        }
      ]
    },
    {
      "tStartMs": 5386960,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "more diversity than teams of AIs might have.\nHow do we elicit meaningful diversity among AIs? "
        }
      ]
    },
    {
      "tStartMs": 5393600,
      "dDurationMs": 3120,
      "segs": [
        {
          "utf8": "I think just raising the temperature \njust results in gibberish. "
        }
      ]
    },
    {
      "tStartMs": 5396720,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "You want something more like different scientists \nhave different prejudices or different ideas. "
        }
      ]
    },
    {
      "tStartMs": 5401360,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "How do you get that kind of \ndiversity among AI agents? "
        }
      ]
    },
    {
      "tStartMs": 5404720,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "So the reason there has been no diversity, \nI believe, is because of pre-training. "
        }
      ]
    },
    {
      "tStartMs": 5410720,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "All the pre-trained models are pretty much the \nsame because they pre-train on the same data. "
        }
      ]
    },
    {
      "tStartMs": 5416800,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "Now RL and post-training is where \nsome differentiation starts to emerge  "
        }
      ]
    },
    {
      "tStartMs": 5420960,
      "dDurationMs": 4000,
      "segs": [
        {
          "utf8": "because different people come \nup with different RL training. "
        }
      ]
    },
    {
      "tStartMs": 5426400,
      "dDurationMs": 5120,
      "segs": [
        {
          "utf8": "I've heard you hint in the past \nabout self-play as a way to either  "
        }
      ]
    },
    {
      "tStartMs": 5431520,
      "dDurationMs": 6480,
      "segs": [
        {
          "utf8": "get data or match agents to other agents of \nequivalent intelligence to kick off learning. "
        }
      ]
    },
    {
      "tStartMs": 5438000,
      "dDurationMs": 8800,
      "segs": [
        {
          "utf8": "How should we think about why there are no public \nproposals of this kind of thing working with LLMs? "
        }
      ]
    },
    {
      "tStartMs": 5446800,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "I would say there are two things to say.\nThe reason why I thought self-play was  "
        }
      ]
    },
    {
      "tStartMs": 5452320,
      "dDurationMs": 8480,
      "segs": [
        {
          "utf8": "interesting is because it offered a way to \ncreate models using compute only, without data. "
        }
      ]
    },
    {
      "tStartMs": 5460800,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "If you think that data is the ultimate bottleneck, \nthen using compute only is very interesting. "
        }
      ]
    },
    {
      "tStartMs": 5466320,
      "dDurationMs": 9440,
      "segs": [
        {
          "utf8": "So that's what makes it interesting.\nThe thing is that self-play, at least the  "
        }
      ]
    },
    {
      "tStartMs": 5475760,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "way it was done in the past—when you have agents \nwhich somehow compete with each other—it's only  "
        }
      ]
    },
    {
      "tStartMs": 5481520,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "good for developing a certain set of skills. It \nis too narrow. It's only good for negotiation,  "
        }
      ]
    },
    {
      "tStartMs": 5489040,
      "dDurationMs": 6000,
      "segs": [
        {
          "utf8": "conflict, certain social skills, \nstrategizing, that kind of stuff. "
        }
      ]
    },
    {
      "tStartMs": 5495040,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "If you care about those skills, \nthen self-play will be useful. "
        }
      ]
    },
    {
      "tStartMs": 5499360,
      "dDurationMs": 7520,
      "segs": [
        {
          "utf8": "Actually, I think that self-play did find \na home, but just in a different form. "
        }
      ]
    },
    {
      "tStartMs": 5508080,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "So things like debate, prover-verifier, you \nhave some kind of an LLM-as-a-Judge which is  "
        }
      ]
    },
    {
      "tStartMs": 5515680,
      "dDurationMs": 4400,
      "segs": [
        {
          "utf8": "also incentivized to find mistakes in your work.\nYou could say this is not exactly self-play,  "
        }
      ]
    },
    {
      "tStartMs": 5520080,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "but this is a related adversarial \nsetup that people are doing, I believe. "
        }
      ]
    },
    {
      "tStartMs": 5524720,
      "dDurationMs": 8080,
      "segs": [
        {
          "utf8": "Really self-play is a special case of \nmore general competition between agents. "
        }
      ]
    },
    {
      "tStartMs": 5533600,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "The natural response to competition \nis to try to be different. "
        }
      ]
    },
    {
      "tStartMs": 5536560,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "So if you were to put multiple agents together \nand you tell them, \"You all need to work on some  "
        }
      ]
    },
    {
      "tStartMs": 5541360,
      "dDurationMs": 5200,
      "segs": [
        {
          "utf8": "problem and you are an agent and you're inspecting \nwhat everyone else is working,\" they’re going to  "
        }
      ]
    },
    {
      "tStartMs": 5546560,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "say, \"Well, if they're already taking this \napproach, it's not clear I should pursue it.  "
        }
      ]
    },
    {
      "tStartMs": 5551520,
      "dDurationMs": 4480,
      "segs": [
        {
          "utf8": "I should pursue something differentiated.\" So I \nthink something like this could also create an  "
        }
      ]
    },
    {
      "tStartMs": 5556000,
      "dDurationMs": 8400,
      "segs": [
        {
          "utf8": "incentive for a diversity of approaches.\nFinal question: What is research taste? "
        }
      ]
    },
    {
      "tStartMs": 5564400,
      "dDurationMs": 6640,
      "segs": [
        {
          "utf8": "You're obviously the person in \nthe world who is considered to  "
        }
      ]
    },
    {
      "tStartMs": 5571040,
      "dDurationMs": 10240,
      "segs": [
        {
          "utf8": "have the best taste in doing research in AI.\nYou were the co-author on the biggest things  "
        }
      ]
    },
    {
      "tStartMs": 5581280,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "that have happened in the history of deep \nlearning, from AlexNet to GPT-3 to so on. "
        }
      ]
    },
    {
      "tStartMs": 5585040,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "What is it, how do you characterize \nhow you come up with these ideas? "
        }
      ]
    },
    {
      "tStartMs": 5591120,
      "dDurationMs": 7600,
      "segs": [
        {
          "utf8": "I can comment on this for myself.\nI think different people do it differently. "
        }
      ]
    },
    {
      "tStartMs": 5598720,
      "dDurationMs": 10960,
      "segs": [
        {
          "utf8": "One thing that guides me personally is an \naesthetic of how AI should be, by thinking  "
        }
      ]
    },
    {
      "tStartMs": 5609680,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "about how people are, but thinking correctly.\nIt's very easy to think about how people are  "
        }
      ]
    },
    {
      "tStartMs": 5615520,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "incorrectly, but what does it mean to think \nabout people correctly? I'll give you some  "
        }
      ]
    },
    {
      "tStartMs": 5620320,
      "dDurationMs": 7760,
      "segs": [
        {
          "utf8": "examples. The idea of the artificial neuron \nis directly inspired by the brain, and it's a  "
        }
      ]
    },
    {
      "tStartMs": 5628080,
      "dDurationMs": 4640,
      "segs": [
        {
          "utf8": "great idea. Why? Because you say the brain has \nall these different organs, it has the folds,  "
        }
      ]
    },
    {
      "tStartMs": 5632720,
      "dDurationMs": 3680,
      "segs": [
        {
          "utf8": "but the folds probably don't matter.\nWhy do we think that the neurons matter? "
        }
      ]
    },
    {
      "tStartMs": 5636400,
      "dDurationMs": 4880,
      "segs": [
        {
          "utf8": "Because there are many of them.\nIt kind of feels right, so you want the neuron. "
        }
      ]
    },
    {
      "tStartMs": 5641280,
      "dDurationMs": 8320,
      "segs": [
        {
          "utf8": "You want some local learning rule that will \nchange the connections between the neurons. "
        }
      ]
    },
    {
      "tStartMs": 5650240,
      "dDurationMs": 5360,
      "segs": [
        {
          "utf8": "It feels plausible that the brain does it.\nThe idea of the distributed representation. "
        }
      ]
    },
    {
      "tStartMs": 5655600,
      "dDurationMs": 4320,
      "segs": [
        {
          "utf8": "The idea that the brain responds \nto experience therefore our neural  "
        }
      ]
    },
    {
      "tStartMs": 5659920,
      "dDurationMs": 3280,
      "segs": [
        {
          "utf8": "net should learn from experience.\nThe brain learns from experience,  "
        }
      ]
    },
    {
      "tStartMs": 5664480,
      "dDurationMs": 4800,
      "segs": [
        {
          "utf8": "the neural net should learn from experience.\nYou kind of ask yourself, is something fundamental  "
        }
      ]
    },
    {
      "tStartMs": 5669280,
      "dDurationMs": 6320,
      "segs": [
        {
          "utf8": "or not fundamental? How things should be. \nI think that's been guiding me a fair bit,  "
        }
      ]
    },
    {
      "tStartMs": 5675600,
      "dDurationMs": 5840,
      "segs": [
        {
          "utf8": "thinking from multiple angles and looking \nfor almost beauty, beauty and simplicity. "
        }
      ]
    },
    {
      "tStartMs": 5681440,
      "dDurationMs": 4960,
      "segs": [
        {
          "utf8": "Ugliness, there's no room for ugliness.\nIt's beauty, simplicity, elegance,  "
        }
      ]
    },
    {
      "tStartMs": 5686400,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "correct inspiration from the brain.\nAll of those things need to  "
        }
      ]
    },
    {
      "tStartMs": 5689360,
      "dDurationMs": 3760,
      "segs": [
        {
          "utf8": "be present at the same time.\nThe more they are present, the  "
        }
      ]
    },
    {
      "tStartMs": 5693120,
      "dDurationMs": 5520,
      "segs": [
        {
          "utf8": "more confident you can be in a top-down belief.\nThe top-down belief is the thing that sustains  "
        }
      ]
    },
    {
      "tStartMs": 5698640,
      "dDurationMs": 5920,
      "segs": [
        {
          "utf8": "you when the experiments contradict you.\nBecause if you trust the data all the time,  "
        }
      ]
    },
    {
      "tStartMs": 5704560,
      "dDurationMs": 2960,
      "segs": [
        {
          "utf8": "well sometimes you can be doing the \ncorrect thing but there's a bug. "
        }
      ]
    },
    {
      "tStartMs": 5707520,
      "dDurationMs": 3520,
      "segs": [
        {
          "utf8": "But you don't know that there is a bug.\nHow can you tell that there is a bug? "
        }
      ]
    },
    {
      "tStartMs": 5711040,
      "dDurationMs": 3920,
      "segs": [
        {
          "utf8": "How do you know if you should keep debugging or \nyou conclude it's the wrong direction? It's the  "
        }
      ]
    },
    {
      "tStartMs": 5714960,
      "dDurationMs": 5760,
      "segs": [
        {
          "utf8": "top-down. You can say things have to be this way.\nSomething like this has to work,  "
        }
      ]
    },
    {
      "tStartMs": 5720720,
      "dDurationMs": 4160,
      "segs": [
        {
          "utf8": "therefore we’ve got to keep going.\nThat's the top-down, and it's based on this  "
        }
      ]
    },
    {
      "tStartMs": 5725520,
      "dDurationMs": 6080,
      "segs": [
        {
          "utf8": "multifaceted beauty and inspiration by the brain.\nAlright, we'll leave it there. "
        }
      ]
    },
    {
      "tStartMs": 5731600,
      "dDurationMs": 3360,
      "segs": [
        {
          "utf8": "Thank you so much.\nIlya, thank you so much. "
        }
      ]
    },
    {
      "tStartMs": 5734960,
      "dDurationMs": 1520,
      "segs": [
        {
          "utf8": "Alright. Appreciate it.\nThat was great. "
        }
      ]
    },
    {
      "tStartMs": 5736480,
      "dDurationMs": 2560,
      "segs": [
        {
          "utf8": "Yeah, I enjoyed it.\nYes, me too."
        }
      ]
    }
  ]
}